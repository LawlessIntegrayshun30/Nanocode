diff --git a/nanocode-core/README.md b/nanocode-core/README.md
index e69de29bb2d1d6434b8b29ae775ad8c2e48c5391..64c8170ef9956a81ef716f651a8937c9f2893eed 100644
--- a/nanocode-core/README.md
+++ b/nanocode-core/README.md
@@ -0,0 +1,174 @@
+## Nanocode prototype
+
+This repository provides a lightweight Nanocode interpreter prototype built on a term-rewriting runtime.
+
+### Running programs
+
+```
+python -m src.cli path/to/program.nanocode --trace-jsonl trace.jsonl
+```
+
+Programs are expressed as S-expressions; see `tests/test_cli.py` for end-to-end examples that emit a JSON summary and optional JSONL trace of runtime events.
+You can also pipe program source directly via stdin using `-` as the program argument:
+
+```
+cat path/to/program.nanocode | python -m src.cli -
+```
+
+Semicolon-delimited comments are ignored by the parser, so you can annotate sources inline or on dedicated lines.
+
+Pass `--walk-children` to automatically schedule child terms for rewriting instead of only rewriting the root/frontier returned by rules:
+
+```
+python -m src.cli path/to/program.nanocode --walk-children
+```
+
+When walking children, you can bound recursion with `--walk-depth N` to avoid traversing very deep trees:
+
+```
+python -m src.cli path/to/program.nanocode --walk-children --walk-depth 2
+```
+
+Use `--strict-matching` to fail fast when multiple rules match the same term; without it, the first matching rule wins:
+
+```
+python -m src.cli path/to/program.nanocode --strict-matching
+```
+
+Enable `--detect-conflicts` to refuse programs whose rules have deterministic overlapping patterns (same symbol and scale without predicates), providing a lightweight coherence guard before execution:
+
+```
+python -m src.cli path/to/program.nanocode --detect-conflicts
+```
+
+Enforce symbol arities and allowed scales by supplying a JSON signature via `--signature` so genomes stay well-formed during parsing and rewriting:
+
+```
+python -m src.cli path/to/program.nanocode --signature path/to/signature.json
+```
+
+Signature files declare per-symbol constraints:
+
+```
+{
+  "symbols": {
+    "root": {"min_children": 1, "max_children": 2, "scales": [0]},
+    "leaf": {"min_children": 0, "max_children": 0, "scales": [1]}
+  }
+}
+```
+
+Validate a program (and optional stored snapshot) without executing any rewrites using `--dry-run` to catch issues quickly:
+
+```
+python -m src.cli path/to/program.nanocode --dry-run
+```
+
+Guard against runaway term growth by halting once the store exceeds a limit with `--max-terms`:
+
+```
+python -m src.cli path/to/program.nanocode --max-terms 1000
+```
+
+Persist a JSON snapshot of the term store for replay or debugging with `--store-json`.
+Snapshots now include the root term ID, the pending frontier, scheduler choice/seed (for random runs), and the set of already-processed terms so runs can be resumed:
+
+```
+python -m src.cli path/to/program.nanocode --store-json state/store.json
+```
+
+Resume from a stored snapshot (for example, after running with `--steps-only` to capture a mid-flight state) using `--load-store`:
+
+```
+python -m src.cli path/to/program.nanocode --load-store state/store.json
+```
+
+Switch the scheduler to depth-first style processing with LIFO semantics using `--scheduler lifo` (FIFO is the default), or use
+`--scheduler random` with `--scheduler-seed` to take a seeded randomized walk through the frontier:
+
+```
+python -m src.cli path/to/program.nanocode --scheduler lifo
+python -m src.cli path/to/program.nanocode --scheduler random --scheduler-seed 7
+```
+
+When resuming from a snapshot, the runtime will honor the persisted scheduler, walk/strict flags, scheduler seed/state, and other metadata unless you explicitly override them (for example, with `--no-walk-children`).
+
+Cap how many times a specific rule may fire with `--rule-budget name=N` (repeatable) to prevent runaway rewrites or enforce fairness:
+
+```
+python -m src.cli path/to/program.nanocode --walk-children --rule-budget grow=3 --rule-budget normalize=5
+```
+
+Rule budgets are stored in snapshots and surfaced in CLI summaries, along with a list of any exhausted budgets, so you can resume runs with the same limits.
+
+Scope execution to a subset of rules with `--only-rule name` (repeatable) or skip specific rules with `--skip-rule name`; when both are provided, an error is raised if the sets overlap. Filters are validated against the program rule names, persisted in snapshots, honored on resume, and surfaced in CLI summaries:
+
+```
+python -m src.cli path/to/program.nanocode --only-rule grow --skip-rule normalize
+```
+
+Similarly, you can confine execution to certain term scales with `--only-scale N` (repeatable) or skip work on specific scales with `--skip-scale N`. Scale filters must be non-negative, are stored in snapshots, and are replayed on resume alongside other runtime configuration:
+
+```
+python -m src.cli path/to/program.nanocode --walk-children --only-scale 1
+```
+
+Programs are validated before execution: rule names must be unique, scales cannot be negative, and step budgets must be positive. Invalid inputs surface as CLI errors so issues are caught early.
+
+The CLI summary now includes runtime configuration and per-rule/per-scale rewrite counts so you can track which rules fired and at what scale:
+
+```
+{
+  "program": "demo",
+  "root": "T0",
+  "walk_children": false,
+  "strict_matching": false,
+  "detect_conflicts": false,
+  "scheduler": "fifo",
+  "scheduler_seed": null,
+  "include_rules": null,
+  "exclude_rules": [],
+  "include_scales": null,
+  "exclude_scales": [],
+  "rule_budgets": {"grow": 3},
+  "max_terms": 1000,
+  "events": 2,
+  "rule_counts": {"grow": 1, "normalize": 1},
+  "scale_counts": {"0": 2},
+  "rule_budget_exhausted": [],
+  "term_limit_exhausted": false,
+  "constraint_exhausted": false,
+  "constraint_violations": [],
+  "constraints": {"max_depth": 3},
+  "idle": true,
+  "budget_exhausted": false,
+  "frontier": [],
+  "store_size": 3
+}
+```
+
+### Prototype success script
+Run the end-to-end prototype demo that builds micro/meso/macro structure from text, classifies it via a bridge, emits a JSONL trace, and saves a term-store snapshot in one command:
+
+```
+python -m src.prototype --text "nanocode" --trace-jsonl demo.trace.jsonl --store-json demo.store.json
+```
+
+The command constructs a deterministic program (micro tokens → meso motifs → lifted summary → macro bridge annotation), runs it with conflict detection and child walking enabled, and prints a JSON summary containing the final macro term and label alongside the optional trace/snapshot paths.
+
+Structural constraints provided on a program (or loaded from snapshots) are enforced during runtime: any rewrite that would exceed node/depth/fanout/scale limits raises immediately, propagates the violations into `constraint_violations`, and persists through stored bundles so resumable runs keep the same guardrails.
+
+### Evolution scaffolding
+- `src.evolution` provides deterministic mutation and recombination helpers over Nanocode genomes expressed as `Term` trees. You can mutate symbols/scales, delete or insert subtrees based on contextual spawn functions, and perform seeded crossovers between genomes to build evolutionary loops that stay within the Nanocode substrate.
+- `measure_structure`/`validate_structure` expose deterministic size/depth/fanout/scale metrics and constraint checks so genomes can carry explicit well-formedness bounds during search.
+- `evolve_population` and `evaluate_population` layer scoring, tournament selection, elitism, and callback-driven generation logging on top of those primitives so you can run reproducible evolutionary searches that keep provenance attached to each Nanocode genome and penalize invalid structures deterministically when constraints are supplied.
+
+### Meta-level program representation
+- `src.meta` converts rules/programs into Nanocode `Term` structures (`program_to_term`, `rule_to_term`) and back (`term_to_program`, `term_to_rule`). Actions now carry explicit names/params via `Action`, so rule sets and interpreter configurations can be serialized as first-class Nanocode data for self-hosting, analysis, or mutation in evolutionary loops.
+- The conversion helpers validate shapes and preserve budgets/configuration (`max_steps`, `max_terms`), making it possible to treat entire interpreters as genomes without losing determinism or guardrails.
+- Structural and formal guardrails are also serializable: `constraints_to_term`/`term_to_constraints` round-trip `StructuralConstraints`, while `signature_to_term`/`term_to_signature` preserve symbol/arity/scale signatures so validation policies stay attached to meta-level genomes.
+
+### Goal-directed agents and policy rollouts
+- `src.agent` defines a minimal agent substrate pairing Nanocode programs with observation encoders and action decoders, plus a lightweight environment protocol. Policies are executed deterministically per observation by re-rooting the program and running through the interpreter.
+- `rollout_agent` runs episodes against an environment, capturing per-step executions, accumulated reward, and optional goal-specific scores via user-provided reward functions. This makes it easy to score Nanocode genomes as agents for evolution or evaluation loops without leaving the symbolic substrate.
+- Bridge schemas make agent/environment boundaries first-class: `BridgeSchema`/`BridgeBinding` describe typed input/output ports (with optional scales) and adapters that deterministically wrap observations/actions in tagged Nanocode terms. Bridge schemas serialize to/from terms (including deterministic metadata), keeping adapter capabilities representable for meta-level mutation alongside the rest of a genome.
diff --git a/nanocode-core/docs/nanocode_interpreter_arch.md b/nanocode-core/docs/nanocode_interpreter_arch.md
new file mode 100644
index 0000000000000000000000000000000000000000..434346ba127822dcee16e6b01c1fe33813bfa252
--- /dev/null
+++ b/nanocode-core/docs/nanocode_interpreter_arch.md
@@ -0,0 +1,104 @@
+# Nanocode Interpreter Architecture (Draft)
+
+## Goals
+- Execute Nanocode programs as a term-rewriting system with explicit scales (micro/meso/macro) and guaranteed coherence properties (D_s(E_s(t^s)) = t^s).
+- Provide a streaming runtime that can emit partial results (anytime property) and support deterministic replay for debugging.
+- Offer extensible bridges to probabilistic/quantum backends without coupling core reduction semantics to hardware assumptions.
+
+## High-level components
+1. **Frontend**
+   - A parser for a minimal Nanocode DSL (S-expressions first) that produces `Term` trees with scale annotations.
+   - Validation layer to ensure rewrite rules are well-typed (e.g., fanout, allowed scale transitions) and to reject non-coherent rules at load time.
+2. **Runtime kernel**
+   - **Term store**: persistent, hash-addressed DAG of `Term` nodes to enable structural sharing and reversible steps.
+   - **Rewrite engine**: orchestrates expansion/reduction; each step is logged with causal metadata for replay and visualization.
+   - **Scheduler**: chooses rewrite order (depth-first, breadth-first, or heuristic) and supports cooperative interruption to expose anytime intermediate states.
+3. **Bridges**
+   - **Classical adapters**: micro/meso/macro hooks that map external streams into terms and export motifs back to clients.
+   - **Quantum adapters**: pluggable samplers that turn shot counts into motifs and feed them into the meso layer; stays optional so the core remains deterministic.
+4. **Developer tooling**
+   - Tracing API to visualize scales and motifs over time.
+   - Property checks for coherence and confluence on small bounded rewrites.
+
+## Execution model
+- **Term representation**: keep `Term(sym, scale, children)` from `src/terms.py`, but store children as stable IDs in the term store to allow deduplication and memoized reductions.
+- **Rewrite rule shape**: `(pattern, action)` where `pattern` matches a `Term` (including scale) and `action` yields expanded or reduced terms.
+- **Step semantics**:
+  1. Load a program = set of rewrite rules + initial root term.
+  2. Scheduler picks a frontier node; rewrite engine applies expansion or reduction.
+  3. Each step emits an event `{before, after, scale, rule_id, timestamp}` to a log.
+  4. Anytime snapshots are just prefixes of the log + term store.
+- **Coherence enforcement**: during rule registration, run static checks that for every expansion rule `E_s`, a paired reduction `D_s` exists such that `D_s(E_s(t)) == t` for representative samples (property-based tests on random symbols).
+
+## Proposed file/module layout
+- `src/ast.py`: parser + AST/Term construction helpers from DSL.
+- `src/term_store.py`: persistent DAG storage, hash-addressing, deduplication, snapshotting.
+- `src/rewrite.py`: rule definitions, pattern matching, expansion/reduction helpers.
+- `src/scheduler.py`: scheduling strategies and interruption hooks.
+- `src/runtime.py`: orchestrates loading programs, executing steps, exposing streaming interface.
+- `src/bridges/quantum.py`: wrappers around oracles/samplers → meso motifs; pure interface first, fake backend stays for tests.
+- `src/bridges/classical.py`: micro/meso adapters for text/JSON streams → motifs.
+- `src/tooling/trace.py`: event log emitters, replay, and visualization hooks.
+
+## Near-term diffs to reach a runnable interpreter
+1. **Refactor terms**: move `Term` to `term_store.py`, add stable IDs and hashing; adjust `expand/reduce` to work against the store rather than in-memory child lists.
+2. **Introduce rewrite rules**: add `rewrite.py` with pattern/action definitions and unit tests covering coherence checks and simple confluence cases.
+3. **Runtime skeleton**: implement `runtime.py` with a stepping API (`step()`, `run(max_steps)`, `snapshot()`), using the scheduler and emitting events.
+4. **Parser**: create `ast.py` to parse a tiny DSL (S-expressions) into `Term` + rule objects, guarded by validation.
+5. **Bridge isolation**: move existing `quantum_bridge.py` into `src/bridges/quantum.py` with a clear interface; add `classical.py` that wraps current micro/meso pipeline from `pipeline.py` as adapters.
+6. **Observability**: add a tracing hook that writes step logs to disk/STDOUT; integrate with tests to assert emitted events.
+7. **Tests**: expand `tests/` to cover coherence property, deterministic replay from logs, and scheduler heuristics (e.g., depth-first vs breadth-first outcomes).
+
+## Prototype entry point (implemented)
+- `Program`: declarative bundle of `root` term, rewrite `rules`, and a step budget.
+- `Interpreter.run(program)`: builds a fresh `Runtime`, loads the root, and drives the scheduler until idle (or the step budget) while returning an execution snapshot (events, frontier, store records).
+- `Event`: now captures the `before`/`after` term payloads to simplify tracing and replay scaffolding.
+
+### Tracing hooks
+- `JSONLTracer`: lightweight hook that can be passed into the runtime to stream `Event` records to any file-like sink for later replay/visualization.
+
+### S-expression parser (prototype)
+- `parse_program` in `src/ast.py` reads a minimal S-expression DSL:
+  - `(root <term>)` builds a nested `Term` tree where `:scale N` overrides per-node scale and child lists follow the symbol.
+  - `(rules (rule <name> (pattern :sym foo :scale 0) (action expand :fanout 2)) ...)` turns into `Rule` objects wired to built-in `expand`/`reduce` actions.
+  - `(max_steps N)` controls the interpreter budget.
+  - Semicolon-delimited comments are ignored so programs can be annotated inline without impacting parsing.
+- Programs are validated before execution: duplicate rule names are rejected, negative scales are disallowed, and `max_steps` must be positive so malformed inputs fail fast.
+- This parser feeds the existing `Interpreter` so end-to-end runs can be described textually and replayed via the runtime/tracer without hand-authoring Python rules.
+
+### CLI entry point
+- `python -m src.cli path/to/program.nanocode` parses an S-expression program, runs it through the runtime, and prints a JSON summary.
+- `--dry-run` parses and validates a program (and optional stored snapshot) without executing rewrites to surface issues quickly.
+- `--trace-jsonl` streams runtime events to a JSONL file for downstream replay/visualization.
+- `--walk-children` instructs the runtime to automatically schedule child terms for rewriting (instead of only rewriting the frontiers returned by rules). Use `--walk-depth N` to cap recursion depth when walking children to avoid traversing very deep trees.
+- `--strict-matching` raises on ambiguous rule matches rather than silently selecting the first rule, helping surface coherence issues early.
+- `--detect-conflicts` rejects programs with deterministic overlapping rule patterns (same symbol/scale without predicates) as a lightweight coherence guard before execution.
+- `--scheduler lifo` switches the rewrite order to a LIFO/stack strategy (FIFO remains the default), useful for depth-first traversals.
+- `--scheduler random` takes a seeded randomized walk through the frontier; pair with `--scheduler-seed` for reproducible runs. Random scheduler state (seed and RNG state) is persisted in snapshots so resuming preserves selection order.
+- The CLI summary includes runtime configuration (scheduler, strict-matching, walk-children) plus per-rule and per-scale counters (`rule_counts`/`scale_counts`) alongside the frontier, store size, idle/budget exhaustion flags to highlight which rules fired, how much work remains, and whether a step budget halted execution.
+- `--store-json` writes the term store snapshot to disk with root/frontier/processed metadata and runtime configuration so runs can be replayed or inspected offline without re-running the program. Pair with `--steps-only` to capture mid-flight snapshots.
+- `--load-store` bootstraps the runtime from a stored snapshot, honoring the persisted scheduler/strict/walk settings (including stored walk-depth) by default (override with `--no-walk-children`, `--walk-depth`, or `--strict-matching/--no-strict-matching`).
+- `--rule-budget name=N` (repeatable) caps how many times a rule may fire, preventing runaway rewrites; budgets are persisted in snapshots and surfaced in summaries alongside any exhausted budget list for visibility/resume fidelity.
+- `--only-rule name` (repeatable) restricts execution to specific rules, while `--skip-rule name` excludes rules; filters are validated against the program, persisted in snapshots, and surfaced in summaries so resumed runs preserve the same rule visibility.
+- `--only-scale N` (repeatable) confines work to terms at specific scales, while `--skip-scale N` excludes scales; filters must be non-negative, persist in snapshots, and are surfaced in summaries so resumed runs mirror the same scale visibility.
+- `--max-terms N` halts rewriting once the store exceeds `N` unique terms, surfacing `term_limit_exhausted` in summaries/snapshots so pipelines can fail fast instead of allocating unbounded DAGs.
+- `--signature path/to/signature.json` enforces per-symbol arity/scale constraints loaded from JSON and persisted in snapshots, keeping genomes well-formed across parses, rewrites, and resumable runs.
+
+### Evolutionary substrate (early scaffolding)
+- `src.evolution` offers deterministic mutations (symbol changes, scale tweaks, subtree insertion/deletion) and crossover between `Term` trees to treat Nanocode programs as genomes. The helpers accept seeded RNGs and contextual spawn functions so evolutionary loops remain deterministic and operate purely over Nanocode structures.
+- `measure_structure`/`validate_structure` provide deterministic footprinting (size, depth, fanout, scale bounds) and constraint validation so evolutionary loops can enforce well-formedness budgets during search.
+- `evaluate_population` and `evolve_population` add scoring-aware selection, elitism, and callback-driven generation logging so deterministic evolutionary searches can run over Nanocode genomes with provenance preserved in annotations and invalid structures penalized deterministically when constraints are supplied.
+
+### Meta-level program representation
+- Actions are wrapped in `Action` objects with stable names/params, enabling serialization of rules and programs into Nanocode `Term` structures via `src.meta` (`rule_to_term`, `program_to_term`) and reconstruction through `term_to_rule`/`term_to_program`. This allows interpreters and rule sets to be manipulated as Nanocode data, paving the way for self-hosted meta-rewrites and evolutionary mutation of full interpreters.
+
+### Agent/goal semantics (initial scaffolding)
+- `src.agent` introduces a minimal agent substrate that pairs Nanocode programs with observation encoders and action decoders plus a lightweight environment protocol. Each observation is re-rooted into the program and run through the interpreter, keeping goal-directed behavior squarely within the deterministic rewrite core.
+- `rollout_agent` executes an episode, recording per-step executions, cumulative reward, and optional goal scores computed by caller-provided reward functions. This provides a deterministic path to score Nanocode genomes as agents and plug them into evolutionary loops without leaving the symbolic substrate.
+- Bridge schemas (`BridgeSchema`/`BridgeBinding`) make environment boundaries representable as Nanocode data: ports are typed (direction + optional scale), validated for determinism, and serializable to terms (including deterministic metadata) for meta-level mutation or evolution alongside policies. Tagged port terms keep adapter semantics explicit during execution and replay.
+
+## Open questions to resolve during implementation
+- What minimal DSL syntax is acceptable for v0? (Recommendation: S-expressions with `(expand ...)`/`(reduce ...)` forms.)
+- Should confluence be enforced globally or verified on-demand per rule set? (Start with bounded checks in tests.)
+- How should quantum motifs be represented so they remain compatible with classical motifs? (Propose `{amplitude_summary: ..., shots: ...}` schema shared across bridges.)
+- What persistence format should the term store use for replay? (JSONL or SQLite; choose based on library policy.)
diff --git a/nanocode-core/src/__init__.py b/nanocode-core/src/__init__.py
index e69de29bb2d1d6434b8b29ae775ad8c2e48c5391..21c6627fe52d29fac0eb9065ba4289176afb71f8 100644
--- a/nanocode-core/src/__init__.py
+++ b/nanocode-core/src/__init__.py
@@ -0,0 +1,60 @@
+from src.bridge import (  # noqa: F401
+    BRIDGE_SYM,
+    PORT_SYM,
+    BridgeBinding,
+    BridgePort,
+    BridgeSchema,
+    InvalidBridgeSchema,
+    bridge_call_action,
+    bridge_schema_from_term,
+    bridge_schema_to_term,
+    validate_bridge_schema,
+)
+from src.agent import AgentPolicy, EpisodeResult, EpisodeStep, Goal, rollout_agent  # noqa: F401
+from src.ast import parse_program, parse_rule, parse_term  # noqa: F401
+from src.constraints import (  # noqa: F401
+    StructuralConstraints,
+    StructuralMetrics,
+    measure_structure,
+    validate_structure,
+)
+from src.interpreter import Execution, Interpreter, Program, detect_conflicts, validate_program  # noqa: F401
+from src.evolution import (  # noqa: F401
+    EvolutionConfig,
+    Evaluation,
+    Genome,
+    annotate_genome,
+    crossover_terms,
+    delete_subtree,
+    evaluate_population,
+    evolve_population,
+    insert_subtree,
+    mutate_scale,
+    mutate_symbol,
+)
+from src.meta import (  # noqa: F401
+    action_to_term,
+    constraints_to_term,
+    program_to_term,
+    rule_to_term,
+    signature_to_term,
+    term_to_action,
+    term_to_constraints,
+    term_to_program,
+    term_to_rule,
+    term_to_signature,
+    term_to_rules,
+    rules_to_term,
+)
+from src.rewrite import (  # noqa: F401
+    Action,
+    AmbiguousRuleError,
+    action_from_spec,
+    expand_action,
+    lift_action,
+    reduce_action,
+)
+from src.scheduler import FIFOScheduler, LIFOScheduler, RandomScheduler  # noqa: F401
+from src.signature import Signature, SignatureError, TermSignature  # noqa: F401
+from src.terms import Term  # noqa: F401
+from src.trace import JSONLTracer, dump_events  # noqa: F401
diff --git a/nanocode-core/src/agent.py b/nanocode-core/src/agent.py
new file mode 100644
index 0000000000000000000000000000000000000000..118f90df462e39b5f4a49ade38380976f1439c52
--- /dev/null
+++ b/nanocode-core/src/agent.py
@@ -0,0 +1,139 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Callable, Iterable, Protocol
+
+from src.bridge import BridgeBinding, InvalidBridgeSchema, validate_bridge_schema
+from src.interpreter import Execution, Interpreter, Program
+from src.terms import Term
+
+
+class Environment(Protocol):
+    """Minimal environment protocol for goal-directed Nanocode agents."""
+
+    def reset(self) -> object:
+        ...
+
+    def step(self, action: object) -> tuple[object, float, bool, dict]:
+        ...
+
+
+@dataclass(frozen=True)
+class Goal:
+    name: str
+    reward_fn: Callable[[Iterable["EpisodeStep"]], float]
+    description: str | None = None
+
+
+@dataclass(frozen=True)
+class AgentPolicy:
+    """Pair a Nanocode program with observation/action adapters."""
+
+    program: Program
+    encode_observation: Callable[[object], Term] | None = None
+    decode_action: Callable[[Term, Execution], object] | None = None
+    bridge: BridgeBinding | None = None
+    observation_port: str | None = None
+    action_port: str | None = None
+
+    def __post_init__(self):
+        if self.bridge:
+            validate_bridge_schema(self.bridge.schema)
+        if self.encode_observation is None and not (self.bridge and self.observation_port):
+            raise InvalidBridgeSchema(
+                "observation encoding must be provided via encode_observation or a bridge port"
+            )
+        if self.decode_action is None and not (self.bridge and self.action_port):
+            raise InvalidBridgeSchema(
+                "action decoding must be provided via decode_action or a bridge port"
+            )
+
+
+@dataclass
+class EpisodeStep:
+    observation: object
+    action: object
+    reward: float
+    done: bool
+    execution: Execution
+    info: dict | None = None
+
+
+@dataclass
+class EpisodeResult:
+    steps: list[EpisodeStep]
+    total_reward: float
+    goal_score: float | None
+
+
+def rollout_agent(
+    policy: AgentPolicy,
+    env: Environment,
+    *,
+    interpreter: Interpreter | None = None,
+    run_kwargs: dict | None = None,
+    max_steps: int | None = None,
+    goal: Goal | None = None,
+) -> EpisodeResult:
+    """Run a Nanocode policy against an environment episode.
+
+    The policy's program is re-rooted per observation, keeping execution
+    deterministic while allowing external sensors/bridges to feed into the
+    rewrite substrate. The interpreter is instantiated once to preserve
+    configuration across steps.
+    """
+
+    interpreter = interpreter or Interpreter()
+    run_kwargs = run_kwargs or {}
+
+    observation = env.reset()
+    steps: list[EpisodeStep] = []
+    total_reward = 0.0
+    step_count = 0
+
+    while True:
+        encoded = _encode_observation(policy, observation)
+        execution = interpreter.run(policy.program.with_root(encoded), **run_kwargs)
+        action_term = execution.materialize_root()
+        action = _decode_action(policy, action_term, execution)
+
+        next_obs, reward, done, info = env.step(action)
+        total_reward += reward
+        steps.append(
+            EpisodeStep(
+                observation=observation,
+                action=action,
+                reward=reward,
+                done=done,
+                execution=execution,
+                info=info,
+            )
+        )
+
+        step_count += 1
+        if done:
+            break
+        if max_steps is not None and step_count >= max_steps:
+            break
+
+        observation = next_obs
+
+    goal_score = goal.reward_fn(steps) if goal else None
+    return EpisodeResult(steps=steps, total_reward=total_reward, goal_score=goal_score)
+
+
+def _encode_observation(policy: AgentPolicy, observation: object) -> Term:
+    if policy.bridge and policy.observation_port:
+        return policy.bridge.encode_input(policy.observation_port, observation)
+    if policy.encode_observation is None:
+        raise InvalidBridgeSchema("no observation encoder available")
+    return policy.encode_observation(observation)
+
+
+def _decode_action(policy: AgentPolicy, action_term: Term, execution: Execution) -> object:
+    if policy.bridge and policy.action_port:
+        return policy.bridge.decode_output(policy.action_port, action_term)
+    if policy.decode_action is None:
+        raise InvalidBridgeSchema("no action decoder available")
+    return policy.decode_action(action_term, execution)
+
diff --git a/nanocode-core/src/ast.py b/nanocode-core/src/ast.py
new file mode 100644
index 0000000000000000000000000000000000000000..cf9610b37c3bc3efd3c92c8472d243b3c8657094
--- /dev/null
+++ b/nanocode-core/src/ast.py
@@ -0,0 +1,208 @@
+from __future__ import annotations
+
+import re
+from typing import Iterable, Iterator, List, Sequence
+
+from src.interpreter import Program, validate_program
+from src.rewrite import Pattern, Rule, expand_action, reduce_action
+from src.terms import Term
+
+
+Token = str
+
+
+def _symbol_from_expr(expr: object) -> str:
+    if isinstance(expr, str):
+        return expr
+    if isinstance(expr, list) and expr:
+        head, *tail = expr
+        rendered_tail = ",".join(_symbol_from_expr(t) for t in tail)
+        return f"{head}({rendered_tail})" if rendered_tail else str(head)
+    raise ValueError(f"Invalid symbol expression: {expr}")
+
+
+def _strip_comments(src: str) -> str:
+    """Remove semicolon-to-EOL comments for a more forgiving DSL."""
+
+    lines = []
+    for line in src.splitlines():
+        # Treat ';' as comment leader until end of line, mirroring Lisp-style
+        # S-expression conventions.
+        uncommented = line.split(";", 1)[0]
+        lines.append(uncommented)
+    return "\n".join(lines)
+
+
+def _tokenize(src: str) -> List[Token]:
+    cleaned = _strip_comments(src)
+    return re.findall(r"\(|\)|[^\s()]+", cleaned)
+
+
+def _read_tokens(tokens: Sequence[Token]) -> object:
+    """Convert a flat token list into a nested S-expression list."""
+
+    def read(queue: List[Token]) -> object:
+        if not queue:
+            return []
+
+        tok = queue.pop(0)
+        if tok == "(":
+            items = []
+            while queue and queue[0] != ")":
+                items.append(read(queue))
+            if not queue:  # pragma: no cover - defensive
+                raise ValueError("Unbalanced parentheses in source")
+            queue.pop(0)  # consume ')'
+            return items
+
+        if tok == ")":  # pragma: no cover - defensive
+            raise ValueError("Unexpected ')'")
+
+        return tok
+
+    return read(list(tokens))
+
+
+def parse_term(expr: object) -> Term:
+    if isinstance(expr, str):
+        return Term(sym=expr)
+
+    if not isinstance(expr, list) or not expr:
+        raise ValueError(f"Invalid term expression: {expr}")
+
+    sym = expr[0]
+    scale = 0
+    children: List[object] = []
+
+    it = iter(expr[1:])
+    for item in it:
+        if isinstance(item, str) and item == ":scale":
+            try:
+                scale_value = next(it)
+            except StopIteration as exc:  # pragma: no cover - defensive
+                raise ValueError("Missing :scale value") from exc
+            scale = int(scale_value)
+        else:
+            children.append(item)
+
+    return Term(sym=sym, scale=scale, children=[parse_term(child) for child in children])
+
+
+def parse_pattern(expr: object) -> Pattern:
+    if not isinstance(expr, list):
+        raise ValueError("Pattern must be a list expression")
+
+    items = expr[1:] if expr and expr[0] == "pattern" else expr
+    sym = None
+    scale = None
+
+    i = 0
+    while i < len(items):
+        key = items[i]
+        i += 1
+        if not isinstance(key, str) or not key.startswith(":"):
+            raise ValueError(f"Unexpected pattern token: {key}")
+
+        values: List[object] = []
+        while i < len(items) and not (isinstance(items[i], str) and items[i].startswith(":")):
+            values.append(items[i])
+            i += 1
+
+        if not values:
+            raise ValueError(f"Missing value for {key}")
+
+        if key == ":sym":
+            value = values[0] if len(values) == 1 else values
+            sym = _symbol_from_expr(value)
+        elif key == ":scale":
+            scale = int(values[0])
+        else:  # pragma: no cover - future extensions
+            raise ValueError(f"Unknown pattern key: {key}")
+
+    return Pattern(sym=sym, scale=scale)
+
+
+def _action_expand(args: Iterable[Token]):
+    fanout = 3
+    it = iter(args)
+    for key in it:
+        if key == ":fanout":
+            try:
+                fanout = int(next(it))
+            except StopIteration as exc:  # pragma: no cover - defensive
+                raise ValueError("Missing :fanout value") from exc
+    return expand_action(fanout=fanout)
+
+
+def _action_reduce(_: Iterable[Token]):
+    return reduce_action()
+
+
+action_registry = {
+    "expand": _action_expand,
+    "reduce": _action_reduce,
+}
+
+
+def parse_action(expr: object):
+    if not isinstance(expr, list) or not expr:
+        raise ValueError("Action must be a list expression")
+
+    items = expr[1:] if expr[0] == "action" else expr
+    name, *args = items
+    if name not in action_registry:
+        raise ValueError(f"Unknown action: {name}")
+
+    return action_registry[name](args)
+
+
+def parse_rule(expr: object) -> Rule:
+    if not isinstance(expr, list) or len(expr) < 4 or expr[0] != "rule":
+        raise ValueError(f"Invalid rule expression: {expr}")
+
+    _, name, pattern_expr, action_expr, *rest = expr
+    if rest:
+        raise ValueError(f"Unexpected tokens in rule {name}: {rest}")
+
+    pattern = parse_pattern(pattern_expr)
+    action = parse_action(action_expr)
+    return Rule(name=name, pattern=pattern, action=action)
+
+
+def parse_program(src: str) -> Program:
+    expr = _read_tokens(_tokenize(src))
+    if not isinstance(expr, list) or not expr or expr[0] != "program":
+        raise ValueError("Program must start with (program ...)")
+
+    name = expr[1] if len(expr) > 1 else "nanocode"
+    root_term: Term | None = None
+    rules: List[Rule] = []
+    max_steps = 256
+    max_terms: int | None = None
+
+    for part in expr[2:]:
+        if not isinstance(part, list) or not part:
+            continue
+        tag = part[0]
+        if tag == "root":
+            if len(part) != 2:
+                raise ValueError("(root ...) expects a single term")
+            root_term = parse_term(part[1])
+        elif tag == "rules":
+            for rule_expr in part[1:]:
+                rules.append(parse_rule(rule_expr))
+        elif tag == "max_steps":
+            if len(part) != 2:
+                raise ValueError("(max_steps N) expects a single integer")
+            max_steps = int(part[1])
+        elif tag == "max_terms":
+            if len(part) != 2:
+                raise ValueError("(max_terms N) expects a single integer")
+            max_terms = int(part[1])
+
+    if root_term is None:
+        raise ValueError("Program missing root term")
+
+    program = Program(name=name, root=root_term, rules=rules, max_steps=max_steps, max_terms=max_terms)
+    validate_program(program)
+    return program
diff --git a/nanocode-core/src/bridge.py b/nanocode-core/src/bridge.py
new file mode 100644
index 0000000000000000000000000000000000000000..599ce039722291a32b099de1d8445383813d327b
--- /dev/null
+++ b/nanocode-core/src/bridge.py
@@ -0,0 +1,220 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Callable, Literal
+
+from src.terms import Term
+from src.rewrite import Action
+
+
+class InvalidBridgeSchema(ValueError):
+    """Raised when a bridge schema violates deterministic constraints."""
+
+
+@dataclass(frozen=True)
+class BridgePort:
+    name: str
+    direction: Literal["in", "out"]
+    scale: int | None = None
+    description: str | None = None
+
+
+@dataclass(frozen=True)
+class BridgeSchema:
+    name: str
+    ports: tuple[BridgePort, ...]
+    metadata: dict[str, object] | None = None
+
+    def inputs(self) -> tuple[BridgePort, ...]:
+        return tuple(p for p in self.ports if p.direction == "in")
+
+    def outputs(self) -> tuple[BridgePort, ...]:
+        return tuple(p for p in self.ports if p.direction == "out")
+
+    def port(self, name: str) -> BridgePort:
+        for port in self.ports:
+            if port.name == name:
+                return port
+        raise KeyError(name)
+
+
+def validate_bridge_schema(schema: BridgeSchema) -> BridgeSchema:
+    seen: set[str] = set()
+    for port in schema.ports:
+        if port.name in seen:
+            raise InvalidBridgeSchema(f"duplicate port name: {port.name}")
+        seen.add(port.name)
+        if port.direction not in {"in", "out"}:
+            raise InvalidBridgeSchema(f"invalid direction for {port.name}: {port.direction}")
+        if port.scale is not None and port.scale < 0:
+            raise InvalidBridgeSchema(f"negative scale for {port.name}: {port.scale}")
+    return schema
+
+
+@dataclass(frozen=True)
+class BridgeBinding:
+    schema: BridgeSchema
+    encode: dict[str, Callable[[object], Term]]
+    decode: dict[str, Callable[[Term], object]]
+
+    def encode_input(self, name: str, payload: object) -> Term:
+        port = self.schema.port(name)
+        if port.direction != "in":
+            raise InvalidBridgeSchema(f"port {name} is not an input")
+        try:
+            encoder = self.encode[name]
+        except KeyError as exc:
+            raise InvalidBridgeSchema(f"missing encoder for port {name}") from exc
+        term = encoder(payload)
+        return _tag_port(port, term)
+
+    def decode_output(self, name: str, term: Term) -> object:
+        port = self.schema.port(name)
+        if port.direction != "out":
+            raise InvalidBridgeSchema(f"port {name} is not an output")
+        try:
+            decoder = self.decode[name]
+        except KeyError as exc:
+            raise InvalidBridgeSchema(f"missing decoder for port {name}") from exc
+        _validate_port_tag(port, term)
+        payload = _untag_port(term)
+        return decoder(payload)
+
+
+BRIDGE_SYM = "bridge"
+PORT_SYM = "port"
+METADATA_SYM = "metadata"
+
+
+def bridge_schema_to_term(schema: BridgeSchema) -> Term:
+    validate_bridge_schema(schema)
+    ports = [
+        Term(
+            sym=f"{PORT_SYM}:{port.direction}:{port.name}",
+            scale=port.scale or 0,
+        )
+        for port in schema.ports
+    ]
+    metadata_term = _metadata_to_term(schema.metadata)
+    children = ports + ([metadata_term] if metadata_term is not None else [])
+    return Term(sym=f"{BRIDGE_SYM}:{schema.name}", children=children)
+
+
+def bridge_schema_from_term(term: Term) -> BridgeSchema:
+    if not term.sym.startswith(f"{BRIDGE_SYM}:"):
+        raise InvalidBridgeSchema(f"not a bridge term: {term.sym}")
+    name = term.sym.split(":", 1)[1]
+    ports: list[BridgePort] = []
+    metadata: dict[str, object] | None = None
+    for child in term.children:
+        if not child.sym.startswith(f"{PORT_SYM}:"):
+            if child.sym == METADATA_SYM:
+                metadata = _metadata_from_term(child)
+                continue
+            raise InvalidBridgeSchema(f"not a port term: {child.sym}")
+        _, direction, port_name = child.sym.split(":", 2)
+        port = BridgePort(name=port_name, direction=direction, scale=child.scale)
+        ports.append(port)
+    schema = BridgeSchema(name=name, ports=tuple(ports), metadata=metadata)
+    return validate_bridge_schema(schema)
+
+
+def _tag_port(port: BridgePort, term: Term) -> Term:
+    return Term(sym=f"{PORT_SYM}:{port.direction}:{port.name}", scale=term.scale, children=[term])
+
+
+def _validate_port_tag(port: BridgePort, term: Term) -> None:
+    expected = f"{PORT_SYM}:{port.direction}:{port.name}"
+    if term.sym != expected:
+        raise InvalidBridgeSchema(f"mismatched port tag: expected {expected}, got {term.sym}")
+
+
+def _untag_port(term: Term) -> Term:
+    if not term.children:
+        raise InvalidBridgeSchema("port term missing payload child")
+    return term.children[0]
+
+
+def labeled_term(sym: str, payload: Term, *, scale: int | None = None) -> Term:
+    """Attach a label sym to a payload term for bridge I/O annotation."""
+
+    return Term(sym=sym, scale=scale if scale is not None else payload.scale, children=payload.children)
+
+
+def _metadata_to_term(metadata: dict[str, object] | None) -> Term | None:
+    if not metadata:
+        return None
+    return Term(
+        sym=METADATA_SYM,
+        children=[Term(sym=key, children=[_value_to_term(val)]) for key, val in sorted(metadata.items())],
+    )
+
+
+def _metadata_from_term(term: Term) -> dict[str, object]:
+    if term.sym != METADATA_SYM:
+        raise InvalidBridgeSchema(f"expected metadata term, got {term.sym}")
+    metadata: dict[str, object] = {}
+    for child in term.children:
+        if not child.children:
+            raise InvalidBridgeSchema("metadata entries must contain a value child")
+        metadata[child.sym] = _value_from_term(child.children[0])
+    return metadata
+
+
+def _value_to_term(value: object) -> Term:
+    if isinstance(value, bool):
+        return Term(sym=str(value))
+    if isinstance(value, int):
+        return Term(sym=str(value))
+    if isinstance(value, float):
+        return Term(sym=str(value))
+    if isinstance(value, str):
+        return Term(sym=value)
+    raise InvalidBridgeSchema(f"unsupported metadata type: {type(value)}")
+
+
+def _value_from_term(term: Term) -> object:
+    if term.children:
+        raise InvalidBridgeSchema("metadata value terms must not have children")
+    text = term.sym
+    if text in {"True", "False"}:
+        return text == "True"
+    try:
+        return int(text)
+    except ValueError:
+        pass
+    try:
+        return float(text)
+    except ValueError:
+        pass
+    return text
+
+
+def bridge_call_action(
+    binding: BridgeBinding,
+    output_port: str,
+    fn: Callable[[Term], object],
+) -> Action:
+    """Create a rewrite Action that consults a bridge and annotates the term.
+
+    The callable ``fn`` acts as the deterministic oracle producing payloads for
+    the named ``output_port``. The resulting term keeps the original payload as
+    a child alongside the port-tagged bridge output so downstream rules can
+    consume the external signal without breaking determinism.
+    """
+
+    port = binding.schema.port(output_port)
+    if port.direction != "out":
+        raise InvalidBridgeSchema(f"Port {output_port} is not an output")
+    try:
+        encoder = binding.encode[output_port]
+    except KeyError as exc:
+        raise InvalidBridgeSchema(f"missing encoder for output port {output_port}") from exc
+
+    def _apply(term: Term, _store) -> Term:
+        payload = fn(term)
+        encoded = encoder(payload)
+        tagged = _tag_port(port, encoded)
+        return Term(sym=f"bridge:{term.sym}", scale=max(term.scale, port.scale or term.scale), children=[term, tagged])
+
+    return Action(name=f"bridge:{binding.schema.name}:{output_port}", params={}, fn=_apply)
diff --git a/nanocode-core/src/cli.py b/nanocode-core/src/cli.py
new file mode 100644
index 0000000000000000000000000000000000000000..beffc468bb54d67d84136b44aab957041461de6e
--- /dev/null
+++ b/nanocode-core/src/cli.py
@@ -0,0 +1,410 @@
+from __future__ import annotations
+
+import argparse
+import json
+import sys
+from dataclasses import replace
+from pathlib import Path
+from typing import Iterable, Optional
+
+from src.ast import parse_program
+from src.constraints import StructuralConstraints, constraints_from_dict, constraints_to_dict
+from src.runtime import Runtime
+from src.scheduler import FIFOScheduler, LIFOScheduler, RandomScheduler
+from src.signature import Signature
+from src.trace import JSONLTracer
+from src.term_store import TermStore
+
+
+def _read_program_source(path: str) -> str:
+    """Load program text from a file path or stdin.
+
+    Passing ``-`` reads from stdin to support piping programs into the CLI.
+    """
+
+    if path == "-":
+        return sys.stdin.read()
+
+    target = Path(path)
+    if not target.exists():
+        raise FileNotFoundError(path)
+    return target.read_text()
+
+
+def _add_tracer(runtime: Runtime, destination: str):
+    sink = open(destination, "w", encoding="utf-8")
+    tracer = JSONLTracer(sink)
+    runtime.event_hooks.append(tracer)
+    return sink
+
+
+def _load_store(path: str) -> dict:
+    target = Path(path)
+    if not target.exists():
+        raise FileNotFoundError(path)
+
+    return json.loads(target.read_text())
+
+
+def _load_signature(path: str) -> Signature:
+    target = Path(path)
+    if not target.exists():
+        raise FileNotFoundError(path)
+
+    return Signature.from_dict(json.loads(target.read_text()))
+
+
+def _resolve_scheduler(choice: Optional[str]) -> str:
+    if choice is None:
+        return "fifo"
+    if choice not in {"fifo", "lifo", "random"}:
+        raise ValueError(f"Unsupported scheduler: {choice}")
+    return choice
+
+
+def _build_scheduler(choice: str, *, seed: int | None = None, state: object | None = None):
+    def _coerce_state(payload: object | None) -> object | None:
+        if isinstance(payload, list):
+            return tuple(_coerce_state(item) for item in payload)
+        if isinstance(payload, tuple):
+            return tuple(_coerce_state(item) for item in payload)
+        return payload
+
+    if choice == "lifo":
+        return LIFOScheduler()
+    if choice == "random":
+        return RandomScheduler(seed=seed, state=_coerce_state(state))
+    return FIFOScheduler()
+
+
+def _parse_rule_budgets(entries: Optional[Iterable[str]]) -> Optional[dict[str, int]]:
+    """Translate CLI ``--rule-budget name=N`` entries into a mapping."""
+
+    if not entries:
+        return None
+
+    budgets: dict[str, int] = {}
+    for entry in entries:
+        if "=" not in entry:
+            raise ValueError(f"Rule budget must be name=N: {entry}")
+        name, raw = entry.split("=", 1)
+        if not name:
+            raise ValueError("Rule budget name cannot be empty")
+        limit = int(raw)
+        if limit <= 0:
+            raise ValueError(f"Rule budget for {name} must be positive")
+        budgets[name] = limit
+
+    return budgets
+
+
+def run_cli(argv: Iterable[str] | None = None) -> int:
+    parser = argparse.ArgumentParser(description="Run a Nanocode program from an S-expression file.")
+    parser.add_argument("program", help="Path to the Nanocode program (S-expression format)")
+    parser.add_argument("--trace-jsonl", dest="trace_jsonl", help="Write runtime events to a JSONL file")
+    parser.add_argument(
+        "--max-steps",
+        dest="max_steps",
+        type=int,
+        help="Override the program step budget (applies to this invocation only)",
+    )
+    parser.add_argument(
+        "--steps-only",
+        action="store_true",
+        help="Run for the max-steps budget without waiting for the scheduler to idle",
+    )
+    parser.add_argument(
+        "--walk-children",
+        action=argparse.BooleanOptionalAction,
+        default=None,
+        help="Automatically schedule child terms for rewriting",
+    )
+    parser.add_argument(
+        "--walk-depth",
+        dest="walk_depth",
+        type=int,
+        help="Limit how deep child-walking recurses (applies when walk-children is enabled)",
+    )
+    parser.add_argument(
+        "--dry-run",
+        action="store_true",
+        help="Parse and validate the program (and optional snapshot) without executing rewrites",
+    )
+    parser.add_argument(
+        "--strict-matching",
+        action=argparse.BooleanOptionalAction,
+        default=None,
+        help="Fail fast if multiple rules match the same term instead of picking the first",
+    )
+    parser.add_argument(
+        "--store-json",
+        dest="store_json",
+        help="Write the term store snapshot to a JSON file for replay/inspection",
+    )
+    parser.add_argument(
+        "--load-store",
+        dest="load_store",
+        help="Bootstrap the runtime from a prior store snapshot (JSON)",
+    )
+    parser.add_argument(
+        "--scheduler",
+        choices=("fifo", "lifo", "random"),
+        default=None,
+        help="Choose the rewrite scheduling strategy (fifo, lifo, or random)",
+    )
+    parser.add_argument(
+        "--scheduler-seed",
+        dest="scheduler_seed",
+        type=int,
+        help="Seed for the random scheduler (ignored for fifo/lifo)",
+    )
+    parser.add_argument(
+        "--max-terms",
+        dest="max_terms",
+        type=int,
+        help="Limit how many terms may be stored before halting",
+    )
+    parser.add_argument(
+        "--rule-budget",
+        action="append",
+        dest="rule_budgets",
+        help="Limit how many times a rule may fire (name=N, can repeat)",
+    )
+    parser.add_argument(
+        "--only-rule",
+        action="append",
+        dest="include_rules",
+        help="Restrict execution to the specified rule(s) (can repeat)",
+    )
+    parser.add_argument(
+        "--skip-rule",
+        action="append",
+        dest="exclude_rules",
+        help="Exclude the specified rule(s) from execution (can repeat)",
+    )
+    parser.add_argument(
+        "--only-scale",
+        action="append",
+        dest="include_scales",
+        type=int,
+        help="Restrict execution to terms at the specified scale(s) (can repeat)",
+    )
+    parser.add_argument(
+        "--skip-scale",
+        action="append",
+        dest="exclude_scales",
+        type=int,
+        help="Exclude terms at the specified scale(s) from execution (can repeat)",
+    )
+    parser.add_argument(
+        "--detect-conflicts",
+        action=argparse.BooleanOptionalAction,
+        default=None,
+        help="Fail fast when rules have deterministic overlapping patterns",
+    )
+    parser.add_argument(
+        "--signature",
+        dest="signature",
+        help="Path to a JSON signature defining allowed symbols, scales, and arities",
+    )
+
+    args = parser.parse_args(list(argv) if argv is not None else None)
+
+    sink = None
+
+    try:
+        src = _read_program_source(args.program)
+        program = parse_program(src)
+        if args.max_steps is not None:
+            program = replace(program, max_steps=args.max_steps)
+
+        state = _load_store(args.load_store) if args.load_store else None
+        signature = program.signature
+        if state and isinstance(state, dict) and state.get("signature") is not None:
+            signature = Signature.from_dict(state["signature"])
+        if args.signature:
+            signature = _load_signature(args.signature)
+        constraints: StructuralConstraints | None = program.constraints
+        if state and isinstance(state, dict) and state.get("constraints") is not None:
+            constraints = constraints_from_dict(state["constraints"])
+        scheduler_choice = _resolve_scheduler(args.scheduler or (state.get("scheduler") if state else None))
+        scheduler_seed = args.scheduler_seed if args.scheduler_seed is not None else (state.get("scheduler_seed") if state else None)
+        scheduler_state = state.get("scheduler_state") if state else None
+        scheduler = _build_scheduler(scheduler_choice, seed=scheduler_seed, state=scheduler_state)
+        walk_children = args.walk_children
+        strict_matching = args.strict_matching
+        state_budgets = state.get("rule_budgets") if state else None
+        rule_budgets = _parse_rule_budgets(args.rule_budgets)
+        state_max_terms = state.get("max_terms") if state else None
+        max_terms = args.max_terms if args.max_terms is not None else state_max_terms
+        if max_terms is not None and max_terms <= 0:
+            raise ValueError("max_terms must be positive")
+        state_walk_depth = state.get("walk_depth") if state else None
+        walk_depth = args.walk_depth if args.walk_depth is not None else state_walk_depth
+        if walk_depth is not None and walk_depth <= 0:
+            raise ValueError("walk_depth must be positive")
+        include_rules = args.include_rules if args.include_rules is not None else None
+        exclude_rules = args.exclude_rules if args.exclude_rules is not None else None
+        include_scales = args.include_scales if args.include_scales is not None else None
+        exclude_scales = args.exclude_scales if args.exclude_scales is not None else None
+        detect_conflicts = args.detect_conflicts
+        if state:
+            if include_rules is None and "include_rules" in state:
+                include_rules = state.get("include_rules")
+            if exclude_rules is None and "exclude_rules" in state:
+                exclude_rules = state.get("exclude_rules")
+            if include_scales is None and "include_scales" in state:
+                include_scales = state.get("include_scales")
+            if exclude_scales is None and "exclude_scales" in state:
+                exclude_scales = state.get("exclude_scales")
+            if detect_conflicts is None and "detect_conflicts" in state:
+                detect_conflicts = bool(state.get("detect_conflicts"))
+        if include_rules and exclude_rules:
+            overlap = set(include_rules) & set(exclude_rules)
+            if overlap:
+                raise ValueError(f"Rules cannot be both included and excluded: {sorted(overlap)}")
+        if include_scales and exclude_scales:
+            overlap = set(include_scales) & set(exclude_scales)
+            if overlap:
+                raise ValueError(f"Scales cannot be both included and excluded: {sorted(overlap)}")
+        rule_names = {rule.name for rule in program.rules}
+        if include_rules:
+            missing = set(include_rules) - rule_names
+            if missing:
+                raise ValueError(f"Included rules not found: {sorted(missing)}")
+        if exclude_rules:
+            missing = set(exclude_rules) - rule_names
+            if missing:
+                raise ValueError(f"Excluded rules not found: {sorted(missing)}")
+        for scale in (include_scales or []) + (exclude_scales or []):
+            if scale < 0:
+                raise ValueError("Scale filters must be non-negative")
+        if state:
+            if walk_children is None:
+                walk_children = bool(state.get("walk_children", False))
+            if strict_matching is None:
+                strict_matching = bool(state.get("strict_matching", False))
+            if rule_budgets is None:
+                rule_budgets = dict(state_budgets) if state_budgets else None
+        if walk_children is None:
+            walk_children = False
+        if strict_matching is None:
+            strict_matching = False
+        if rule_budgets is None:
+            rule_budgets = {}
+        if detect_conflicts is None:
+            detect_conflicts = False
+        runtime = Runtime(
+            program.rules,
+            scheduler=scheduler,
+            walk_children=walk_children,
+            walk_depth=walk_depth,
+            strict_matching=strict_matching,
+            rule_budgets=rule_budgets,
+            max_terms=max_terms,
+            include_rules=include_rules,
+            exclude_rules=exclude_rules,
+            include_scales=include_scales,
+            exclude_scales=exclude_scales,
+            detect_conflicts=detect_conflicts,
+            signature=signature,
+            constraints=constraints,
+        )
+        sink = _add_tracer(runtime, args.trace_jsonl) if args.trace_jsonl else None
+        if state:
+            store = TermStore.from_json(state)
+            frontier = state.get("frontier") if isinstance(state, dict) else None
+            processed = state.get("processed") if isinstance(state, dict) else None
+            exhausted = state.get("rule_budget_exhausted") if isinstance(state, dict) else None
+            root_id = state.get("root") if isinstance(state, dict) else None
+            if root_id is None:
+                raise ValueError("Store snapshot is missing a root term id")
+            runtime.load_state(
+                store=store,
+                root_id=root_id,
+                frontier=frontier,
+                processed=processed,
+                rule_budgets=rule_budgets,
+                rule_budget_exhausted=exhausted,
+                scheduler_state=scheduler_state,
+                include_rules=include_rules,
+                exclude_rules=exclude_rules,
+                include_scales=include_scales,
+                exclude_scales=exclude_scales,
+                detect_conflicts=detect_conflicts,
+                constraints=constraints,
+            )
+        else:
+            runtime.load(program.root)
+        if args.dry_run:
+            stats = runtime.stats()
+        else:
+            if args.steps_only:
+                runtime.run(max_steps=program.max_steps)
+            else:
+                runtime.run_until_idle(max_steps=program.max_steps)
+
+            stats = runtime.stats()
+        summary = {
+            "program": program.name,
+            "root": runtime.root_id,
+            "dry_run": args.dry_run,
+            "walk_children": runtime.walk_children,
+            "walk_depth": runtime.walk_depth,
+            "strict_matching": runtime.strict_matching,
+            "detect_conflicts": runtime.detect_conflicts,
+            "scheduler": runtime._scheduler_name(),
+            "scheduler_seed": getattr(runtime.scheduler, "seed", None),
+            "rule_budgets": runtime.rule_budgets,
+            "max_terms": runtime.max_terms,
+            "include_rules": sorted(runtime.include_rules) if runtime.include_rules else None,
+            "exclude_rules": sorted(runtime.exclude_rules) if runtime.exclude_rules else [],
+            "include_scales": sorted(runtime.include_scales) if runtime.include_scales else None,
+            "exclude_scales": sorted(runtime.exclude_scales) if runtime.exclude_scales else [],
+            "signature_symbols": len(signature.to_dict()["symbols"]) if signature else None,
+            "constraints": constraints_to_dict(constraints) if constraints else None,
+            **stats,
+        }
+
+        if args.store_json:
+            store_path = Path(args.store_json)
+            store_path.parent.mkdir(parents=True, exist_ok=True)
+            store_path.write_text(json.dumps(runtime.store.to_bundle(
+                root=runtime.root_id,
+                frontier=runtime.scheduler.pending(),
+                scheduler=scheduler_choice,
+                scheduler_seed=getattr(runtime.scheduler, "seed", None),
+                scheduler_state=runtime.scheduler.state() if hasattr(runtime.scheduler, "state") else None,
+                processed=runtime._processed,
+                walk_children=runtime.walk_children,
+                walk_depth=runtime.walk_depth,
+                strict_matching=runtime.strict_matching,
+                rule_budgets=runtime.rule_budgets,
+                rule_budget_exhausted=runtime.rule_budget_exhausted,
+                max_terms=runtime.max_terms,
+                term_limit_exhausted=runtime.term_limit_exhausted,
+                include_rules=runtime.include_rules,
+                exclude_rules=runtime.exclude_rules,
+                include_scales=runtime.include_scales,
+                exclude_scales=runtime.exclude_scales,
+                detect_conflicts=runtime.detect_conflicts,
+                signature=signature.to_dict() if signature else None,
+                constraints=constraints_to_dict(constraints) if constraints else None,
+            ), indent=2))
+
+        print(json.dumps(summary, indent=2))
+        return 0
+    except Exception as exc:  # pragma: no cover - defensive shell entry
+        print(f"nanocode: {exc}", file=sys.stderr)
+        return 1
+    finally:
+        if args.trace_jsonl and sink is not None:
+            sink.close()
+
+
+def main() -> int:  # pragma: no cover - thin wrapper
+    return run_cli()
+
+
+if __name__ == "__main__":  # pragma: no cover - CLI entrypoint
+    sys.exit(main())
diff --git a/nanocode-core/src/constraints.py b/nanocode-core/src/constraints.py
new file mode 100644
index 0000000000000000000000000000000000000000..71d7654a183530cca0d26535df4e3d3884fc3101
--- /dev/null
+++ b/nanocode-core/src/constraints.py
@@ -0,0 +1,124 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from dataclasses import dataclass
+from typing import Iterable, Mapping
+
+from src.terms import Term
+
+
+@dataclass(frozen=True)
+class StructuralMetrics:
+    """Aggregate measurements of a Nanocode term tree.
+
+    These metrics provide a deterministic footprint of a genome's shape and
+    scale usage so evolutionary loops and validators can reason about
+    complexity budgets.
+    """
+
+    nodes: int
+    leaves: int
+    max_depth: int
+    max_fanout: int
+    min_scale: int
+    max_scale: int
+
+
+@dataclass(frozen=True)
+class StructuralConstraints:
+    """Bounds that define a well-formed Nanocode genome structure."""
+
+    max_nodes: int | None = None
+    max_depth: int | None = None
+    max_fanout: int | None = None
+    min_scale: int | None = None
+    max_scale: int | None = None
+
+
+def constraints_to_dict(constraints: StructuralConstraints) -> dict:
+    """Serialize constraints into a JSON-friendly mapping."""
+
+    payload: dict[str, int] = {}
+    if constraints.max_nodes is not None:
+        payload["max_nodes"] = constraints.max_nodes
+    if constraints.max_depth is not None:
+        payload["max_depth"] = constraints.max_depth
+    if constraints.max_fanout is not None:
+        payload["max_fanout"] = constraints.max_fanout
+    if constraints.min_scale is not None:
+        payload["min_scale"] = constraints.min_scale
+    if constraints.max_scale is not None:
+        payload["max_scale"] = constraints.max_scale
+    return payload
+
+
+def constraints_from_dict(data: Mapping[str, object]) -> StructuralConstraints:
+    """Deserialize constraints from a JSON-friendly mapping."""
+
+    return StructuralConstraints(
+        max_nodes=int(data["max_nodes"]) if "max_nodes" in data else None,
+        max_depth=int(data["max_depth"]) if "max_depth" in data else None,
+        max_fanout=int(data["max_fanout"]) if "max_fanout" in data else None,
+        min_scale=int(data["min_scale"]) if "min_scale" in data else None,
+        max_scale=int(data["max_scale"]) if "max_scale" in data else None,
+    )
+
+
+def _walk_terms(root: Term) -> Iterable[tuple[Term, int]]:
+    stack: list[tuple[Term, int]] = [(root, 1)]
+    while stack:
+        term, depth = stack.pop()
+        yield term, depth
+        for child in reversed(term.children):
+            stack.append((child, depth + 1))
+
+
+def measure_structure(root: Term) -> StructuralMetrics:
+    """Compute size/depth/fanout/scale metrics for a term tree."""
+
+    nodes = 0
+    leaves = 0
+    max_depth = 0
+    max_fanout = 0
+    min_scale = root.scale
+    max_scale = root.scale
+
+    for term, depth in _walk_terms(root):
+        nodes += 1
+        if not term.children:
+            leaves += 1
+        max_depth = max(max_depth, depth)
+        max_fanout = max(max_fanout, len(term.children))
+        min_scale = min(min_scale, term.scale)
+        max_scale = max(max_scale, term.scale)
+
+    return StructuralMetrics(
+        nodes=nodes,
+        leaves=leaves,
+        max_depth=max_depth,
+        max_fanout=max_fanout,
+        min_scale=min_scale,
+        max_scale=max_scale,
+    )
+
+
+def validate_structure(root: Term, constraints: StructuralConstraints) -> list[str]:
+    """Return human-readable violations of the provided constraints."""
+
+    metrics = measure_structure(root)
+    violations: list[str] = []
+
+    if constraints.max_nodes is not None and metrics.nodes > constraints.max_nodes:
+        violations.append(f"nodes={metrics.nodes} exceeds max_nodes={constraints.max_nodes}")
+    if constraints.max_depth is not None and metrics.max_depth > constraints.max_depth:
+        violations.append(f"max_depth={metrics.max_depth} exceeds max_depth={constraints.max_depth}")
+    if constraints.max_fanout is not None and metrics.max_fanout > constraints.max_fanout:
+        violations.append(
+            f"max_fanout={metrics.max_fanout} exceeds max_fanout={constraints.max_fanout}"
+        )
+    if constraints.min_scale is not None and metrics.min_scale < constraints.min_scale:
+        violations.append(f"min_scale={metrics.min_scale} below min_scale={constraints.min_scale}")
+    if constraints.max_scale is not None and metrics.max_scale > constraints.max_scale:
+        violations.append(f"max_scale={metrics.max_scale} exceeds max_scale={constraints.max_scale}")
+
+    return violations
diff --git a/nanocode-core/src/evolution.py b/nanocode-core/src/evolution.py
new file mode 100644
index 0000000000000000000000000000000000000000..d546ae57a282974f028d808b46012329c5891448
--- /dev/null
+++ b/nanocode-core/src/evolution.py
@@ -0,0 +1,295 @@
+from __future__ import annotations
+
+import random
+from dataclasses import dataclass
+from typing import Callable, Iterable, List, Sequence, Tuple
+
+from src.constraints import StructuralConstraints, validate_structure
+from src.terms import Term
+
+
+@dataclass(frozen=True)
+class Genome:
+    """A Nanocode genome represented as a root term and optional rule payloads.
+
+    The root term encodes the structure to evolve (e.g., an agent policy tree),
+    while optional annotations can track provenance or scoring metadata without
+    affecting rewrite semantics.
+    """
+
+    root: Term
+    annotations: dict | None = None
+
+
+@dataclass(frozen=True)
+class Evaluation:
+    """An evaluated genome with an explicit fitness score and metadata."""
+
+    genome: Genome
+    score: float
+    info: dict | None = None
+
+
+@dataclass(frozen=True)
+class EvolutionConfig:
+    """Configuration for a deterministic evolutionary loop over Nanocode genomes."""
+
+    population_size: int
+    generations: int = 1
+    mutation_rate: float = 0.5
+    crossover_rate: float = 0.5
+    elitism: int = 1
+    tournament_size: int = 2
+    constraints: StructuralConstraints | None = None
+    violation_penalty: float = float("-inf")
+
+
+def _iter_paths(term: Term, prefix: Tuple[int, ...] | Tuple[int] = ()) -> Iterable[Tuple[Tuple[int, ...], Term]]:
+    yield prefix, term
+    for idx, child in enumerate(term.children):
+        yield from _iter_paths(child, prefix + (idx,))
+
+
+def _replace_subterm(term: Term, path: Sequence[int], new_subterm: Term) -> Term:
+    if not path:
+        return new_subterm
+
+    head, *rest = path
+    children: List[Term] = list(term.children)
+    children[head] = _replace_subterm(children[head], rest, new_subterm)
+    return Term(sym=term.sym, scale=term.scale, children=children)
+
+
+def mutate_symbol(term: Term, symbol_pool: Sequence[str], *, rng: random.Random | None = None) -> Term:
+    """Replace the symbol of a randomly selected node with another from the pool.
+
+    The mutation is deterministic with respect to the provided ``rng`` and
+    leaves scales/children untouched.
+    """
+
+    rng = rng or random.Random()
+    paths = list(_iter_paths(term))
+    if not paths:
+        return term
+
+    path, node = rng.choice(paths)
+    candidates = [sym for sym in symbol_pool if sym != node.sym] or [node.sym]
+    new_sym = rng.choice(candidates)
+    new_node = Term(sym=new_sym, scale=node.scale, children=node.children)
+    return _replace_subterm(term, path, new_node)
+
+
+def mutate_scale(
+    term: Term,
+    *,
+    delta_range: Tuple[int, int] = (-1, 1),
+    min_scale: int = 0,
+    max_scale: int | None = None,
+    rng: random.Random | None = None,
+) -> Term:
+    """Tweak the scale of a random node by a bounded delta.
+
+    The adjusted scale is clamped to ``[min_scale, max_scale]`` when
+    ``max_scale`` is provided.
+    """
+
+    rng = rng or random.Random()
+    paths = list(_iter_paths(term))
+    if not paths:
+        return term
+
+    path, node = rng.choice(paths)
+    delta = rng.randint(delta_range[0], delta_range[1])
+    new_scale = node.scale + delta
+    if max_scale is not None:
+        new_scale = min(new_scale, max_scale)
+    new_scale = max(min_scale, new_scale)
+
+    new_node = Term(sym=node.sym, scale=new_scale, children=node.children)
+    return _replace_subterm(term, path, new_node)
+
+
+def delete_subtree(term: Term, *, rng: random.Random | None = None) -> Term:
+    """Remove a random non-root subtree.
+
+    If the term is a leaf (no deletable children), it is returned unchanged.
+    """
+
+    rng = rng or random.Random()
+    candidates = [(path, node) for path, node in _iter_paths(term) if path]
+    if not candidates:
+        return term
+
+    path, _ = rng.choice(candidates)
+    parent_path, delete_idx = path[:-1], path[-1]
+
+    def _delete(node: Term, remaining: Sequence[int]) -> Term:
+        if not remaining:
+            return node
+        head, *rest = remaining
+        children: List[Term] = list(node.children)
+        if not rest:
+            del children[head]
+        else:
+            children[head] = _delete(children[head], rest)
+        return Term(sym=node.sym, scale=node.scale, children=children)
+
+    return _delete(term, parent_path + (delete_idx,))
+
+
+def insert_subtree(
+    term: Term,
+    spawn: Callable[[Term], Term],
+    *,
+    rng: random.Random | None = None,
+) -> Term:
+    """Insert a spawned subtree as a new child of a random node.
+
+    The ``spawn`` callable receives the chosen parent term, enabling context-
+    aware generation (e.g., matching scale or symbol schemas).
+    """
+
+    rng = rng or random.Random()
+    paths = list(_iter_paths(term))
+    if not paths:
+        return term
+
+    path, parent = rng.choice(paths)
+    new_child = spawn(parent)
+    children: List[Term] = list(parent.children) + [new_child]
+    new_parent = Term(sym=parent.sym, scale=parent.scale, children=children)
+    return _replace_subterm(term, path, new_parent)
+
+
+def crossover_terms(a: Term, b: Term, *, rng: random.Random | None = None) -> Tuple[Term, Term]:
+    """Swap random subtrees between two genomes.
+
+    The operation is deterministic with respect to ``rng`` and returns new
+    roots, leaving the inputs unchanged.
+    """
+
+    rng = rng or random.Random()
+    paths_a = list(_iter_paths(a))
+    paths_b = list(_iter_paths(b))
+    path_a, node_a = rng.choice(paths_a)
+    path_b, node_b = rng.choice(paths_b)
+
+    new_a = _replace_subterm(a, path_a, node_b)
+    new_b = _replace_subterm(b, path_b, node_a)
+    return new_a, new_b
+
+
+def annotate_genome(genome: Genome, **annotations: object) -> Genome:
+    """Return a copy of ``genome`` with merged annotations.
+
+    Existing annotations take lower precedence than the provided keys so that
+    scoring and selection metadata can be layered deterministically without
+    mutating the original payload.
+    """
+
+    merged = {**(genome.annotations or {}), **annotations}
+    return Genome(root=genome.root, annotations=merged)
+
+
+def evaluate_population(
+    population: Sequence[Genome],
+    scorer: Callable[[Genome], float | tuple[float, dict]],
+    *,
+    constraints: StructuralConstraints | None = None,
+    violation_penalty: float | None = float("-inf"),
+) -> list[Evaluation]:
+    """Evaluate a population with a deterministic scorer.
+
+    The scorer may return either a bare float or a ``(score, info)`` tuple to
+    surface auxiliary metadata (e.g., constraint violations or bridge-derived
+    metrics) without affecting ordering.
+    """
+
+    evaluations: list[Evaluation] = []
+    for genome in population:
+        violations = validate_structure(genome.root, constraints) if constraints else []
+
+        result = scorer(genome)
+        if isinstance(result, tuple):
+            score, info = result
+        else:
+            score, info = result, None
+
+        info = {**(info or {})}
+        if violations:
+            info.setdefault("violations", violations)
+            if violation_penalty is not None:
+                score = violation_penalty
+        evaluations.append(Evaluation(genome=genome, score=score, info=info))
+    evaluations.sort(key=lambda ev: ev.score, reverse=True)
+    return evaluations
+
+
+def _tournament_select(evaluations: Sequence[Evaluation], size: int, rng: random.Random) -> Evaluation:
+    """Pick the best of ``size`` random candidates."""
+
+    contenders = [rng.choice(evaluations) for _ in range(size)]
+    return max(contenders, key=lambda ev: ev.score)
+
+
+def evolve_population(
+    population: Sequence[Genome],
+    scorer: Callable[[Genome], float | tuple[float, dict]],
+    mutate: Callable[[Genome, random.Random], Genome],
+    *,
+    crossover: Callable[[Genome, Genome, random.Random], tuple[Genome, Genome]] | None = None,
+    config: EvolutionConfig,
+    rng: random.Random | None = None,
+    on_generation: Callable[[int, Evaluation, list[Evaluation]], None] | None = None,
+) -> list[Evaluation]:
+    """Run a deterministic evolutionary loop and return the final evaluations.
+
+    The loop supports elitism, tournament selection, optional crossover, and
+    per-generation callbacks for logging. All randomness flows through the
+    provided ``rng`` to ensure reproducibility.
+    """
+
+    rng = rng or random.Random()
+    current_population = list(population)
+
+    for generation in range(config.generations):
+        evaluations = evaluate_population(
+            current_population,
+            scorer,
+            constraints=config.constraints,
+            violation_penalty=config.violation_penalty,
+        )
+        best = evaluations[0]
+
+        if on_generation:
+            on_generation(generation, best, evaluations)
+
+        next_population: list[Genome] = []
+        elites = evaluations[: config.elitism]
+        next_population.extend(
+            annotate_genome(elite.genome, score=elite.score, generation=generation) for elite in elites
+        )
+
+        while len(next_population) < config.population_size:
+            parent_a = _tournament_select(evaluations, config.tournament_size, rng)
+            parent_b = _tournament_select(evaluations, config.tournament_size, rng)
+
+            children: list[Genome] = [parent_a.genome]
+            if crossover and rng.random() < config.crossover_rate:
+                child_a, child_b = crossover(parent_a.genome, parent_b.genome, rng)
+                children = [child_a, child_b]
+
+            for child in children:
+                mutated = mutate(child, rng) if rng.random() < config.mutation_rate else child
+                next_population.append(annotate_genome(mutated, parent_score=parent_a.score))
+                if len(next_population) >= config.population_size:
+                    break
+
+        current_population = next_population
+
+    return evaluate_population(
+        current_population,
+        scorer,
+        constraints=config.constraints,
+        violation_penalty=config.violation_penalty,
+    )
diff --git a/nanocode-core/src/interpreter.py b/nanocode-core/src/interpreter.py
new file mode 100644
index 0000000000000000000000000000000000000000..0829c4435dd6a5cc1c7b8b96c3f073d297749563
--- /dev/null
+++ b/nanocode-core/src/interpreter.py
@@ -0,0 +1,221 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Dict, List, Optional
+
+from src.constraints import StructuralConstraints, validate_structure
+from src.signature import Signature
+
+from src.rewrite import Pattern, Rule, conflicting_rules
+from src.runtime import Event, Runtime
+from src.terms import Term
+
+
+@dataclass(frozen=True)
+class Program:
+    """Declarative description of a Nanocode program."""
+
+    name: str
+    root: Term
+    rules: List[Rule]
+    max_steps: int = 256
+    max_terms: Optional[int] = None
+    constraints: Optional[StructuralConstraints] = None
+    signature: Signature | None = None
+
+    def with_root(self, root: Term) -> "Program":
+        """Return a copy of this program using a different root term."""
+
+        return Program(
+            name=self.name,
+            root=root,
+            rules=self.rules,
+            max_steps=self.max_steps,
+            max_terms=self.max_terms,
+            constraints=self.constraints,
+            signature=self.signature,
+        )
+
+
+@dataclass
+class Execution:
+    program: Program
+    root_id: str
+    events: List[Event]
+    snapshot: dict
+    stats: dict
+
+    def materialize_store(self) -> "TermStore":
+        """Rehydrate a ``TermStore`` from the captured runtime snapshot."""
+
+        from src.term_store import TermStore
+
+        records_payload = {
+            term_id: {
+                "sym": record.sym,
+                "scale": record.scale,
+                "children": list(record.children),
+            }
+            for term_id, record in self.snapshot["records"].items()
+        }
+        return TermStore.from_json({"records": records_payload})
+
+    def final_term_id(self) -> str:
+        """Return the last produced term ID (or the original root if no events)."""
+
+        if self.events:
+            return self.events[-1].after
+        return self.root_id
+
+    def materialize_root(self) -> Term:
+        """Materialize the final root term from the captured snapshot."""
+
+        store = self.materialize_store()
+        return store.materialize(self.final_term_id())
+
+
+def _validate_term(term: Term) -> None:
+    if term.scale < 0:
+        raise ValueError(f"Term {term.sym} has negative scale {term.scale}")
+
+    for child in term.children:
+        if abs(child.scale - term.scale) > 1:
+            raise ValueError(
+                f"Invalid scale jump from {term.sym}@{term.scale} to {child.sym}@{child.scale}"
+            )
+        _validate_term(child)
+
+
+def _validate_pattern(pattern: Pattern, signature: Signature | None) -> None:
+    if pattern.scale is not None and pattern.scale < 0:
+        raise ValueError(f"Pattern scale cannot be negative: {pattern.scale}")
+
+    if signature and pattern.sym is not None:
+        entry = signature.get(pattern.sym)
+        if entry is None:
+            raise ValueError(f"Pattern references unknown symbol '{pattern.sym}'")
+        if pattern.scale is not None and entry.allowed_scales is not None:
+            if pattern.scale not in entry.allowed_scales:
+                raise ValueError(
+                    f"Pattern for {pattern.sym} uses disallowed scale {pattern.scale}; "
+                    f"allowed: {sorted(entry.allowed_scales)}"
+                )
+
+
+def validate_program(program: Program, *, signature: Signature | None = None) -> None:
+    """Basic sanity checks to catch malformed programs before execution."""
+
+    sig = signature if signature is not None else program.signature
+
+    if program.max_steps <= 0:
+        raise ValueError("Program max_steps must be positive")
+    if program.max_terms is not None and program.max_terms <= 0:
+        raise ValueError("Program max_terms must be positive when provided")
+
+    seen_rules: set[str] = set()
+    for rule in program.rules:
+        if rule.name in seen_rules:
+            raise ValueError(f"Duplicate rule name: {rule.name}")
+        seen_rules.add(rule.name)
+        _validate_pattern(rule.pattern, sig)
+
+    _validate_term(program.root)
+
+    if sig is not None:
+        sig.validate_tree(program.root)
+
+    if program.constraints:
+        violations = validate_structure(program.root, program.constraints)
+        if violations:
+            joined = "; ".join(violations)
+            raise ValueError(f"Program root violates structural constraints: {joined}")
+
+
+def detect_conflicts(rules: List[Rule]) -> list[tuple[Rule, Rule]]:
+    """Lightweight conflict detection for clearly overlapping patterns."""
+
+    return conflicting_rules(rules)
+
+
+class Interpreter:
+    """Thin orchestration layer around the runtime and scheduler."""
+
+    def run(
+        self,
+        program: Program,
+        until_idle: bool = True,
+        *,
+        walk_children: bool = False,
+        walk_depth: Optional[int] = None,
+        strict_matching: bool = False,
+        rule_budgets: Optional[Dict[str, int]] = None,
+        max_terms: Optional[int] = None,
+        include_rules: Optional[List[str]] = None,
+        exclude_rules: Optional[List[str]] = None,
+        include_scales: Optional[List[int]] = None,
+        exclude_scales: Optional[List[int]] = None,
+        detect_conflicts: bool = False,
+        signature: Signature | None = None,
+    ) -> Execution:
+        sig = signature if signature is not None else program.signature
+        validate_program(program, signature=sig)
+
+        if include_rules and exclude_rules:
+            overlap = set(include_rules) & set(exclude_rules)
+            if overlap:
+                raise ValueError(f"Rules cannot be both included and excluded: {sorted(overlap)}")
+        if include_scales and exclude_scales:
+            overlap = set(include_scales) & set(exclude_scales)
+            if overlap:
+                raise ValueError(f"Scales cannot be both included and excluded: {sorted(overlap)}")
+
+        rule_names = {rule.name for rule in program.rules}
+        if include_rules:
+            missing = set(include_rules) - rule_names
+            if missing:
+                raise ValueError(f"Included rules not found: {sorted(missing)}")
+        if exclude_rules:
+            missing = set(exclude_rules) - rule_names
+            if missing:
+                raise ValueError(f"Excluded rules not found: {sorted(missing)}")
+        for scale in (include_scales or []) + (exclude_scales or []):
+            if scale < 0:
+                raise ValueError("Scale filters must be non-negative")
+
+        if detect_conflicts:
+            conflicts = conflicting_rules(program.rules)
+            if conflicts:
+                details = ", ".join(f"{a.name}/{b.name}" for a, b in conflicts)
+                raise ValueError(f"Conflicting rule patterns detected: {details}")
+
+        limit = max_terms if max_terms is not None else program.max_terms
+        runtime = Runtime(
+            program.rules,
+            walk_children=walk_children,
+            walk_depth=walk_depth,
+            strict_matching=strict_matching,
+            rule_budgets=rule_budgets,
+            max_terms=limit,
+            include_rules=include_rules,
+            exclude_rules=exclude_rules,
+            include_scales=include_scales,
+            exclude_scales=exclude_scales,
+            detect_conflicts=detect_conflicts,
+            signature=sig,
+            constraints=program.constraints,
+        )
+        root_id = runtime.load(program.root)
+
+        if until_idle:
+            events = runtime.run_until_idle(max_steps=program.max_steps)
+        else:
+            events = runtime.run(max_steps=program.max_steps)
+
+        return Execution(
+            program=program,
+            root_id=root_id,
+            events=events,
+            snapshot=runtime.snapshot(),
+            stats=runtime.stats(),
+        )
+
diff --git a/nanocode-core/src/meta.py b/nanocode-core/src/meta.py
new file mode 100644
index 0000000000000000000000000000000000000000..e1b0aa2500fe3d8edc9042963cf634456c5bee13
--- /dev/null
+++ b/nanocode-core/src/meta.py
@@ -0,0 +1,322 @@
+from __future__ import annotations
+
+from typing import Callable, Iterable, Optional
+
+from src.constraints import StructuralConstraints
+from src.interpreter import Program, validate_program
+from src.rewrite import Action, Pattern, Rule, action_from_spec
+from src.signature import Signature, TermSignature
+from src.terms import Term
+
+
+def _child_by_sym(term: Term, sym: str) -> Optional[Term]:
+    for child in term.children:
+        if child.sym == sym:
+            return child
+    return None
+
+
+def _value_to_term(value: object) -> Term:
+    if isinstance(value, (int, float, bool)):
+        return Term(sym=str(value))
+    if isinstance(value, str):
+        return Term(sym=value)
+    raise TypeError(f"Unsupported parameter type for meta serialization: {type(value)}")
+
+
+def _value_from_term(term: Term) -> object:
+    if term.children:
+        raise ValueError("Value terms must not have children")
+    text = term.sym
+    try:
+        return int(text)
+    except ValueError:
+        pass
+    try:
+        return float(text)
+    except ValueError:
+        pass
+    if text in {"True", "False"}:
+        return text == "True"
+    return text
+
+
+def pattern_to_term(pattern: Pattern) -> Term:
+    return Term(
+        sym="pattern",
+        children=[
+            Term(sym="sym", children=[Term(sym=pattern.sym)]) if pattern.sym is not None else Term(sym="sym"),
+            Term(sym="scale", children=[Term(sym=str(pattern.scale))]) if pattern.scale is not None else Term(sym="scale"),
+        ],
+    )
+
+
+def term_to_pattern(term: Term) -> Pattern:
+    if term.sym != "pattern":
+        raise ValueError(f"Expected pattern term, got {term.sym}")
+
+    sym_child = _child_by_sym(term, "sym")
+    scale_child = _child_by_sym(term, "scale")
+
+    sym_value = sym_child.children[0].sym if sym_child and sym_child.children else None
+    scale_value = None
+    if scale_child and scale_child.children:
+        scale_value = int(_value_from_term(scale_child.children[0]))
+
+    return Pattern(sym=sym_value, scale=scale_value)
+
+
+def action_to_term(action: Action) -> Term:
+    if not isinstance(action, Action):
+        raise TypeError("Only Action instances can be serialized to terms")
+
+    param_terms = [
+        Term(sym=key, children=[_value_to_term(value)]) for key, value in sorted(action.params.items())
+    ]
+
+    return Term(
+        sym="action",
+        children=[
+            Term(sym="name", children=[Term(sym=action.name)]),
+            Term(sym="params", children=param_terms),
+        ],
+    )
+
+
+def term_to_action(
+    term: Term, summarizers: dict[str, Callable[[list[Term]], str]] | None = None
+) -> Action:
+    if term.sym != "action":
+        raise ValueError(f"Expected action term, got {term.sym}")
+
+    name_term = _child_by_sym(term, "name")
+    params_term = _child_by_sym(term, "params")
+    if not name_term or not name_term.children:
+        raise ValueError("Action term missing name child")
+
+    name = name_term.children[0].sym
+    params: dict[str, object] = {}
+    if params_term:
+        for param in params_term.children:
+            if not param.children:
+                continue
+            params[param.sym] = _value_from_term(param.children[0])
+
+    return action_from_spec(name, params, summarizers=summarizers)
+
+
+def _constraints_to_term(constraints: StructuralConstraints) -> Term:
+    children: list[Term] = []
+    if constraints.max_nodes is not None:
+        children.append(Term(sym="max_nodes", children=[_value_to_term(constraints.max_nodes)]))
+    if constraints.max_depth is not None:
+        children.append(Term(sym="max_depth", children=[_value_to_term(constraints.max_depth)]))
+    if constraints.max_fanout is not None:
+        children.append(Term(sym="max_fanout", children=[_value_to_term(constraints.max_fanout)]))
+    if constraints.min_scale is not None:
+        children.append(Term(sym="min_scale", children=[_value_to_term(constraints.min_scale)]))
+    if constraints.max_scale is not None:
+        children.append(Term(sym="max_scale", children=[_value_to_term(constraints.max_scale)]))
+
+    return Term(sym="constraints", children=children)
+
+
+def _constraints_from_term(term: Term) -> StructuralConstraints:
+    if term.sym != "constraints":
+        raise ValueError(f"Expected constraints term, got {term.sym}")
+
+    kwargs: dict[str, int | None] = {
+        "max_nodes": None,
+        "max_depth": None,
+        "max_fanout": None,
+        "min_scale": None,
+        "max_scale": None,
+    }
+
+    for child in term.children:
+        if not child.children:
+            continue
+        kwargs[child.sym] = int(_value_from_term(child.children[0]))
+
+    return StructuralConstraints(**kwargs)
+
+
+def _signature_to_term(signature: Signature) -> Term:
+    entries: list[Term] = []
+    for sym, entry in signature.items():
+        entry_children = [
+            Term(sym="name", children=[Term(sym=sym)]),
+            Term(sym="min_children", children=[_value_to_term(entry.min_children)]),
+        ]
+        if entry.max_children is not None:
+            entry_children.append(Term(sym="max_children", children=[_value_to_term(entry.max_children)]))
+        else:
+            entry_children.append(Term(sym="max_children"))
+
+        if entry.allowed_scales is not None:
+            entry_children.append(
+                Term(sym="scales", children=[_value_to_term(scale) for scale in sorted(entry.allowed_scales)])
+            )
+        else:
+            entry_children.append(Term(sym="scales"))
+
+        entries.append(Term(sym="symbol", children=entry_children))
+
+    return Term(sym="signature", children=entries)
+
+
+def _signature_from_term(term: Term) -> Signature:
+    if term.sym != "signature":
+        raise ValueError(f"Expected signature term, got {term.sym}")
+
+    entries: list[TermSignature] = []
+    for child in term.children:
+        if child.sym != "symbol":
+            continue
+        name_child = _child_by_sym(child, "name")
+        if not name_child or not name_child.children:
+            raise ValueError("Signature entry missing symbol name")
+        min_children_child = _child_by_sym(child, "min_children")
+        max_children_child = _child_by_sym(child, "max_children")
+        scales_child = _child_by_sym(child, "scales")
+
+        allowed_scales = None
+        if scales_child and scales_child.children:
+            allowed_scales = {int(_value_from_term(scale_term)) for scale_term in scales_child.children}
+
+        entries.append(
+            TermSignature(
+                sym=name_child.children[0].sym,
+                min_children=int(_value_from_term(min_children_child.children[0])) if min_children_child and min_children_child.children else 0,
+                max_children=(
+                    int(_value_from_term(max_children_child.children[0]))
+                    if max_children_child and max_children_child.children
+                    else None
+                ),
+                allowed_scales=allowed_scales,
+            )
+        )
+
+    return Signature(entries)
+
+
+def rule_to_term(rule: Rule) -> Term:
+    return Term(
+        sym="rule",
+        children=[
+            Term(sym="name", children=[Term(sym=rule.name)]),
+            pattern_to_term(rule.pattern),
+            action_to_term(rule.action),
+        ],
+    )
+
+
+def term_to_rule(
+    term: Term, summarizers: dict[str, Callable[[list[Term]], str]] | None = None
+) -> Rule:
+    if term.sym != "rule":
+        raise ValueError(f"Expected rule term, got {term.sym}")
+
+    name_child = _child_by_sym(term, "name")
+    if not name_child or not name_child.children:
+        raise ValueError("Rule term missing name child")
+
+    pattern_child = next((c for c in term.children if c.sym == "pattern"), None)
+    action_child = next((c for c in term.children if c.sym == "action"), None)
+    if pattern_child is None or action_child is None:
+        raise ValueError("Rule term missing pattern or action")
+
+    return Rule(
+        name=name_child.children[0].sym,
+        pattern=term_to_pattern(pattern_child),
+        action=term_to_action(action_child, summarizers=summarizers),
+    )
+
+
+def program_to_term(program: Program) -> Term:
+    rules_term = Term(sym="rules", children=[rule_to_term(rule) for rule in program.rules])
+    config_children = [
+        Term(sym="name", children=[Term(sym=program.name)]),
+        Term(sym="root", children=[program.root]),
+        rules_term,
+        Term(sym="max_steps", children=[Term(sym=str(program.max_steps))]),
+        Term(sym="max_terms", children=[Term(sym=str(program.max_terms))]) if program.max_terms is not None else Term(sym="max_terms"),
+    ]
+
+    if program.constraints is not None:
+        config_children.append(_constraints_to_term(program.constraints))
+    if program.signature is not None:
+        config_children.append(_signature_to_term(program.signature))
+
+    return Term(sym="program", children=config_children)
+
+
+def term_to_program(
+    term: Term, summarizers: dict[str, Callable[[list[Term]], str]] | None = None
+) -> Program:
+    if term.sym != "program":
+        raise ValueError(f"Expected program term, got {term.sym}")
+
+    name_child = _child_by_sym(term, "name")
+    root_child = _child_by_sym(term, "root")
+    rules_child = _child_by_sym(term, "rules")
+    steps_child = _child_by_sym(term, "max_steps")
+    max_terms_child = _child_by_sym(term, "max_terms")
+    constraints_child = _child_by_sym(term, "constraints")
+    signature_child = _child_by_sym(term, "signature")
+
+    if not name_child or not name_child.children:
+        raise ValueError("Program term missing name")
+    if not root_child or not root_child.children:
+        raise ValueError("Program term missing root")
+    if not rules_child:
+        raise ValueError("Program term missing rules")
+
+    rules = [term_to_rule(rule_term, summarizers=summarizers) for rule_term in rules_child.children]
+    max_steps = int(_value_from_term(steps_child.children[0])) if steps_child and steps_child.children else 256
+    max_terms = None
+    if max_terms_child and max_terms_child.children:
+        max_terms = int(_value_from_term(max_terms_child.children[0]))
+
+    constraints = _constraints_from_term(constraints_child) if constraints_child else None
+    signature = _signature_from_term(signature_child) if signature_child else None
+
+    program = Program(
+        name=name_child.children[0].sym,
+        root=root_child.children[0],
+        rules=rules,
+        max_steps=max_steps,
+        max_terms=max_terms,
+        constraints=constraints,
+        signature=signature,
+    )
+    validate_program(program)
+    return program
+
+
+def rules_to_term(rules: Iterable[Rule]) -> Term:
+    return Term(sym="rules", children=[rule_to_term(rule) for rule in rules])
+
+
+def term_to_rules(
+    term: Term, summarizers: dict[str, Callable[[list[Term]], str]] | None = None
+) -> list[Rule]:
+    if term.sym != "rules":
+        raise ValueError(f"Expected rules term, got {term.sym}")
+    return [term_to_rule(child, summarizers=summarizers) for child in term.children]
+
+
+def constraints_to_term(constraints: StructuralConstraints) -> Term:
+    return _constraints_to_term(constraints)
+
+
+def term_to_constraints(term: Term) -> StructuralConstraints:
+    return _constraints_from_term(term)
+
+
+def signature_to_term(signature: Signature) -> Term:
+    return _signature_to_term(signature)
+
+
+def term_to_signature(term: Term) -> Signature:
+    return _signature_from_term(term)
diff --git a/nanocode-core/src/pipeline.py b/nanocode-core/src/pipeline.py
index b763903651ac9cfde616a40eb7cafa5bc21a3ed3..2898df33da94462f03ba695890c0d7c00b2b5cba 100644
--- a/nanocode-core/src/pipeline.py
+++ b/nanocode-core/src/pipeline.py
@@ -1,23 +1,61 @@
-from typing import List, Dict, Any
-
-def micro_layer(data: str) -> List[str]:
-    return list(data)
-
-def meso_layer(tokens: List[str]) -> Dict[str, Any]:
-    motifs = {}
-    for i in range(len(tokens) - 1):
-        bg = tokens[i] + tokens[i+1]
-        motifs[bg] = motifs.get(bg, 0) + 1
-    return {"bigrams": motifs}
-
-def macro_layer(motifs: Dict[str, Any]) -> str:
-    bigrams = motifs.get("bigrams", {})
-    if not bigrams:
-        return "empty"
-    dominant = max(bigrams.items(), key=lambda kv: kv[1])[0]
-    return f"dominant={dominant}"
-
-def run_pipeline(raw: str) -> str:
-    atoms = micro_layer(raw)
-    motifs = meso_layer(atoms)
-    return macro_layer(motifs)
+"""Pipeline expressed purely as Nanocode terms and rewrite rules.
+
+This module replaces the ad hoc Python pipeline with a Program that
+materializes micro/meso/macro terms and runs through the runtime so the
+entire workflow is traceable and evolvable.
+"""
+
+from __future__ import annotations
+
+from collections import Counter
+from typing import List
+
+from src.interpreter import Program
+from src.rewrite import Pattern, Rule, expand_action, reduce_action
+from src.terms import Term
+
+
+def _tokenize(text: str) -> List[Term]:
+    return [Term(sym=ch, scale=0) for ch in text]
+
+
+def _meso_summary(children: List[Term]) -> str:
+    def _normalize(sym: str) -> str:
+        if sym.startswith("motif[") and sym.endswith("]"):
+            payload = sym[len("motif[") : -1]
+            return payload.replace("|", "")
+        return sym
+
+    counts = Counter(_normalize(child.sym) for child in children)
+    if not counts:
+        return "empty"
+    dominant, freq = counts.most_common(1)[0]
+    return f"dominant={dominant};freq={freq}"
+
+
+def make_text_program(text: str) -> Program:
+    """Construct a multi-scale program that mirrors the original pipeline."""
+
+    root = Term(sym="text", scale=0, children=_tokenize(text))
+
+    def is_micro(term: Term) -> bool:
+        return term.sym == "text" and term.scale == 0
+
+    def is_meso(term: Term) -> bool:
+        return term.sym.startswith("F(text") or (term.sym.startswith("expand[text") and term.scale == 1)
+
+    rules = [
+        Rule(name="to-meso", pattern=Pattern(predicate=is_micro), action=expand_action(fanout=2)),
+        Rule(name="to-macro", pattern=Pattern(predicate=is_meso), action=reduce_action(summarizer=_meso_summary)),
+    ]
+
+    return Program(name="text-pipeline", root=root, rules=rules, max_steps=32)
+
+
+def run_pipeline(text: str) -> Term:
+    from src.interpreter import Interpreter
+
+    program = make_text_program(text)
+    execution = Interpreter().run(program)
+    return execution.materialize_root()
+
diff --git a/nanocode-core/src/prototype.py b/nanocode-core/src/prototype.py
new file mode 100644
index 0000000000000000000000000000000000000000..4e91dea8338b3e22f72c0c1233f433d13d0bdb37
--- /dev/null
+++ b/nanocode-core/src/prototype.py
@@ -0,0 +1,150 @@
+"""Prototype demo runner to exercise micro/meso/macro flows end-to-end."""
+
+from __future__ import annotations
+
+import argparse
+import json
+from collections import Counter
+from pathlib import Path
+from typing import Callable, Iterable, Tuple
+
+from src.bridge import BridgeBinding, BridgePort, BridgeSchema, bridge_call_action, validate_bridge_schema
+from src.interpreter import Execution, Interpreter, Program, validate_program
+from src.rewrite import Pattern, Rule, expand_action, lift_action, reduce_action
+from src.runtime import Runtime
+from src.term_store import TermStore
+from src.terms import Term, term_to_dict
+from src.trace import JSONLTracer
+
+
+VOWELS = set("aeiouAEIOU")
+
+
+def _leaves(term: Term) -> Iterable[Term]:
+    stack = [term]
+    while stack:
+        node = stack.pop()
+        if not node.children:
+            yield node
+            continue
+        stack.extend(reversed(node.children))
+
+
+def _motif_summary(children: list[Term]) -> str:
+    """Summarize motif payloads for reduction provenance."""
+
+    labels = [child.sym for child in children]
+    counts = Counter(labels)
+    dominant, freq = counts.most_common(1)[0]
+    return f"dominant={dominant};freq={freq};span={len(labels)}"
+
+
+def _macro_binding(text: str) -> Tuple[BridgeBinding, Callable[[Term], object]]:
+    schema = validate_bridge_schema(
+        BridgeSchema(
+            name="prototype-macro",
+            ports=(BridgePort(name="macro", direction="out", scale=2, description="macro label"),),
+            metadata={"demo": True},
+        )
+    )
+
+    vowels = sum(1 for ch in text if ch in VOWELS)
+    consonants = len(text) - vowels
+
+    def _macro_label(_term: Term) -> str:
+        if not text:
+            return "empty"
+        ratio = vowels / len(text)
+        balance = "vowel-heavy" if ratio > 0.6 else "consonant-heavy" if ratio < 0.4 else "mixed"
+        return f"{balance};len={len(text)};v={vowels};c={consonants}"
+
+    def _encode(payload: object) -> Term:
+        port = schema.port("macro")
+        return Term(sym=str(payload), scale=port.scale or 0)
+
+    binding = BridgeBinding(schema=schema, encode={"macro": _encode}, decode={"macro": lambda t: t.sym})
+    return binding, _macro_label
+
+
+def _tokenize(text: str) -> list[Term]:
+    return [Term(sym=ch, scale=0) for ch in text]
+
+
+def make_prototype_program(text: str) -> Program:
+    """Build a deterministic micro→meso→macro program for the demo."""
+
+    binding, oracle = _macro_binding(text)
+    root = Term(sym="text", scale=0, children=_tokenize(text))
+
+    def has_summary(term: Term) -> bool:
+        return any(child.sym.startswith("summary:") for child in term.children)
+
+    def is_expand_target(term: Term) -> bool:
+        return term.sym == "text" and term.scale == 0 and not has_summary(term)
+
+    rules = [
+        Rule(name="expand-meso", pattern=Pattern(predicate=is_expand_target), action=expand_action(fanout=3)),
+        Rule(name="reduce-text", pattern=Pattern(predicate=lambda t: t.sym.startswith("F(text")), action=reduce_action(summarizer=_motif_summary)),
+        Rule(name="lift-summary", pattern=Pattern(predicate=has_summary), action=lift_action()),
+        Rule(name="classify", pattern=Pattern(predicate=lambda t: t.sym.startswith("lift[text")), action=bridge_call_action(binding, "macro", oracle)),
+    ]
+
+    program = Program(name="prototype-demo", root=root, rules=rules, max_steps=128)
+    validate_program(program)
+    return program
+
+
+def run_prototype(text: str, trace: Path | None = None, store: Path | None = None) -> Execution:
+    program = make_prototype_program(text)
+    runtime = Runtime(program.rules, walk_children=True, detect_conflicts=True)
+    trace_sink = None
+    if trace:
+        trace_sink = trace.open("w", encoding="utf-8")
+        runtime.event_hooks.append(JSONLTracer(trace_sink))
+
+    root_id = runtime.load(program.root)
+    runtime.run_until_idle(max_steps=program.max_steps)
+
+    snapshot = runtime.snapshot()
+    stats = runtime.stats()
+    if trace_sink:
+        trace_sink.close()
+
+    if store:
+        payload = {
+            "store": runtime.store.to_json(),
+            "root": snapshot["root"],
+            "frontier": snapshot["frontier"],
+            "processed": list(snapshot["processed"]),
+        }
+        store.write_text(json.dumps(payload, indent=2), encoding="utf-8")
+
+    return Execution(program=program, root_id=root_id, events=list(runtime.events), snapshot=snapshot, stats=stats)
+
+
+def _cli() -> None:
+    parser = argparse.ArgumentParser(description="Run the Nanocode prototype demo end-to-end")
+    parser.add_argument("--text", required=True, help="Input text to classify")
+    parser.add_argument("--trace-jsonl", type=Path, help="Optional JSONL trace output path")
+    parser.add_argument("--store-json", type=Path, help="Optional term store snapshot path")
+    args = parser.parse_args()
+
+    execution = run_prototype(args.text, trace=args.trace_jsonl, store=args.store_json)
+    final_term = execution.materialize_root()
+
+    macro = next(child for child in final_term.children if child.sym.startswith("port:out:macro"))
+    macro_label = macro.children[0].sym
+
+    summary = {
+        "program": execution.program.name,
+        "input": args.text,
+        "final_term": term_to_dict(final_term),
+        "macro_label": macro_label,
+        "trace": str(args.trace_jsonl) if args.trace_jsonl else None,
+        "store": str(args.store_json) if args.store_json else None,
+    }
+    print(json.dumps(summary, indent=2))
+
+
+if __name__ == "__main__":  # pragma: no cover - CLI entry
+    _cli()
diff --git a/nanocode-core/src/rewrite.py b/nanocode-core/src/rewrite.py
new file mode 100644
index 0000000000000000000000000000000000000000..be7fd739d141094f3580cc133672210b3fe87ba9
--- /dev/null
+++ b/nanocode-core/src/rewrite.py
@@ -0,0 +1,139 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Callable, Iterable, Optional
+
+from src.term_store import TermStore
+from src.terms import Term, expand, reduce
+
+
+@dataclass(frozen=True)
+class Action:
+    """Named, metadata-carrying rewrite action.
+
+    Actions remain regular callables while exposing a stable `name` and
+    structured `params` mapping so they can be serialized into Nanocode
+    terms and round-tripped as meta-level data. This keeps rewrite
+    semantics deterministic while making rule sets representable as data
+    for evolution or self-hosted interpreters.
+    """
+
+    name: str
+    params: dict[str, object]
+    fn: Callable[[Term, TermStore], Term]
+
+    def __call__(self, term: Term, store: TermStore) -> Term:  # pragma: no cover - thin wrapper
+        return self.fn(term, store)
+
+
+def expand_action(fanout: int = 3) -> Action:
+    return Action(name="expand", params={"fanout": fanout}, fn=lambda term, store: expand(term, fanout=fanout))
+
+
+def reduce_action(summarizer: Callable[[list[Term]], str] | None = None) -> Action:
+    params = {"summarizer": summarizer.__name__} if summarizer else {}
+    return Action(
+        name="reduce",
+        params=params,
+        fn=lambda term, store: reduce(term, summarizer=summarizer),
+    )
+
+
+def lift_action() -> Action:
+    """Raise a term to the next scale while preserving its subtree."""
+
+    def _lift(term: Term, _store: TermStore) -> Term:
+        return Term(sym=f"lift[{term.sym}]", scale=term.scale + 1, children=[term])
+
+    return Action(name="lift", params={}, fn=_lift)
+
+
+def action_from_spec(
+    name: str,
+    params: dict[str, object],
+    summarizers: dict[str, Callable[[list[Term]], str]] | None = None,
+) -> Action:
+    if name == "expand":
+        fanout = int(params.get("fanout", 3))
+        return expand_action(fanout=fanout)
+    if name == "reduce":
+        summarizer_name = params.get("summarizer")
+        if summarizer_name is None:
+            return reduce_action()
+        if summarizers and summarizer_name in summarizers:
+            return reduce_action(summarizer=summarizers[summarizer_name])
+        raise ValueError(f"Unknown summarizer '{summarizer_name}' for reduce action")
+    if name == "lift":
+        return lift_action()
+    raise ValueError(f"Unknown action spec: {name}")
+
+
+@dataclass(frozen=True)
+class Pattern:
+    sym: Optional[str] = None
+    scale: Optional[int] = None
+    predicate: Optional[Callable[[Term], bool]] = None
+
+    def matches(self, term: Term) -> bool:
+        if self.sym is not None and term.sym != self.sym:
+            return False
+        if self.scale is not None and term.scale != self.scale:
+            return False
+        if self.predicate and not self.predicate(term):
+            return False
+        return True
+
+
+@dataclass(frozen=True)
+class Rule:
+    name: str
+    pattern: Pattern
+    action: Callable[[Term, TermStore], Term]
+
+    def applies(self, term: Term) -> bool:
+        return self.pattern.matches(term)
+
+
+class AmbiguousRuleError(Exception):
+    def __init__(self, term: Term, rules: Iterable[Rule]):
+        rule_names = ", ".join(rule.name for rule in rules)
+        super().__init__(f"ambiguous match for term {term.sym} at scale {term.scale}: {rule_names}")
+        self.term = term
+        self.rules = list(rules)
+
+
+def matching_rules(rules: list[Rule], term: Term) -> list[Rule]:
+    return [rule for rule in rules if rule.applies(term)]
+
+
+def first_match(rules: list[Rule], term: Term) -> Optional[Rule]:
+    matches = matching_rules(rules, term)
+    return matches[0] if matches else None
+
+
+def conflicting_rules(rules: list[Rule]) -> list[tuple[Rule, Rule]]:
+    """Identify rule pairs that deterministically overlap.
+
+    Today we only flag conflicts when both rules target the same symbol and
+    scale without predicates. Broader confluence checking is left for future
+    work but this provides a lightweight guardrail against accidental
+    ambiguity.
+    """
+
+    conflicts: list[tuple[Rule, Rule]] = []
+    seen: dict[tuple[str, int], Rule] = {}
+    for rule in rules:
+        pattern = rule.pattern
+        if pattern.predicate is not None:
+            continue
+        if pattern.sym is None or pattern.scale is None:
+            continue
+
+        key = (pattern.sym, pattern.scale)
+        other = seen.get(key)
+        if other is None:
+            seen[key] = rule
+            continue
+        conflicts.append((other, rule))
+
+    return conflicts
diff --git a/nanocode-core/src/runtime.py b/nanocode-core/src/runtime.py
new file mode 100644
index 0000000000000000000000000000000000000000..9171c15becafb189f28bf1fdef3625174f69ef87
--- /dev/null
+++ b/nanocode-core/src/runtime.py
@@ -0,0 +1,422 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Callable, Dict, List, Mapping, Optional
+
+from src.constraints import (
+    StructuralConstraints,
+    constraints_from_dict,
+    constraints_to_dict,
+    validate_structure,
+)
+from src.rewrite import AmbiguousRuleError, Rule, conflicting_rules, matching_rules
+from src.scheduler import FIFOScheduler, LIFOScheduler, RandomScheduler
+from src.signature import Signature, SignatureError
+from src.term_store import TermStore
+from src.terms import Term, term_to_dict
+
+
+@dataclass
+class Event:
+    before: str
+    after: str
+    rule: str
+    scale: int
+    before_term: Term
+    after_term: Term
+
+    def as_dict(self) -> Dict[str, object]:  # pragma: no cover - convenience helper
+        return {
+            "before": self.before,
+            "after": self.after,
+            "rule": self.rule,
+            "scale": self.scale,
+            "before_term": self.before_term,
+            "after_term": self.after_term,
+        }
+
+    def to_record(self) -> Dict[str, object]:
+        """JSON-ready event representation for tracing."""
+
+        return {
+            "before": self.before,
+            "after": self.after,
+            "rule": self.rule,
+            "scale": self.scale,
+            "before_term": term_to_dict(self.before_term),
+            "after_term": term_to_dict(self.after_term),
+        }
+
+
+class Runtime:
+    """Minimal stepping runtime for Nanocode rewrites."""
+
+    def __init__(
+        self,
+        rules: List[Rule],
+        scheduler: Optional[FIFOScheduler | LIFOScheduler | RandomScheduler] = None,
+        event_hooks: Optional[List[Callable[[Event], None]]] = None,
+        walk_children: bool = False,
+        walk_depth: Optional[int] = None,
+        strict_matching: bool = False,
+        rule_budgets: Optional[Dict[str, int]] = None,
+        max_terms: Optional[int] = None,
+        include_rules: Optional[List[str]] = None,
+        exclude_rules: Optional[List[str]] = None,
+        include_scales: Optional[List[int]] = None,
+        exclude_scales: Optional[List[int]] = None,
+        detect_conflicts: bool = False,
+        signature: Signature | None = None,
+        constraints: StructuralConstraints | None = None,
+    ):
+        if max_terms is not None and max_terms <= 0:
+            raise ValueError("max_terms must be positive when provided")
+        if walk_depth is not None and walk_depth <= 0:
+            raise ValueError("walk_depth must be positive when provided")
+        if include_rules and exclude_rules:
+            overlap = set(include_rules) & set(exclude_rules)
+            if overlap:
+                raise ValueError(f"Rules cannot be both included and excluded: {sorted(overlap)}")
+        if include_scales and exclude_scales:
+            overlap = set(include_scales) & set(exclude_scales)
+            if overlap:
+                raise ValueError(f"Scales cannot be both included and excluded: {sorted(overlap)}")
+        for scale in (include_scales or []) + (exclude_scales or []):
+            if scale < 0:
+                raise ValueError("Scales must be non-negative")
+        if detect_conflicts:
+            conflicts = conflicting_rules(rules)
+            if conflicts:
+                details = ", ".join(f"{a.name}/{b.name}" for a, b in conflicts)
+                raise ValueError(f"Conflicting rule patterns detected: {details}")
+
+        self.store = TermStore()
+        self.rules = rules
+        self.scheduler = scheduler if scheduler is not None else FIFOScheduler()
+        self.event_hooks: List[Callable[[Event], None]] = event_hooks or []
+        self.events: List[Event] = []
+        self.root_id: Optional[str] = None
+        self._processed: set[str] = set()
+        self._queued: set[str] = set()
+        self.walk_children = walk_children
+        self.walk_depth = walk_depth
+        self.strict_matching = strict_matching
+        self.rule_counts: Dict[str, int] = {}
+        self.scale_counts: Dict[int, int] = {}
+        self.exhausted_budget: bool = False
+        self.rule_budgets: Dict[str, int] = dict(rule_budgets) if rule_budgets else {}
+        self.rule_budget_exhausted: set[str] = set()
+        self.max_terms = max_terms
+        self.term_limit_exhausted = False
+        self.include_rules = set(include_rules) if include_rules else None
+        self.exclude_rules = set(exclude_rules) if exclude_rules else set()
+        self.include_scales = set(include_scales) if include_scales else None
+        self.exclude_scales = set(exclude_scales) if exclude_scales else set()
+        self.detect_conflicts = detect_conflicts
+        self.signature = signature
+        self.constraints = constraints
+        self.constraint_violations: list[str] = []
+        self.constraint_exhausted: bool = False
+
+    def _reset_state(self) -> None:
+        self.events.clear()
+        self._processed.clear()
+        self._queued.clear()
+        self.scheduler.clear()
+        self.rule_counts.clear()
+        self.scale_counts.clear()
+        self.exhausted_budget = False
+        self.rule_budget_exhausted.clear()
+        self.term_limit_exhausted = False
+        self.constraint_violations.clear()
+        self.constraint_exhausted = False
+
+    def load(self, root: Term) -> str:
+        # Reset state for a fresh program load
+        self.store = TermStore()
+        self._reset_state()
+
+        if self.signature is not None:
+            self.signature.validate_tree(root)
+
+        self.root_id = self.store.add_term(root)
+        self._check_term_limit()
+        self._schedule_tree(self.root_id)
+        self._check_constraints(self.store.materialize(self.root_id))
+        return self.root_id
+
+    def load_state(
+        self,
+        *,
+        store: TermStore,
+        root_id: str,
+        frontier: Optional[List[str]] = None,
+        processed: Optional[List[str]] = None,
+        rule_budgets: Optional[Dict[str, int]] = None,
+        rule_budget_exhausted: Optional[List[str]] = None,
+        scheduler_state: object | None = None,
+        include_rules: Optional[List[str]] = None,
+        exclude_rules: Optional[List[str]] = None,
+        include_scales: Optional[List[int]] = None,
+        exclude_scales: Optional[List[int]] = None,
+        detect_conflicts: bool | None = None,
+        constraints: StructuralConstraints | Mapping[str, object] | None = None,
+    ) -> str:
+        """Restore runtime state from a serialized snapshot."""
+
+        def _coerce_state(payload: object | None) -> object | None:
+            if isinstance(payload, list):
+                return tuple(_coerce_state(item) for item in payload)
+            if isinstance(payload, tuple):
+                return tuple(_coerce_state(item) for item in payload)
+            return payload
+
+        self.store = store
+        self._reset_state()
+
+        if self.signature is not None:
+            self.signature.validate_tree(self.store.materialize(root_id))
+
+        if rule_budgets is not None:
+            self.rule_budgets = dict(rule_budgets)
+        if rule_budget_exhausted is not None:
+            self.rule_budget_exhausted = set(rule_budget_exhausted)
+        if include_rules is not None:
+            self.include_rules = set(include_rules)
+        if exclude_rules is not None:
+            self.exclude_rules = set(exclude_rules)
+        if include_scales is not None:
+            self.include_scales = set(include_scales)
+        if exclude_scales is not None:
+            self.exclude_scales = set(exclude_scales)
+        if detect_conflicts is not None:
+            self.detect_conflicts = detect_conflicts
+        if constraints is not None:
+            self.constraints = (
+                constraints
+                if isinstance(constraints, StructuralConstraints)
+                else constraints_from_dict(constraints)
+            )
+
+        if root_id not in self.store:
+            raise KeyError(f"Root term {root_id} not found in store")
+
+        self.root_id = root_id
+        if processed:
+            self._processed = set(processed)
+
+        if frontier:
+            for term_id in frontier:
+                if term_id not in self.store:
+                    raise KeyError(f"Frontier term {term_id} not found in store")
+                self._schedule_term(term_id)
+        else:
+            self._schedule_tree(self.root_id)
+
+        if scheduler_state is not None and hasattr(self.scheduler, "set_state"):
+            self.scheduler.set_state(_coerce_state(scheduler_state))
+
+        self._check_term_limit()
+        self._check_constraints(self.store.materialize(self.root_id))
+        return self.root_id
+
+    def _schedule_term(self, term_id: str) -> None:
+        if term_id in self._processed or term_id in self._queued:
+            return
+
+        self.scheduler.push(term_id)
+        self._queued.add(term_id)
+
+    def _schedule_tree(self, term_id: str, depth: int = 0) -> None:
+        self._schedule_term(term_id)
+        if not self.walk_children:
+            return
+
+        if self.walk_depth is not None and depth >= self.walk_depth:
+            return
+
+        for child_id in self.store.children_of(term_id):
+            self._schedule_tree(child_id, depth + 1)
+
+    def step(self) -> Optional[Event]:
+        term_id = self.scheduler.pop()
+        if term_id is None:
+            return None
+
+        self._queued.discard(term_id)
+
+        term = self.store.materialize(term_id)
+        self._processed.add(term_id)
+        rules = self.rules
+        if self.include_rules is not None:
+            rules = [rule for rule in rules if rule.name in self.include_rules]
+        if self.exclude_rules:
+            rules = [rule for rule in rules if rule.name not in self.exclude_rules]
+
+        if self.include_scales is not None and term.scale not in self.include_scales:
+            matches: List[Rule] = []
+        elif term.scale in self.exclude_scales:
+            matches = []
+        else:
+            matches = matching_rules(rules, term)
+
+        if self.strict_matching and len(matches) > 1:
+            raise AmbiguousRuleError(term, matches)
+
+        rule = matches[0] if matches else None
+        if rule is None:
+            return None
+
+        limit = self.rule_budgets.get(rule.name)
+        fired = self.rule_counts.get(rule.name, 0)
+        if limit is not None and fired >= limit:
+            self.rule_budget_exhausted.add(rule.name)
+            return None
+
+        new_term = rule.action(term, self.store)
+        if self.signature is not None:
+            self.signature.validate_tree(new_term)
+        new_id = self.store.add_term(new_term)
+        self._check_constraints(new_term)
+        self._check_term_limit()
+        event = Event(
+            before=term_id,
+            after=new_id,
+            rule=rule.name,
+            scale=term.scale,
+            before_term=term,
+            after_term=new_term,
+        )
+        self.events.append(event)
+        self.rule_counts[rule.name] = self.rule_counts.get(rule.name, 0) + 1
+        self.scale_counts[term.scale] = self.scale_counts.get(term.scale, 0) + 1
+        for hook in self.event_hooks:
+            hook(event)
+
+        # Only push again if the term actually changed to avoid endless cycles when
+        # a rule is idempotent with respect to the store's interning.
+        if (
+            not self.term_limit_exhausted
+            and new_id != term_id
+            and new_id not in self._processed
+        ):
+            self._schedule_tree(new_id)
+        return event
+
+    def run(self, max_steps: int = 1) -> List[Event]:
+        emitted: List[Event] = []
+        steps = 0
+        for _ in range(max_steps):
+            if self.term_limit_exhausted:
+                break
+            ev = self.step()
+            steps += 1
+            if ev is None:
+                if len(self.scheduler) == 0:
+                    break
+                continue
+            emitted.append(ev)
+
+        self.exhausted_budget = steps >= max_steps and len(self.scheduler) > 0
+        return emitted
+
+    def run_until_idle(self, max_steps: Optional[int] = None) -> List[Event]:
+        """Drive the scheduler until it empties or a step budget is hit."""
+
+        emitted: List[Event] = []
+        steps = 0
+        while len(self.scheduler) and not self.term_limit_exhausted:
+            ev = self.step()
+            if ev is not None:
+                emitted.append(ev)
+
+            steps += 1
+            if max_steps is not None and steps >= max_steps:
+                self.exhausted_budget = len(self.scheduler) > 0
+                break
+
+        if max_steps is None or steps < max_steps:
+            self.exhausted_budget = False
+
+        return emitted
+
+    def snapshot(self) -> Dict[str, object]:
+        return {
+            "root": self.root_id,
+            "events": list(self.events),
+            "records": self.store.snapshot(),
+            "frontier": self.scheduler.pending(),
+            "processed": set(self._processed),
+            "rule_counts": dict(self.rule_counts),
+            "scale_counts": dict(self.scale_counts),
+        }
+
+    def stats(self) -> Dict[str, object]:
+        """Summaries of runtime activity and remaining work."""
+
+        return {
+            "events": len(self.events),
+            "rule_counts": dict(self.rule_counts),
+            "scale_counts": dict(self.scale_counts),
+            "frontier": list(self.scheduler.pending()),
+            "store_size": len(self.store),
+            "idle": len(self.scheduler) == 0,
+            "budget_exhausted": self.exhausted_budget,
+            "rule_budget_exhausted": sorted(self.rule_budget_exhausted),
+            "term_limit_exhausted": self.term_limit_exhausted,
+            "constraint_exhausted": self.constraint_exhausted,
+            "constraint_violations": list(self.constraint_violations),
+        }
+
+    def state(self) -> Dict[str, object]:
+        """Serializable snapshot of runtime data for persistence or replay."""
+
+        return {
+            "root": self.root_id,
+            "records": self.store.to_json(),
+            "frontier": list(self.scheduler.pending()),
+            "processed": list(self._processed),
+            "scheduler": self._scheduler_name(),
+            "scheduler_seed": getattr(self.scheduler, "seed", None),
+            "scheduler_state": self.scheduler.state() if hasattr(self.scheduler, "state") else None,
+            "walk_children": self.walk_children,
+            "strict_matching": self.strict_matching,
+            "walk_depth": self.walk_depth,
+            "rule_budgets": dict(self.rule_budgets),
+            "rule_budget_exhausted": sorted(self.rule_budget_exhausted),
+            "max_terms": self.max_terms,
+            "term_limit_exhausted": self.term_limit_exhausted,
+            "include_rules": sorted(self.include_rules) if self.include_rules else None,
+            "exclude_rules": sorted(self.exclude_rules) if self.exclude_rules else [],
+            "include_scales": sorted(self.include_scales) if self.include_scales else None,
+            "exclude_scales": sorted(self.exclude_scales) if self.exclude_scales else [],
+            "detect_conflicts": self.detect_conflicts,
+            "constraints": constraints_to_dict(self.constraints) if self.constraints else None,
+            "constraint_violations": list(self.constraint_violations),
+            "constraint_exhausted": self.constraint_exhausted,
+        }
+
+    def _scheduler_name(self) -> str:
+        if isinstance(self.scheduler, LIFOScheduler):
+            return "lifo"
+        if isinstance(self.scheduler, RandomScheduler):
+            return "random"
+        return "fifo"
+
+    def _check_term_limit(self) -> None:
+        if self.max_terms is None:
+            return
+
+        if len(self.store) > self.max_terms:
+            self.term_limit_exhausted = True
+
+    def _check_constraints(self, term: Term) -> None:
+        if self.constraints is None or self.constraint_exhausted:
+            return
+
+        violations = validate_structure(term, self.constraints)
+        if violations:
+            self.constraint_violations = violations
+            self.constraint_exhausted = True
+            joined = "; ".join(violations)
+            raise ValueError(f"Structural constraints violated: {joined}")
diff --git a/nanocode-core/src/scheduler.py b/nanocode-core/src/scheduler.py
new file mode 100644
index 0000000000000000000000000000000000000000..e9b9eb7b1638e50b4d4cccfc130b9e9c5b556741
--- /dev/null
+++ b/nanocode-core/src/scheduler.py
@@ -0,0 +1,90 @@
+import random
+from collections import deque
+from typing import Optional
+
+
+class FIFOScheduler:
+    """Simple FIFO scheduler for rewrite frontiers."""
+
+    def __init__(self) -> None:
+        self._queue: deque[str] = deque()
+
+    def push(self, term_id: str) -> None:
+        self._queue.append(term_id)
+
+    def pop(self) -> Optional[str]:
+        if not self._queue:
+            return None
+        return self._queue.popleft()
+
+    def clear(self) -> None:
+        self._queue.clear()
+
+    def pending(self) -> tuple[str, ...]:
+        return tuple(self._queue)
+
+    def __len__(self) -> int:  # pragma: no cover - trivial
+        return len(self._queue)
+
+
+class LIFOScheduler:
+    """LIFO scheduler that behaves like a stack for rewrite frontiers."""
+
+    def __init__(self) -> None:
+        self._stack: list[str] = []
+
+    def push(self, term_id: str) -> None:
+        self._stack.append(term_id)
+
+    def pop(self) -> Optional[str]:
+        if not self._stack:
+            return None
+        return self._stack.pop()
+
+    def clear(self) -> None:
+        self._stack.clear()
+
+    def pending(self) -> tuple[str, ...]:
+        return tuple(self._stack)
+
+    def __len__(self) -> int:  # pragma: no cover - trivial
+        return len(self._stack)
+
+
+class RandomScheduler:
+    """Randomized scheduler with optional seeding for reproducibility."""
+
+    def __init__(self, seed: Optional[int] = None, state: object | None = None) -> None:
+        self._items: list[str] = []
+        self._seed = seed
+        self._rng = random.Random(seed)
+        if state is not None:
+            self._rng.setstate(state)
+
+    def push(self, term_id: str) -> None:
+        self._items.append(term_id)
+
+    def pop(self) -> Optional[str]:
+        if not self._items:
+            return None
+        idx = self._rng.randrange(len(self._items))
+        return self._items.pop(idx)
+
+    def clear(self) -> None:
+        self._items.clear()
+
+    def pending(self) -> tuple[str, ...]:
+        return tuple(self._items)
+
+    def state(self) -> object:
+        return self._rng.getstate()
+
+    def set_state(self, state: object) -> None:
+        self._rng.setstate(state)
+
+    @property
+    def seed(self) -> Optional[int]:
+        return self._seed
+
+    def __len__(self) -> int:  # pragma: no cover - trivial
+        return len(self._items)
diff --git a/nanocode-core/src/signature.py b/nanocode-core/src/signature.py
new file mode 100644
index 0000000000000000000000000000000000000000..4780a6bad9455aca95f62d790dd13d8922478fb7
--- /dev/null
+++ b/nanocode-core/src/signature.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Iterable
+
+from src.terms import Term
+
+
+class SignatureError(ValueError):
+    """Raised when a term violates a declared Nanocode signature."""
+
+
+@dataclass(frozen=True)
+class TermSignature:
+    sym: str
+    min_children: int = 0
+    max_children: int | None = None
+    allowed_scales: set[int] | None = None
+
+    def validate(self, term: Term) -> None:
+        if term.sym != self.sym:
+            raise SignatureError(f"signature mismatch: expected {self.sym}, got {term.sym}")
+        if term.scale < 0:
+            raise SignatureError(f"term {term.sym} has negative scale {term.scale}")
+        if term.children is None:
+            raise SignatureError(f"term {term.sym} must expose children list")
+
+        count = len(term.children)
+        if count < self.min_children:
+            raise SignatureError(
+                f"term {term.sym} expected at least {self.min_children} children, found {count}"
+            )
+        if self.max_children is not None and count > self.max_children:
+            raise SignatureError(
+                f"term {term.sym} expected at most {self.max_children} children, found {count}"
+            )
+        if self.allowed_scales is not None and term.scale not in self.allowed_scales:
+            raise SignatureError(
+                f"term {term.sym} scale {term.scale} not in allowed scales {sorted(self.allowed_scales)}"
+            )
+
+
+class Signature:
+    """Declarative term constraints for Nanocode genomes."""
+
+    def __init__(self, entries: Iterable[TermSignature]):
+        entries_list = list(entries)
+        self._by_sym = {entry.sym: entry for entry in entries_list}
+        if len(self._by_sym) != len(entries_list):
+            raise SignatureError("duplicate signature entries detected")
+
+    def get(self, sym: str) -> TermSignature | None:
+        return self._by_sym.get(sym)
+
+    def items(self) -> list[tuple[str, TermSignature]]:
+        """Deterministic access to signature entries."""
+
+        return sorted(self._by_sym.items())
+
+    def validate_term(self, term: Term) -> None:
+        entry = self.get(term.sym)
+        if entry is None:
+            raise SignatureError(f"no signature declared for symbol {term.sym}")
+        entry.validate(term)
+
+    def validate_tree(self, term: Term) -> None:
+        self.validate_term(term)
+        for child in term.children:
+            self.validate_tree(child)
+
+    def to_dict(self) -> dict:
+        return {
+            "symbols": {
+                sym: {
+                    "min_children": entry.min_children,
+                    "max_children": entry.max_children,
+                    "scales": sorted(entry.allowed_scales) if entry.allowed_scales is not None else None,
+                }
+                for sym, entry in sorted(self._by_sym.items())
+            }
+        }
+
+    @classmethod
+    def from_dict(cls, payload: dict) -> "Signature":
+        try:
+            symbols = payload["symbols"]
+        except KeyError as exc:  # pragma: no cover - defensive
+            raise SignatureError("signature payload missing 'symbols'") from exc
+
+        entries: list[TermSignature] = []
+        for sym, spec in symbols.items():
+            entries.append(
+                TermSignature(
+                    sym=sym,
+                    min_children=int(spec.get("min_children", 0)),
+                    max_children=(
+                        int(spec["max_children"]) if spec.get("max_children") is not None else None
+                    ),
+                    allowed_scales=set(spec["scales"]) if spec.get("scales") is not None else None,
+                )
+            )
+        return cls(entries)
+
diff --git a/nanocode-core/src/term_store.py b/nanocode-core/src/term_store.py
new file mode 100644
index 0000000000000000000000000000000000000000..f4c95ed34809bb1526a240b19d1bb401332b577d
--- /dev/null
+++ b/nanocode-core/src/term_store.py
@@ -0,0 +1,189 @@
+from __future__ import annotations
+
+import hashlib
+from dataclasses import dataclass
+from typing import Dict, Iterable, Mapping, Tuple
+
+from src.terms import Term
+
+
+@dataclass(frozen=True)
+class TermRecord:
+    """Immutable representation of a term inside the store.
+
+    Children are stored as IDs to enable structural sharing and deterministic hashing.
+    """
+
+    sym: str
+    scale: int
+    children: Tuple[str, ...] = ()
+
+
+@dataclass(frozen=True)
+class TermKey:
+    """Hashable key for interning a term in the store."""
+
+    sym: str
+    scale: int
+    children: Tuple[str, ...]
+
+
+class TermStore:
+    """Persistent term store with structural sharing.
+
+    Terms are interned by (sym, scale, child_ids) and addressed by stable IDs derived
+    from their content. The store never mutates existing records, enabling snapshots
+    and deterministic replay of rewrite steps.
+    """
+
+    def __init__(self) -> None:
+        self._records: Dict[str, TermRecord] = {}
+        self._index: Dict[TermKey, str] = {}
+
+    def add_term(self, term: Term) -> str:
+        """Add a term (recursively) and return its stable ID.
+
+        If an equivalent term already exists, the existing ID is returned.
+        """
+
+        child_ids = tuple(self.add_term(child) for child in term.children)
+        key = TermKey(term.sym, term.scale, child_ids)
+        if key in self._index:
+            return self._index[key]
+
+        term_id = self._hash_key(key)
+        self._records[term_id] = TermRecord(term.sym, term.scale, child_ids)
+        self._index[key] = term_id
+        return term_id
+
+    def get(self, term_id: str) -> TermRecord:
+        return self._records[term_id]
+
+    def materialize(self, term_id: str) -> Term:
+        """Reconstruct a `Term` tree from a stored ID."""
+
+        record = self.get(term_id)
+        children = [self.materialize(cid) for cid in record.children]
+        return Term(sym=record.sym, scale=record.scale, children=children)
+
+    def children_of(self, term_id: str) -> Tuple[str, ...]:
+        """Return the immediate child IDs for a stored term."""
+
+        return self.get(term_id).children
+
+    def snapshot(self) -> Dict[str, TermRecord]:
+        """Return a shallow copy of stored records for inspection/replay."""
+
+        return dict(self._records)
+
+    def to_json(self) -> Dict[str, Dict[str, object]]:
+        """JSON-friendly view of stored records keyed by term ID."""
+
+        return {
+            term_id: {"sym": record.sym, "scale": record.scale, "children": list(record.children)}
+            for term_id, record in self._records.items()
+        }
+
+    @classmethod
+    def from_json(cls, payload: Mapping[str, Mapping[str, object]]) -> "TermStore":
+        """Rehydrate a term store from a JSON-ready mapping."""
+
+        store = cls()
+        records = payload["records"] if "records" in payload else payload
+        for term_id, record_data in records.items():
+            children = tuple(str(child) for child in record_data.get("children", ()))
+            record = TermRecord(
+                sym=str(record_data["sym"]),
+                scale=int(record_data["scale"]),
+                children=children,
+            )
+            store._records[term_id] = record
+            store._index[TermKey(record.sym, record.scale, record.children)] = term_id
+        return store
+
+    def to_bundle(
+        self,
+        *,
+        root: str | None = None,
+        frontier: Iterable[str] | None = None,
+        scheduler: str | None = None,
+        scheduler_seed: int | None = None,
+        scheduler_state: object | None = None,
+        processed: Iterable[str] | None = None,
+        walk_children: bool | None = None,
+        strict_matching: bool | None = None,
+        walk_depth: int | None = None,
+        rule_budgets: Dict[str, int] | None = None,
+        rule_budget_exhausted: Iterable[str] | None = None,
+        max_terms: int | None = None,
+        term_limit_exhausted: bool | None = None,
+        include_rules: Iterable[str] | None = None,
+        exclude_rules: Iterable[str] | None = None,
+        include_scales: Iterable[int] | None = None,
+        exclude_scales: Iterable[int] | None = None,
+        detect_conflicts: bool | None = None,
+        signature: dict | None = None,
+        constraints: dict | None = None,
+    ) -> Dict[str, object]:
+        """Package the store with runtime metadata for replay/resume."""
+
+        bundle: Dict[str, object] = {
+            "records": self.to_json(),
+        }
+
+        if root is not None:
+            bundle["root"] = root
+        if frontier is not None:
+            bundle["frontier"] = list(frontier)
+        if scheduler is not None:
+            bundle["scheduler"] = scheduler
+        if scheduler_seed is not None:
+            bundle["scheduler_seed"] = scheduler_seed
+        if scheduler_state is not None:
+            bundle["scheduler_state"] = scheduler_state
+        if processed is not None:
+            bundle["processed"] = list(processed)
+        if walk_children is not None:
+            bundle["walk_children"] = walk_children
+        if strict_matching is not None:
+            bundle["strict_matching"] = strict_matching
+        if walk_depth is not None:
+            bundle["walk_depth"] = walk_depth
+        if rule_budgets is not None:
+            bundle["rule_budgets"] = dict(rule_budgets)
+        if rule_budget_exhausted is not None:
+            bundle["rule_budget_exhausted"] = list(rule_budget_exhausted)
+        if max_terms is not None:
+            bundle["max_terms"] = max_terms
+        if term_limit_exhausted is not None:
+            bundle["term_limit_exhausted"] = term_limit_exhausted
+        if include_rules is not None:
+            bundle["include_rules"] = list(include_rules)
+        if exclude_rules is not None:
+            bundle["exclude_rules"] = list(exclude_rules)
+        if include_scales is not None:
+            bundle["include_scales"] = list(include_scales)
+        if exclude_scales is not None:
+            bundle["exclude_scales"] = list(exclude_scales)
+        if detect_conflicts is not None:
+            bundle["detect_conflicts"] = detect_conflicts
+        if signature is not None:
+            bundle["signature"] = signature
+        if constraints is not None:
+            bundle["constraints"] = constraints
+
+        return bundle
+
+    def __len__(self) -> int:  # pragma: no cover - trivial
+        return len(self._records)
+
+    @staticmethod
+    def _hash_key(key: TermKey) -> str:
+        raw = f"{key.sym}|{key.scale}|{','.join(key.children)}"
+        return hashlib.sha256(raw.encode()).hexdigest()[:16]
+
+    def __contains__(self, term_id: str) -> bool:  # pragma: no cover - trivial
+        return term_id in self._records
+
+    def iter_records(self) -> Iterable[Tuple[str, TermRecord]]:
+        return self._records.items()
diff --git a/nanocode-core/src/terms.py b/nanocode-core/src/terms.py
index 251d38cd2818db77769f342a050d2eb3fadf0775..70b12c89b6b72e909affd6db4269253d95690c06 100644
--- a/nanocode-core/src/terms.py
+++ b/nanocode-core/src/terms.py
@@ -1,28 +1,121 @@
 from __future__ import annotations
-from dataclasses import dataclass, field
-from typing import List, Callable
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Iterable, List
 
 @dataclass(frozen=True)
 class Term:
     sym: str
     scale: int = 0
     children: List["Term"] = field(default_factory=list)
 
-def expand(t: Term, fanout: int = 3) -> Term:
-    new_scale = t.scale + 1
-    children = [
-        Term(sym=f"{t.sym}.{i}", scale=new_scale)
-        for i in range(fanout)
-    ]
-    return Term(sym=f"F({t.sym})", scale=new_scale, children=children)
-
-def reduce(u: Term, summarizer: Callable[[List[Term]], str] = None) -> Term:
-    if not (u.sym.startswith("F(") and u.sym.endswith(")")):
-        return u
-
-    base_sym = u.sym[2:-1]
-
-    if summarizer:
-        _ = summarizer(u.children)
-
-    return Term(sym=base_sym, scale=u.scale - 1)
+def expand(t: Term, fanout: int = 3) -> Term:
+    """Construct a higher-scale motif from lower-scale structure.
+
+    Instead of merely renaming a symbol, expansion lifts the existing
+    structure by emitting explicit motif children derived from the
+    lower-scale leaves. Each motif captures contiguous slices of the
+    leaf sequence up to ``fanout`` tokens, turning scale into a structural
+    coordinate rather than a label.
+    """
+
+    def _leaves(term: Term) -> List[Term]:
+        stack = [term]
+        leaves: List[Term] = []
+        while stack:
+            node = stack.pop()
+            if not node.children:
+                leaves.append(node)
+                continue
+            stack.extend(reversed(node.children))
+        return leaves
+
+    leaves = _leaves(t)
+    if not leaves:
+        leaves = [t]
+
+    motifs: List[Term] = []
+    for size in range(1, min(fanout, len(leaves)) + 1):
+        for idx in range(0, len(leaves) - size + 1):
+            window = leaves[idx : idx + size]
+            label = "|".join(child.sym for child in window)
+            motifs.append(
+                Term(
+                    sym=f"motif[{label}]",
+                    scale=t.scale + 1,
+                    children=list(window),
+                )
+            )
+
+    if not motifs and fanout > 0:
+        # If we have fewer leaves than the requested fanout, synthesize
+        # repeated motifs to preserve the requested branching factor.
+        motifs = [
+            Term(sym=f"motif[{t.sym}#{i}]", scale=t.scale + 1, children=[leaves[0]])
+            for i in range(fanout)
+        ]
+
+    while len(motifs) < fanout:
+        motifs.append(
+            Term(
+                sym=f"motif[{t.sym}#{len(motifs)}]",
+                scale=t.scale + 1,
+                children=[leaves[0]],
+            )
+        )
+
+    return Term(
+        sym=f"F({t.sym})",
+        scale=t.scale + 1,
+        children=motifs,
+    )
+
+def reduce(u: Term, summarizer: Callable[[List[Term]], str] | None = None) -> Term:
+    """Collapse a higher-scale motif back to a lower-scale representative.
+
+    The summarizer's output is threaded into the reduced term to preserve
+    provenance. By default we emit a stable summary derived from child
+    symbols and scales so round-tripping through expand→reduce produces a
+    structure that reflects the lower-scale evidence.
+    """
+
+    if not u.children:
+        return u
+
+    base_sym = u.sym
+    if u.sym.startswith("expand[") and u.sym.endswith("]"):
+        base_sym = u.sym[len("expand[") : -1]
+    if u.sym.startswith("F(") and u.sym.endswith(")"):
+        base_sym = u.sym[2:-1]
+
+    if summarizer is None:
+        summarizer = _default_summarizer
+
+    summary = summarizer(u.children)
+    summary_child = Term(sym=f"summary:{summary}", scale=u.scale - 1)
+    return Term(sym=base_sym, scale=u.scale - 1, children=[summary_child])
+
+
+def _default_summarizer(children: Iterable[Term]) -> str:
+    counts: Dict[str, int] = {}
+
+    def _walk(term: Term) -> None:
+        key = f"{term.sym}@{term.scale}"
+        counts[key] = counts.get(key, 0) + 1
+        for child in term.children:
+            _walk(child)
+
+    for child in children:
+        _walk(child)
+
+    parts = [f"{key}={counts[key]}" for key in sorted(counts)]
+    return ";".join(parts)
+
+
+def term_to_dict(term: Term) -> Dict[str, Any]:
+    """Serialize a term into a JSON-friendly dict for tracing."""
+
+    return {
+        "sym": term.sym,
+        "scale": term.scale,
+        "children": [term_to_dict(child) for child in term.children],
+    }
diff --git a/nanocode-core/src/trace.py b/nanocode-core/src/trace.py
new file mode 100644
index 0000000000000000000000000000000000000000..d33f7a2bb792b6625968e7cf6c7187de46c3cdf0
--- /dev/null
+++ b/nanocode-core/src/trace.py
@@ -0,0 +1,25 @@
+from __future__ import annotations
+
+import io
+import json
+from typing import Iterable, List
+
+from src.runtime import Event
+
+
+class JSONLTracer:
+    """Simple tracer that writes JSONL event records to a file-like sink."""
+
+    def __init__(self, sink: io.TextIOBase):
+        self.sink = sink
+
+    def __call__(self, event: Event) -> None:
+        self.sink.write(json.dumps(event.to_record()))
+        self.sink.write("\n")
+        self.sink.flush()
+
+
+def dump_events(events: Iterable[Event]) -> List[dict]:
+    """Convert an event stream to JSON-serializable dicts."""
+
+    return [ev.to_record() for ev in events]
diff --git a/nanocode-core/tests/conftest.py b/nanocode-core/tests/conftest.py
new file mode 100644
index 0000000000000000000000000000000000000000..6094b55bc3828755d9429662dffdd9a6d4070a1c
--- /dev/null
+++ b/nanocode-core/tests/conftest.py
@@ -0,0 +1,6 @@
+import os
+import sys
+
+ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
+if ROOT not in sys.path:
+    sys.path.insert(0, ROOT)
diff --git a/nanocode-core/tests/test_agent.py b/nanocode-core/tests/test_agent.py
new file mode 100644
index 0000000000000000000000000000000000000000..75afa7e76535c737337d7e0f74e501b091624e96
--- /dev/null
+++ b/nanocode-core/tests/test_agent.py
@@ -0,0 +1,122 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+
+from src.agent import AgentPolicy, Goal, rollout_agent
+from src.bridge import PORT_SYM, BridgeBinding, BridgePort, BridgeSchema
+from src.interpreter import Program
+from src.rewrite import Pattern, Rule
+from src.terms import Term
+
+
+@dataclass
+class CounterEnv:
+    target: int
+
+    def reset(self) -> int:
+        self.state = 0
+        return self.state
+
+    def step(self, action: str):
+        if action == "act_inc":
+            self.state += 1
+            reward = 1.0
+        else:
+            reward = 0.0
+
+        done = self.state >= self.target
+        return self.state, reward, done, {"state": self.state}
+
+
+def encode_observation(value: int) -> Term:
+    return Term(sym="obs", scale=0, children=[Term(sym=str(value), scale=0)])
+
+
+def make_policy(target: int) -> AgentPolicy:
+    def choose_action(term: Term, store):
+        value = int(term.children[0].sym)
+        next_sym = "act_inc" if value < target else "act_hold"
+        return Term(sym=next_sym, scale=term.scale)
+
+    program = Program(
+        name="counter",
+        root=encode_observation(0),
+        rules=[Rule(name="choose", pattern=Pattern(sym="obs", scale=0), action=choose_action)],
+        max_steps=4,
+    )
+
+    return AgentPolicy(
+        program=program,
+        encode_observation=encode_observation,
+        decode_action=lambda term, _: term.sym,
+    )
+
+
+def test_rollout_agent_with_bridge_binding():
+    env = CounterEnv(target=2)
+
+    schema = BridgeSchema(
+        name="io",
+        ports=(
+            BridgePort(name="obs", direction="in", scale=0),
+            BridgePort(name="act", direction="out", scale=0),
+        ),
+    )
+
+    def rule_action(term: Term, _):
+        current_value = int(term.children[0].sym)
+        action_sym = "act_inc" if current_value < env.target else "act_hold"
+        return Term(
+            sym=f"{PORT_SYM}:out:act",
+            scale=term.scale,
+            children=[Term(sym=action_sym, scale=term.scale)],
+        )
+
+    program = Program(
+        name="counter_bridge",
+        root=Term(sym=f"{PORT_SYM}:in:obs", scale=0, children=[Term(sym="0", scale=0)]),
+        rules=[Rule(name="choose", pattern=Pattern(sym=f"{PORT_SYM}:in:obs", scale=0), action=rule_action)],
+        max_steps=4,
+    )
+
+    policy = AgentPolicy(
+        program=program,
+        bridge=BridgeBinding(
+            schema=schema,
+            encode={"obs": lambda value: Term(sym=str(value), scale=0)},
+            decode={"act": lambda term: term.sym},
+        ),
+        observation_port="obs",
+        action_port="act",
+    )
+
+    result = rollout_agent(policy, env)
+
+    assert result.total_reward == 2.0
+    assert [step.action for step in result.steps] == ["act_inc", "act_inc"]
+
+
+def test_rollout_agent_counts_to_target():
+    env = CounterEnv(target=3)
+    policy = make_policy(target=3)
+    result = rollout_agent(policy, env)
+
+    assert result.total_reward == 3.0
+    assert [step.action for step in result.steps] == ["act_inc", "act_inc", "act_inc"]
+    assert result.steps[-1].done is True
+    assert result.goal_score is None
+
+
+def test_rollout_agent_with_goal_reward():
+    env = CounterEnv(target=2)
+    policy = make_policy(target=2)
+
+    def goal_reward(steps):
+        return sum(step.reward for step in steps) - len(steps) * 0.1
+
+    goal = Goal(name="dense_reward", reward_fn=goal_reward)
+    result = rollout_agent(policy, env, goal=goal)
+
+    assert result.goal_score == 1.8  # (1+1) - 2*0.1
+    assert len(result.steps) == 2
+    assert result.total_reward == 2.0
diff --git a/nanocode-core/tests/test_ast.py b/nanocode-core/tests/test_ast.py
new file mode 100644
index 0000000000000000000000000000000000000000..ced4b1566a0df3912391167f52b76b86540c47ec
--- /dev/null
+++ b/nanocode-core/tests/test_ast.py
@@ -0,0 +1,115 @@
+import pytest
+
+from src import Interpreter
+from src.ast import parse_program, parse_rule, parse_term
+from src.runtime import Runtime
+
+
+def test_parse_term_with_scale_and_children():
+    term = parse_term(["root", ":scale", 2, ["child"], ["leaf", ["bud"]]])
+    assert term.sym == "root"
+    assert term.scale == 2
+    assert len(term.children) == 2
+    assert term.children[0].sym == "child"
+    assert term.children[1].children[0].sym == "bud"
+
+
+def test_parse_rule_expand_fanout():
+    rule = parse_rule([
+        "rule",
+        "expand-root",
+        ["pattern", ":sym", "seed"],
+        ["action", "expand", ":fanout", 2],
+    ])
+    term = parse_term("seed")
+    runtime = Runtime([rule])
+    runtime.load(term)
+    event = runtime.step()
+    assert event is not None
+    assert event.after_term.sym == "F(seed)"
+    assert len(event.after_term.children) == 2
+
+
+def test_parse_program_runs_via_interpreter():
+    program_source = """
+    (program demo
+      (root (seed))
+      (rules
+        (rule expand-seed
+          (pattern :sym seed :scale 0)
+          (action expand :fanout 2))
+        (rule reduce-seed
+          (pattern :sym F(seed) :scale 1)
+          (action reduce)))
+      (max_steps 4))
+    """
+
+    program = parse_program(program_source)
+    result = Interpreter().run(program)
+
+    assert result.root_id is not None
+    # Expect at least one expand followed by a reduce
+    assert len(result.events) >= 2
+    assert result.events[0].after_term.sym == "F(seed)"
+    assert result.events[1].after_term.sym == "seed"
+    assert result.snapshot["root"] == result.root_id
+
+
+def test_parse_program_reads_max_terms_and_validates():
+    program_source = """
+    (program limited
+      (root (seed))
+      (rules (rule grow (pattern :sym seed) (action expand :fanout 1)))
+      (max_steps 2)
+      (max_terms 1))
+    """
+
+    program = parse_program(program_source)
+    assert program.max_terms == 1
+
+    with pytest.raises(ValueError, match="max_terms must be positive"):
+        parse_program(program_source.replace("1))", "0))"))
+
+
+def test_parse_program_rejects_duplicate_rule_names():
+    program_source = """
+    (program dup-rules
+      (root (seed))
+      (rules
+        (rule same-name (pattern :sym seed) (action expand :fanout 1))
+        (rule same-name (pattern :sym seed :scale 1) (action reduce))))
+    """
+
+    with pytest.raises(ValueError, match="Duplicate rule name"):
+        parse_program(program_source)
+
+
+def test_parse_program_rejects_negative_scales():
+    program_source = """
+    (program negative-scale
+      (root (seed :scale -1))
+      (rules
+        (rule expand-seed (pattern :sym seed) (action expand :fanout 1))))
+    """
+
+    with pytest.raises(ValueError, match="negative scale"):
+        parse_program(program_source)
+
+
+def test_parse_program_ignores_semicolon_comments():
+    program_source = """
+    ; full-line comment should be ignored
+    (program commented
+      (root (seed)) ; trailing comment after root
+      (rules
+        (rule expand-seed ; inline rule comment
+          (pattern :sym seed)
+          (action expand :fanout 1))))
+    """
+
+    program = parse_program(program_source)
+
+    assert program.name == "commented"
+    assert program.root.sym == "seed"
+    assert len(program.rules) == 1
+    assert program.rules[0].name == "expand-seed"
diff --git a/nanocode-core/tests/test_bridge.py b/nanocode-core/tests/test_bridge.py
new file mode 100644
index 0000000000000000000000000000000000000000..18e057171fbde28bf82a44773cca9501484dd07b
--- /dev/null
+++ b/nanocode-core/tests/test_bridge.py
@@ -0,0 +1,122 @@
+from __future__ import annotations
+
+import pytest
+
+from src.bridge import (
+    BRIDGE_SYM,
+    METADATA_SYM,
+    PORT_SYM,
+    BridgeBinding,
+    BridgePort,
+    BridgeSchema,
+    InvalidBridgeSchema,
+    bridge_call_action,
+    bridge_schema_from_term,
+    bridge_schema_to_term,
+    validate_bridge_schema,
+)
+from src.terms import Term
+
+
+def test_validate_bridge_schema_rejects_duplicates_and_negatives():
+    schema = BridgeSchema(
+        name="demo",
+        ports=(
+            BridgePort(name="obs", direction="in", scale=0),
+            BridgePort(name="obs", direction="out", scale=1),
+        ),
+    )
+    with pytest.raises(InvalidBridgeSchema):
+        validate_bridge_schema(schema)
+
+    schema = BridgeSchema(name="demo", ports=(BridgePort(name="obs", direction="in", scale=-1),))
+    with pytest.raises(InvalidBridgeSchema):
+        validate_bridge_schema(schema)
+
+
+def test_bridge_schema_term_round_trip():
+    schema = BridgeSchema(
+        name="adapter",
+        ports=(
+            BridgePort(name="obs", direction="in", scale=0),
+            BridgePort(name="act", direction="out", scale=1),
+        ),
+        metadata={"version": 1, "train": False, "note": "demo"},
+    )
+
+    as_term = bridge_schema_to_term(schema)
+    assert as_term.sym == f"{BRIDGE_SYM}:{schema.name}"
+    assert {child.sym for child in as_term.children} == {
+        f"{PORT_SYM}:in:obs",
+        f"{PORT_SYM}:out:act",
+        "metadata",
+    }
+
+    restored = bridge_schema_from_term(as_term)
+    assert restored == schema
+
+
+def test_bridge_binding_encode_decode():
+    schema = BridgeSchema(
+        name="adapter",
+        ports=(
+            BridgePort(name="obs", direction="in", scale=0),
+            BridgePort(name="act", direction="out", scale=0),
+        ),
+    )
+    binding = BridgeBinding(
+        schema=schema,
+        encode={"obs": lambda payload: Term(sym=str(payload), scale=0)},
+        decode={"act": lambda term: term.sym},
+    )
+
+    encoded = binding.encode_input("obs", 3)
+    assert encoded.sym == f"{PORT_SYM}:in:obs"
+    assert encoded.children[0].sym == "3"
+
+    with pytest.raises(InvalidBridgeSchema):
+        binding.encode_input("act", 3)
+
+    action_term = Term(sym=f"{PORT_SYM}:out:act", scale=0, children=[Term(sym="fire", scale=0)])
+    decoded = binding.decode_output("act", action_term)
+    assert decoded == "fire"
+
+    with pytest.raises(InvalidBridgeSchema):
+        binding.decode_output("obs", action_term)
+
+    mismatched = Term(sym=f"{PORT_SYM}:out:wrong", scale=0, children=[Term(sym="fire", scale=0)])
+    with pytest.raises(InvalidBridgeSchema):
+        binding.decode_output("act", mismatched)
+
+
+def test_bridge_schema_rejects_unsupported_metadata_types():
+    schema = BridgeSchema(
+        name="adapter",
+        ports=(BridgePort(name="obs", direction="in", scale=0),),
+        metadata={"unsupported": object()},
+    )
+
+    with pytest.raises(InvalidBridgeSchema):
+        bridge_schema_to_term(schema)
+
+
+def test_bridge_call_action_enriches_term():
+    schema = BridgeSchema(
+        name="oracle",
+        ports=(BridgePort(name="score", direction="out", scale=1),),
+    )
+    binding = BridgeBinding(
+        schema=schema,
+        encode={"score": lambda payload: Term(sym=str(payload), scale=1)},
+        decode={"score": lambda term: float(term.sym)},
+    )
+
+    def scorer(term: Term) -> float:
+        return float(len(term.sym))
+
+    action = bridge_call_action(binding, "score", scorer)
+    base = Term(sym="seed", scale=0)
+    enriched = action(base, None)
+    assert enriched.sym.startswith("bridge:")
+    assert enriched.children[1].sym.startswith(f"{PORT_SYM}:out:score")
+    assert enriched.children[1].children[0].sym == "4.0"
diff --git a/nanocode-core/tests/test_cli.py b/nanocode-core/tests/test_cli.py
new file mode 100644
index 0000000000000000000000000000000000000000..4864599abb0a1a4e1814f670b7e91f0b72e34b0b
--- /dev/null
+++ b/nanocode-core/tests/test_cli.py
@@ -0,0 +1,605 @@
+import json
+import subprocess
+import sys
+from pathlib import Path
+
+
+def test_cli_runs_program(tmp_path: Path):
+    program_src = """
+    (program demo
+      (root (seed :scale 0 (seed :scale 0) (seed :scale 0)))
+      (rules
+        (rule grow (pattern :sym seed) (action expand :fanout 2))
+        (rule normalize (pattern :sym F(seed)) (action reduce))
+      )
+      (max_steps 4)
+    )
+    """
+
+    program_file = tmp_path / "demo.nanocode"
+    program_file.write_text(program_src)
+    trace_file = tmp_path / "trace.jsonl"
+
+    result = subprocess.run(
+        [sys.executable, "-m", "src.cli", str(program_file), "--trace-jsonl", str(trace_file)],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+
+    summary = json.loads(result.stdout)
+    assert summary["program"] == "demo"
+    assert summary["scheduler"] == "fifo"
+    assert summary["walk_children"] is False
+    assert summary["strict_matching"] is False
+    assert summary["detect_conflicts"] is False
+    assert summary["events"] > 0
+    assert summary["rule_counts"]
+    assert summary["scale_counts"]
+    assert summary["idle"] in {True, False}
+    # The richer expand/reduce pipeline may consume the step budget
+    # when generating higher-scale motifs.
+    assert summary["budget_exhausted"] in {True, False}
+    assert trace_file.exists()
+    trace_lines = trace_file.read_text().strip().splitlines()
+    assert len(trace_lines) == summary["events"]
+
+
+def test_cli_accepts_scheduler_override(tmp_path: Path):
+    program_src = """
+    (program demo
+      (root (seed :scale 0 (child1) (child2)))
+      (rules
+        (rule mark (pattern) (action reduce))
+      )
+      (max_steps 3)
+    )
+    """
+
+    program_file = tmp_path / "demo.nanocode"
+    program_file.write_text(program_src)
+
+    result = subprocess.run(
+        [sys.executable, "-m", "src.cli", str(program_file), "--scheduler", "lifo"],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+
+    summary = json.loads(result.stdout)
+    assert summary["events"] >= 1
+
+
+def test_cli_accepts_random_scheduler_with_seed(tmp_path: Path):
+    program_src = """
+    (program demo
+      (root (seed :scale 0 (child1) (child2)))
+      (rules
+        (rule mark (pattern) (action reduce))
+      )
+      (max_steps 3)
+    )
+    """
+
+    program_file = tmp_path / "demo.nanocode"
+    program_file.write_text(program_src)
+    store_file = tmp_path / "store.json"
+
+    initial = subprocess.run(
+        [
+            sys.executable,
+            "-m",
+            "src.cli",
+            str(program_file),
+            "--scheduler",
+            "random",
+            "--scheduler-seed",
+            "5",
+            "--store-json",
+            str(store_file),
+        ],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+
+    summary = json.loads(initial.stdout)
+    assert summary["scheduler"] == "random"
+    assert summary["scheduler_seed"] == 5
+
+    stored_state = json.loads(store_file.read_text())
+    assert stored_state["scheduler"] == "random"
+    assert stored_state["scheduler_seed"] == 5
+    assert "scheduler_state" in stored_state
+
+    resumed = subprocess.run(
+        [
+            sys.executable,
+            "-m",
+            "src.cli",
+            str(program_file),
+            "--load-store",
+            str(store_file),
+        ],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+
+    resumed_summary = json.loads(resumed.stdout)
+    assert resumed_summary["scheduler"] == "random"
+    assert resumed_summary["scheduler_seed"] == 5
+
+
+def test_cli_accepts_stdin(tmp_path: Path):
+    program_src = """
+    (program demo
+      (root (seed :scale 0))
+      (rules
+        (rule grow (pattern :sym seed) (action expand :fanout 2))
+        (rule normalize (pattern :sym F(seed)) (action reduce))
+      )
+      (max_steps 4)
+    )
+    """
+
+    result = subprocess.run(
+        [sys.executable, "-m", "src.cli", "-"],
+        check=True,
+        capture_output=True,
+        text=True,
+        input=program_src,
+    )
+
+    summary = json.loads(result.stdout)
+    assert summary["program"] == "demo"
+    assert summary["events"] > 0
+    assert summary["rule_counts"]
+    assert summary["budget_exhausted"] in {True, False}
+
+
+def test_cli_dry_run_validates_without_execution(tmp_path: Path):
+    program_src = """
+    (program demo
+      (root (seed :scale 0))
+      (rules
+        (rule grow (pattern :sym seed) (action expand :fanout 2))
+      )
+      (max_steps 4)
+    )
+    """
+
+    program_file = tmp_path / "demo.nanocode"
+    program_file.write_text(program_src)
+
+    result = subprocess.run(
+        [sys.executable, "-m", "src.cli", str(program_file), "--dry-run"],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+
+    summary = json.loads(result.stdout)
+    assert summary["dry_run"] is True
+    assert summary["events"] == 0
+    assert summary["rule_counts"] == {}
+    assert summary["frontier"]  # root should be scheduled but not processed
+    assert summary["budget_exhausted"] is False
+
+
+def test_cli_can_emit_store_snapshot(tmp_path: Path):
+    program_src = """
+    (program demo
+      (root (seed :scale 0))
+      (rules
+        (rule grow (pattern :sym seed) (action expand :fanout 2))
+        (rule normalize (pattern :sym F(seed)) (action reduce))
+      )
+      (max_steps 4)
+    )
+    """
+
+    program_file = tmp_path / "demo.nanocode"
+    program_file.write_text(program_src)
+    store_file = tmp_path / "store.json"
+
+    result = subprocess.run(
+        [sys.executable, "-m", "src.cli", str(program_file), "--store-json", str(store_file)],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+
+    summary = json.loads(result.stdout)
+    store_payload = json.loads(store_file.read_text())
+
+    assert store_payload["root"] == summary["root"]
+    assert any(record["children"] for record in store_payload["records"].values())
+    assert store_payload["frontier"] == list(summary["frontier"])
+    assert store_payload["processed"]
+
+
+def test_cli_can_resume_from_store_snapshot(tmp_path: Path):
+    program_src = """
+    (program demo
+      (root (seed :scale 0))
+      (rules
+        (rule grow (pattern :sym seed) (action expand :fanout 1))
+        (rule normalize (pattern :sym F(seed)) (action reduce))
+      )
+      (max_steps 4)
+    )
+    """
+
+    program_file = tmp_path / "demo.nanocode"
+    program_file.write_text(program_src)
+    store_file = tmp_path / "store.json"
+
+    initial = subprocess.run(
+        [
+            sys.executable,
+            "-m",
+            "src.cli",
+            str(program_file),
+            "--store-json",
+            str(store_file),
+            "--steps-only",
+            "--max-steps",
+            "1",
+        ],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+
+    initial_summary = json.loads(initial.stdout)
+    stored_state = json.loads(store_file.read_text())
+
+    assert stored_state["frontier"]
+    assert initial_summary["events"] == 1
+    assert initial_summary["budget_exhausted"] is True
+
+    resumed = subprocess.run(
+        [
+            sys.executable,
+            "-m",
+            "src.cli",
+            str(program_file),
+            "--load-store",
+            str(store_file),
+        ],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+
+    resumed_summary = json.loads(resumed.stdout)
+    assert resumed_summary["events"] >= initial_summary["events"]
+    assert isinstance(resumed_summary["frontier"], list)
+
+
+def test_cli_restores_runtime_flags_from_store(tmp_path: Path):
+    program_src = """
+    (program demo
+      (root (seed :scale 0 (child1) (child2)))
+      (rules
+        (rule mark (pattern) (action reduce))
+      )
+      (max_steps 2)
+    )
+    """
+
+    program_file = tmp_path / "demo.nanocode"
+    program_file.write_text(program_src)
+    store_file = tmp_path / "store.json"
+
+    subprocess.run(
+        [
+            sys.executable,
+            "-m",
+            "src.cli",
+            str(program_file),
+            "--walk-children",
+            "--walk-depth",
+            "1",
+            "--strict-matching",
+            "--scheduler",
+            "lifo",
+            "--store-json",
+            str(store_file),
+            "--steps-only",
+            "--max-steps",
+            "1",
+        ],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+
+    stored_state = json.loads(store_file.read_text())
+    assert stored_state["walk_children"] is True
+    assert stored_state["strict_matching"] is True
+    assert stored_state["detect_conflicts"] is False
+    assert stored_state["walk_depth"] == 1
+    assert stored_state["scheduler"] == "lifo"
+
+    resumed = subprocess.run(
+        [
+            sys.executable,
+            "-m",
+            "src.cli",
+            str(program_file),
+            "--load-store",
+            str(store_file),
+        ],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+
+    summary = json.loads(resumed.stdout)
+    assert summary["walk_children"] is True
+    assert summary["strict_matching"] is True
+    assert summary["detect_conflicts"] is False
+    assert summary["walk_depth"] == 1
+    assert summary["scheduler"] == "lifo"
+
+
+def test_cli_enforces_rule_budgets_and_persists_them(tmp_path: Path):
+    program_src = """
+    (program demo
+      (root (seed :scale 0 (seed :scale 0) (seed :scale 0)))
+      (rules
+        (rule grow (pattern :sym seed) (action expand :fanout 2))
+      )
+      (max_steps 6)
+    )
+    """
+
+    program_file = tmp_path / "demo.nanocode"
+    program_file.write_text(program_src)
+    store_file = tmp_path / "store.json"
+
+    result = subprocess.run(
+        [
+            sys.executable,
+            "-m",
+            "src.cli",
+            str(program_file),
+            "--walk-children",
+            "--rule-budget",
+            "grow=1",
+            "--store-json",
+            str(store_file),
+        ],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+
+    summary = json.loads(result.stdout)
+    assert summary["rule_counts"] == {"grow": 1}
+    assert summary["rule_budget_exhausted"] == ["grow"]
+    assert summary["rule_budgets"] == {"grow": 1}
+
+    stored_state = json.loads(store_file.read_text())
+    assert stored_state["rule_budgets"] == {"grow": 1}
+    assert stored_state["rule_budget_exhausted"] == ["grow"]
+
+    resumed = subprocess.run(
+        [sys.executable, "-m", "src.cli", str(program_file), "--load-store", str(store_file)],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+
+    resumed_summary = json.loads(resumed.stdout)
+    assert resumed_summary["rule_budgets"] == {"grow": 1}
+    # No additional events should fire because the budget was already exhausted.
+    assert resumed_summary["events"] == 0
+
+
+def test_cli_filters_rules_and_persists_selection(tmp_path: Path):
+    program_src = """
+    (program demo
+      (root (seed :scale 0))
+      (rules
+        (rule grow (pattern :sym seed) (action expand :fanout 1))
+        (rule normalize (pattern :sym F(seed)) (action reduce))
+      )
+      (max_steps 3)
+    )
+    """
+
+    program_file = tmp_path / "demo.nanocode"
+    program_file.write_text(program_src)
+    store_file = tmp_path / "store.json"
+
+    result = subprocess.run(
+        [
+            sys.executable,
+            "-m",
+            "src.cli",
+            str(program_file),
+            "--only-rule",
+            "grow",
+            "--skip-rule",
+            "normalize",
+            "--store-json",
+            str(store_file),
+        ],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+
+    summary = json.loads(result.stdout)
+    assert summary["rule_counts"] == {"grow": 1}
+    assert summary["include_rules"] == ["grow"]
+    assert summary["exclude_rules"] == ["normalize"]
+
+    stored_state = json.loads(store_file.read_text())
+    assert stored_state["include_rules"] == ["grow"]
+    assert stored_state["exclude_rules"] == ["normalize"]
+
+    resumed = subprocess.run(
+        [sys.executable, "-m", "src.cli", str(program_file), "--load-store", str(store_file)],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+
+    resumed_summary = json.loads(resumed.stdout)
+    assert resumed_summary["include_rules"] == ["grow"]
+    assert resumed_summary["exclude_rules"] == ["normalize"]
+    # Resumed run should honor the stored rule filters and find no additional work.
+    assert resumed_summary["events"] == 0
+
+
+def test_cli_filters_scales_and_persists_selection(tmp_path: Path):
+    program_src = """
+    (program demo
+      (root (seed :scale 0 (leaf :scale 1)))
+      (rules
+        (rule grow-root (pattern :sym seed :scale 0) (action expand :fanout 1))
+        (rule grow-leaf (pattern :sym leaf :scale 1) (action expand :fanout 1))
+      )
+      (max_steps 4)
+    )
+    """
+
+    program_file = tmp_path / "demo.nanocode"
+    program_file.write_text(program_src)
+    store_file = tmp_path / "store.json"
+
+    result = subprocess.run(
+        [
+            sys.executable,
+            "-m",
+            "src.cli",
+            str(program_file),
+            "--walk-children",
+            "--only-scale",
+            "1",
+            "--store-json",
+            str(store_file),
+        ],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+
+    summary = json.loads(result.stdout)
+    assert summary["rule_counts"] == {"grow-leaf": 1}
+    assert summary["scale_counts"] == {"1": 1}
+    assert summary["include_scales"] == [1]
+    assert summary["exclude_scales"] == []
+
+    stored_state = json.loads(store_file.read_text())
+    assert stored_state["include_scales"] == [1]
+    assert stored_state["exclude_scales"] == []
+
+    resumed = subprocess.run(
+        [sys.executable, "-m", "src.cli", str(program_file), "--load-store", str(store_file)],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+
+    resumed_summary = json.loads(resumed.stdout)
+    assert resumed_summary["include_scales"] == [1]
+    assert resumed_summary["exclude_scales"] == []
+    # Resumed run should honor stored scale filters and find no additional work.
+    assert resumed_summary["events"] == 0
+
+
+def test_cli_enforces_max_term_limit_and_persists_state(tmp_path: Path):
+    program_src = """
+    (program demo
+      (root (seed :scale 0))
+      (rules
+        (rule grow (pattern :sym seed) (action expand :fanout 2))
+      )
+      (max_steps 4)
+    )
+    """
+
+    program_file = tmp_path / "demo.nanocode"
+    program_file.write_text(program_src)
+    store_file = tmp_path / "store.json"
+
+    result = subprocess.run(
+        [
+            sys.executable,
+            "-m",
+            "src.cli",
+            str(program_file),
+            "--max-terms",
+            "1",
+            "--store-json",
+            str(store_file),
+        ],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+
+    summary = json.loads(result.stdout)
+    assert summary["term_limit_exhausted"] is True
+    assert summary["max_terms"] == 1
+
+    stored_state = json.loads(store_file.read_text())
+    assert stored_state["max_terms"] == 1
+    assert stored_state["term_limit_exhausted"] is True
+
+
+def test_cli_strict_matching_fails_on_ambiguity(tmp_path: Path):
+    program_src = """
+    (program demo
+      (root (seed :scale 0))
+      (rules
+        (rule first (pattern :sym seed) (action expand :fanout 1))
+        (rule second (pattern :sym seed) (action reduce))
+      )
+      (max_steps 2)
+    )
+    """
+
+    program_file = tmp_path / "ambiguous.nanocode"
+    program_file.write_text(program_src)
+
+    result = subprocess.run(
+        [sys.executable, "-m", "src.cli", str(program_file), "--strict-matching"],
+        check=False,
+        capture_output=True,
+        text=True,
+    )
+
+    assert result.returncode == 1
+    assert "ambiguous match" in result.stderr
+
+
+def test_cli_detect_conflicts_blocks_overlapping_rules(tmp_path: Path):
+    program_src = """
+    (program demo
+      (root (seed :scale 0))
+      (rules
+        (rule a (pattern :sym seed :scale 0) (action reduce))
+        (rule b (pattern :sym seed :scale 0) (action reduce))
+      )
+      (max_steps 2)
+    )
+    """
+
+    program_file = tmp_path / "demo.nanocode"
+    program_file.write_text(program_src)
+
+    result = subprocess.run(
+        [sys.executable, "-m", "src.cli", str(program_file), "--detect-conflicts"],
+        capture_output=True,
+        text=True,
+    )
+
+    assert result.returncode == 1
+    assert "Conflicting rule patterns" in result.stderr
+
diff --git a/nanocode-core/tests/test_constraints.py b/nanocode-core/tests/test_constraints.py
new file mode 100644
index 0000000000000000000000000000000000000000..263e230a46ecf240db543d22bcaf2a27e96cb8e8
--- /dev/null
+++ b/nanocode-core/tests/test_constraints.py
@@ -0,0 +1,81 @@
+from src.constraints import StructuralConstraints, measure_structure, validate_structure
+from src.evolution import EvolutionConfig, Genome, evaluate_population, evolve_population
+from src.terms import Term
+
+
+def _sample_term() -> Term:
+    return Term(
+        sym="root",
+        children=[
+            Term(sym="leaf"),
+            Term(sym="branch", scale=2, children=[Term(sym="child", scale=3)]),
+        ],
+    )
+
+
+def test_measure_structure_tracks_size_depth_and_scales():
+    metrics = measure_structure(_sample_term())
+
+    assert metrics.nodes == 4
+    assert metrics.leaves == 2
+    assert metrics.max_depth == 3
+    assert metrics.max_fanout == 2
+    assert metrics.min_scale == 0
+    assert metrics.max_scale == 3
+
+
+def test_validate_structure_reports_violations():
+    constraints = StructuralConstraints(
+        max_nodes=2, max_depth=2, max_fanout=1, min_scale=1, max_scale=2
+    )
+
+    violations = validate_structure(_sample_term(), constraints)
+
+    assert "nodes=4" in violations[0]
+    assert any("max_depth=3" in violation for violation in violations)
+    assert any("max_fanout=2" in violation for violation in violations)
+    assert any("min_scale=0" in violation for violation in violations)
+    assert any("max_scale=3" in violation for violation in violations)
+
+
+def test_evaluate_population_penalizes_constraint_violations():
+    good = Genome(root=Term(sym="ok"))
+    bad = Genome(root=_sample_term())
+
+    evaluations = evaluate_population(
+        [bad, good],
+        scorer=lambda genome: (1.0, {"id": genome.root.sym}),
+        constraints=StructuralConstraints(max_nodes=1),
+        violation_penalty=-99,
+    )
+
+    assert evaluations[0].genome is good
+    assert evaluations[0].score == 1.0
+    assert evaluations[1].score == -99
+    assert "violations" in evaluations[1].info
+
+
+def test_evolve_population_propagates_constraints_and_penalties():
+    bad = Genome(root=_sample_term())
+
+    config = EvolutionConfig(
+        population_size=1,
+        generations=1,
+        mutation_rate=0.0,
+        crossover_rate=0.0,
+        elitism=0,
+        tournament_size=1,
+        constraints=StructuralConstraints(max_nodes=1),
+        violation_penalty=-50,
+    )
+
+    final = evolve_population(
+        [bad],
+        scorer=lambda genome: 1.0,
+        mutate=lambda genome, rng: genome,
+        config=config,
+        rng=None,
+    )
+
+    assert final[0].score == -50
+    assert "violations" in final[0].info
diff --git a/nanocode-core/tests/test_evolution.py b/nanocode-core/tests/test_evolution.py
new file mode 100644
index 0000000000000000000000000000000000000000..8bc49a1cbeb5e3d33fbf2aed3e77c90edbb07039
--- /dev/null
+++ b/nanocode-core/tests/test_evolution.py
@@ -0,0 +1,132 @@
+from random import Random
+
+from src.evolution import (
+    EvolutionConfig,
+    Evaluation,
+    Genome,
+    annotate_genome,
+    crossover_terms,
+    delete_subtree,
+    evaluate_population,
+    evolve_population,
+    insert_subtree,
+    mutate_scale,
+    mutate_symbol,
+)
+from src.terms import Term
+
+
+def test_mutate_symbol_uses_pool_and_rng():
+    term = Term("root", children=[Term("a"), Term("b")])
+    rng = Random(0)
+
+    mutated = mutate_symbol(term, ["a", "b", "c"], rng=rng)
+
+    assert mutated.children[0].sym == "c"
+    assert mutated.children[1] == term.children[1]
+
+
+def test_mutate_scale_respects_bounds():
+    term = Term("root", scale=1, children=[Term("a", scale=2), Term("b", scale=3)])
+    rng = Random(1)
+
+    mutated = mutate_scale(term, delta_range=(-2, 2), min_scale=0, max_scale=3, rng=rng)
+
+    assert mutated.scale == 3
+    assert mutated.children[0].scale == 2
+
+
+def test_delete_subtree_skips_root():
+    term = Term("root", children=[Term("a", children=[Term("a1")]), Term("b")])
+    rng = Random(0)
+
+    mutated = delete_subtree(term, rng=rng)
+
+    assert mutated.children == [Term("a"), Term("b")]
+
+
+def test_insert_subtree_appends_spawned_child():
+    term = Term("root", children=[Term("a")])
+    rng = Random(0)
+
+    def spawn(parent: Term) -> Term:
+        return Term(sym=f"{parent.sym}.child", scale=parent.scale + 1)
+
+    mutated = insert_subtree(term, spawn, rng=rng)
+
+    assert mutated.children[0].children == [Term("a.child", scale=1)]
+
+
+def test_crossover_swaps_subtrees_deterministically():
+    a = Term("A", children=[Term("a1"), Term("a2")])
+    b = Term("B", children=[Term("b1"), Term("b2")])
+    rng = Random(1)
+
+    new_a, new_b = crossover_terms(a, b, rng=rng)
+
+    assert new_a == Term("b2")
+    assert new_b.children[1] == Term("A", children=[Term("a1"), Term("a2")])
+
+
+def test_genome_annotations_allow_metadata():
+    genome = Genome(root=Term("root"), annotations={"score": 0.5})
+
+    assert genome.annotations == {"score": 0.5}
+
+
+def test_evaluate_population_returns_sorted_scores_with_info():
+    genomes = [Genome(root=Term("a")), Genome(root=Term("b"))]
+
+    def scorer(genome: Genome) -> tuple[float, dict]:
+        return (1.0 if genome.root.sym == "a" else 0.25, {"sym": genome.root.sym})
+
+    evaluations = evaluate_population(genomes, scorer)
+
+    assert [ev.genome.root.sym for ev in evaluations] == ["a", "b"]
+    assert evaluations[0].info == {"sym": "a"}
+
+
+def test_annotate_genome_merges_metadata():
+    genome = Genome(root=Term("root"), annotations={"score": 1})
+
+    annotated = annotate_genome(genome, parent=0, score=2)
+
+    assert annotated.annotations == {"score": 2, "parent": 0}
+
+
+def test_evolve_population_runs_deterministically():
+    genomes = [Genome(root=Term("bar")) for _ in range(3)]
+    rng = Random(0)
+    config = EvolutionConfig(
+        population_size=3,
+        generations=2,
+        mutation_rate=1.0,
+        crossover_rate=0.0,
+        elitism=1,
+        tournament_size=2,
+    )
+
+    def scorer(genome: Genome) -> float:
+        return 1.0 if genome.root.sym == "foo" else 0.0
+
+    def mutate(genome: Genome, rng: Random) -> Genome:
+        mutated = mutate_symbol(genome.root, ["foo", "bar"], rng=rng)
+        return Genome(root=mutated, annotations=genome.annotations)
+
+    seen_best: list[Evaluation] = []
+
+    def on_generation(generation: int, best: Evaluation, _: list[Evaluation]):
+        seen_best.append(best)
+
+    final = evolve_population(
+        genomes,
+        scorer,
+        mutate,
+        config=config,
+        rng=rng,
+        on_generation=on_generation,
+    )
+
+    assert len(seen_best) == config.generations
+    assert final[0].score == 1.0
+    assert final[0].genome.root.sym == "foo"
diff --git a/nanocode-core/tests/test_interpreter.py b/nanocode-core/tests/test_interpreter.py
new file mode 100644
index 0000000000000000000000000000000000000000..3c38d4ffa549df0b68f2b2e174652653c5fe04d8
--- /dev/null
+++ b/nanocode-core/tests/test_interpreter.py
@@ -0,0 +1,113 @@
+import pytest
+from dataclasses import replace
+
+import pytest
+
+from src.interpreter import Execution, Interpreter, Program
+import pytest
+
+from src import terms
+from src.constraints import StructuralConstraints
+from src.interpreter import Execution, Interpreter, Program, validate_program
+from src.rewrite import Pattern, Rule, expand_action
+from src.signature import Signature, TermSignature
+
+
+def expand_leaf(term: terms.Term, _store) -> terms.Term:
+    return terms.expand(term, fanout=2)
+
+
+def reduce_wrapper(term: terms.Term, _store) -> terms.Term:
+    return terms.reduce(term)
+
+
+def test_interpreter_runs_to_idle_and_records_snapshot():
+    program = Program(
+        name="expand_reduce",
+        root=terms.Term("Seed", 0),
+        rules=[
+            Rule(name="expand", pattern=Pattern(predicate=lambda t: not t.children), action=expand_leaf),
+            Rule(name="reduce", pattern=Pattern(predicate=lambda t: t.sym.startswith("F(")), action=reduce_wrapper),
+        ],
+        max_steps=8,
+    )
+
+    result: Execution = Interpreter().run(program)
+
+    assert result.root_id == result.snapshot["root"]
+    assert [e.rule for e in result.events] == ["expand", "reduce"]
+    # The store should retain the interned root and the expanded variant.
+    assert len(result.snapshot["records"]) >= 2
+    assert result.stats["rule_counts"] == {"expand": 1, "reduce": 1}
+
+
+def test_run_until_idle_honors_step_budget():
+    program = Program(
+        name="one_step",
+        root=terms.Term("A", 0),
+        rules=[Rule(name="expand", pattern=Pattern(predicate=lambda t: not t.children), action=expand_leaf)],
+        max_steps=1,
+    )
+
+    interpreter = Interpreter()
+    result = interpreter.run(program)
+
+    assert len(result.events) == 1
+    # New work remains on the frontier because we stopped early.
+    assert len(result.snapshot["frontier"]) == 1
+
+
+def test_interpreter_can_optionally_detect_conflicts():
+    program = Program(
+        name="conflict", 
+        root=terms.Term("X", 0),
+        rules=[
+            Rule(name="a", pattern=Pattern(sym="X", scale=0), action=lambda t, s: t),
+            Rule(name="b", pattern=Pattern(sym="X", scale=0), action=lambda t, s: t),
+        ],
+        max_steps=1,
+    )
+
+    interpreter = Interpreter()
+    with pytest.raises(ValueError):
+        interpreter.run(program, detect_conflicts=True)
+
+    result = interpreter.run(program, detect_conflicts=False)
+    assert result.snapshot["root"] == result.root_id
+
+
+def test_validate_program_rejects_scale_jumps():
+    bad_root = terms.Term("root", 0, children=[terms.Term("child", 3)])
+    program = Program(name="bad", root=bad_root, rules=[])
+    interpreter = Interpreter()
+    with pytest.raises(ValueError):
+        interpreter.run(program)
+
+
+def test_program_constraints_are_enforced():
+    constrained_root = terms.Term("r", 0, children=[terms.Term("c", 0)])
+    program = Program(
+        name="constrained",
+        root=constrained_root,
+        rules=[],
+        constraints=StructuralConstraints(max_depth=1),
+    )
+    interpreter = Interpreter()
+    with pytest.raises(ValueError):
+        interpreter.run(program)
+
+
+def test_program_validation_respects_signature_patterns():
+    signature = Signature([TermSignature(sym="seed", min_children=0, max_children=0, allowed_scales={0})])
+    program = Program(name="demo", root=terms.Term("seed"), rules=[], signature=signature)
+
+    # unknown pattern symbol
+    bad_rules = [Rule(name="grow", pattern=Pattern(sym="unknown", scale=0), action=expand_action())]
+    with pytest.raises(ValueError):
+        validate_program(replace(program, rules=bad_rules))
+
+    # disallowed pattern scale
+    bad_rules = [Rule(name="grow", pattern=Pattern(sym="seed", scale=2), action=expand_action())]
+    with pytest.raises(ValueError):
+        validate_program(replace(program, rules=bad_rules))
+
diff --git a/nanocode-core/tests/test_meta.py b/nanocode-core/tests/test_meta.py
new file mode 100644
index 0000000000000000000000000000000000000000..2b963b4496f0f94617b05438406e8f1e13d0a7cc
--- /dev/null
+++ b/nanocode-core/tests/test_meta.py
@@ -0,0 +1,111 @@
+import pytest
+
+from src.interpreter import Program
+import pytest
+
+from src.constraints import StructuralConstraints
+from src.interpreter import Program
+from src.meta import (
+    action_to_term,
+    constraints_to_term,
+    program_to_term,
+    rule_to_term,
+    signature_to_term,
+    term_to_action,
+    term_to_constraints,
+    term_to_program,
+    term_to_rule,
+    term_to_signature,
+)
+from src.rewrite import Pattern, Rule, expand_action, reduce_action
+from src.signature import Signature, TermSignature
+from src.terms import Term
+
+
+def test_program_roundtrip_preserves_rules_and_actions():
+    rules = [
+        Rule(name="grow", pattern=Pattern(sym="seed", scale=0), action=expand_action(fanout=2)),
+        Rule(name="shrink", pattern=Pattern(scale=1), action=reduce_action()),
+    ]
+    program = Program(name="demo", root=Term("seed"), rules=rules, max_steps=32, max_terms=10)
+
+    program_term = program_to_term(program)
+    rebuilt = term_to_program(program_term)
+
+    assert rebuilt.name == program.name
+    assert rebuilt.root == program.root
+    assert rebuilt.max_steps == program.max_steps
+    assert rebuilt.max_terms == program.max_terms
+    assert [rule.name for rule in rebuilt.rules] == [rule.name for rule in rules]
+    assert rebuilt.rules[0].pattern.sym == "seed"
+    assert rebuilt.rules[0].action.name == "expand"
+    assert rebuilt.rules[0].action.params["fanout"] == 2
+    assert rebuilt.rules[1].pattern.scale == 1
+    assert rebuilt.rules[1].action.name == "reduce"
+
+
+def test_rule_and_action_terms_require_metadata_actions():
+    action = expand_action(fanout=4)
+    rule = Rule(name="grow", pattern=Pattern(sym="seed"), action=action)
+
+    roundtrip_rule = term_to_rule(rule_to_term(rule))
+    assert roundtrip_rule.action.name == "expand"
+    assert roundtrip_rule.action.params["fanout"] == 4
+
+    with pytest.raises(TypeError):
+        action_to_term(lambda t, s: t)  # type: ignore[arg-type]
+
+
+def test_action_term_rejects_unknown_names():
+    bad_term = Term(sym="action", children=[Term(sym="name", children=[Term(sym="unknown")])])
+    with pytest.raises(ValueError):
+        term_to_action(bad_term)
+
+
+def test_reduce_action_roundtrip_requires_registered_summarizer():
+    def summarize(children: list[Term]) -> str:
+        return f"span={len(children)}"
+
+    rule = Rule(name="shrink", pattern=Pattern(sym="x"), action=reduce_action(summarizer=summarize))
+    rule_term = rule_to_term(rule)
+
+    with pytest.raises(ValueError):
+        term_to_rule(rule_term)
+
+    restored = term_to_rule(rule_term, summarizers={"summarize": summarize})
+    assert restored.action.name == "reduce"
+    assert restored.action.params["summarizer"] == "summarize"
+
+
+def test_program_roundtrip_carries_constraints_and_signature():
+    signature = Signature(
+        [
+            TermSignature(sym="seed", min_children=0, max_children=0, allowed_scales={0}),
+            TermSignature(sym="grow", min_children=2, max_children=3, allowed_scales={1}),
+        ]
+    )
+    constraints = StructuralConstraints(max_nodes=10, max_depth=4, min_scale=0, max_scale=2)
+    rules = [Rule(name="grow", pattern=Pattern(sym="seed", scale=0), action=expand_action(fanout=2))]
+    program = Program(
+        name="demo",
+        root=Term("seed"),
+        rules=rules,
+        max_steps=32,
+        max_terms=10,
+        constraints=constraints,
+        signature=signature,
+    )
+
+    program_term = program_to_term(program)
+    rebuilt = term_to_program(program_term)
+
+    assert rebuilt.constraints == constraints
+    assert rebuilt.signature is not None
+    assert rebuilt.signature.to_dict() == signature.to_dict()
+
+    constraints_term = constraints_to_term(constraints)
+    assert term_to_constraints(constraints_term) == constraints
+
+    signature_term = signature_to_term(signature)
+    assert term_to_signature(signature_term).to_dict() == signature.to_dict()
+
diff --git a/nanocode-core/tests/test_pipeline.py b/nanocode-core/tests/test_pipeline.py
index e69de29bb2d1d6434b8b29ae775ad8c2e48c5391..003dc47527b9dcf8ff6f443aabffc175c70a7114 100644
--- a/nanocode-core/tests/test_pipeline.py
+++ b/nanocode-core/tests/test_pipeline.py
@@ -0,0 +1,25 @@
+from src.pipeline import make_text_program, run_pipeline
+from src.interpreter import Interpreter
+
+
+def test_pipeline_program_runs_multiscale():
+    program = make_text_program("abba")
+    execution = Interpreter().run(program)
+    final = execution.materialize_root()
+    assert final.sym in {"text", "F(text)"}
+    # Summary child reflects dominant motif
+    def _flatten(term):
+        if not term.children:
+            yield term.sym
+        for child in term.children:
+            yield from _flatten(child)
+
+    assert any("dominant=" in sym for sym in _flatten(final))
+    # Both expand and reduce should have fired
+    assert set(execution.stats["rule_counts"].keys()) == {"to-meso", "to-macro"}
+
+
+def test_run_pipeline_wrapper_returns_macro_term():
+    result = run_pipeline("abc")
+    assert result.sym in {"text", "F(text)"}
+    assert result.children
diff --git a/nanocode-core/tests/test_prototype.py b/nanocode-core/tests/test_prototype.py
new file mode 100644
index 0000000000000000000000000000000000000000..ef9b0883f19a7f11d9aaf20e4a98a50be2b49969
--- /dev/null
+++ b/nanocode-core/tests/test_prototype.py
@@ -0,0 +1,40 @@
+import json
+import subprocess
+from pathlib import Path
+
+from src.prototype import run_prototype
+
+
+def test_prototype_program_materializes_macro_label(tmp_path: Path):
+    execution = run_prototype("nanocode", trace=tmp_path / "trace.jsonl", store=tmp_path / "store.json")
+    final_term = execution.materialize_root()
+
+    assert final_term.scale == 2
+    macro = next(child for child in final_term.children if child.sym.startswith("port:out:macro"))
+    macro_label = macro.children[0].sym
+    assert any(macro_label.startswith(prefix) for prefix in ("vowel-heavy", "consonant-heavy", "mixed"))
+    assert "len=" in macro_label
+
+    trace = (tmp_path / "trace.jsonl").read_text().strip().splitlines()
+    assert trace, "trace should contain events"
+
+    stored = json.loads((tmp_path / "store.json").read_text())
+    assert stored["root"] == execution.snapshot["root"]
+
+
+def test_prototype_cli_runs(tmp_path: Path):
+    trace_path = tmp_path / "trace.jsonl"
+    store_path = tmp_path / "store.json"
+    result = subprocess.run(
+        ["python", "-m", "src.prototype", "--text", "aeiou", "--trace-jsonl", str(trace_path), "--store-json", str(store_path)],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+
+    payload = json.loads(result.stdout)
+    assert payload["macro_label"].startswith("vowel-heavy")
+    assert trace_path.exists()
+    assert store_path.exists()
+    # final term should surface the macro port tag
+    assert any(child["sym"].startswith("port:out:macro") for child in payload["final_term"]["children"])
diff --git a/nanocode-core/tests/test_quantum_bridge.py b/nanocode-core/tests/test_quantum_bridge.py
index e69de29bb2d1d6434b8b29ae775ad8c2e48c5391..4d92f81cfe47fbbdefd592275b5d90da98bb2550 100644
--- a/nanocode-core/tests/test_quantum_bridge.py
+++ b/nanocode-core/tests/test_quantum_bridge.py
@@ -0,0 +1,21 @@
+import random
+
+from src.quantum_bridge import classical_decision, motif_counts, quantum_to_classical, sample_oracle
+
+
+def test_motif_counts_and_decision():
+    samples = ["000", "111", "000", "101"]
+    counts = motif_counts(samples)
+    assert counts["000"] == 2
+    assert classical_decision(counts) in counts
+
+
+def test_quantum_to_classical_is_deterministic_with_seed():
+    random.seed(123)
+    result = quantum_to_classical(n=10)
+    assert isinstance(result, str)
+
+
+def test_sample_oracle_respects_n_argument():
+    samples = sample_oracle(lambda: "abc", n=3)
+    assert samples == ["abc", "abc", "abc"]
diff --git a/nanocode-core/tests/test_runtime.py b/nanocode-core/tests/test_runtime.py
new file mode 100644
index 0000000000000000000000000000000000000000..29191ffe384e631c94417cc3b3661a7f77e33745
--- /dev/null
+++ b/nanocode-core/tests/test_runtime.py
@@ -0,0 +1,311 @@
+import pytest
+
+from src import terms
+from src.constraints import StructuralConstraints
+from src.rewrite import AmbiguousRuleError, Pattern, Rule
+from src.runtime import Runtime
+from src.scheduler import LIFOScheduler
+from src.term_store import TermStore
+
+
+def expand_leaf(term: terms.Term, _store) -> terms.Term:
+    return terms.expand(term, fanout=2)
+
+
+def reduce_f_term(term: terms.Term, _store) -> terms.Term:
+    return terms.reduce(term)
+
+
+def test_runtime_applies_rules_and_logs_events():
+    rules = [
+        Rule(name="expand_leaf", pattern=Pattern(predicate=lambda t: not t.children), action=expand_leaf),
+        Rule(name="reduce_f", pattern=Pattern(predicate=lambda t: t.sym.startswith("F(")), action=reduce_f_term),
+    ]
+
+    runtime = Runtime(rules=rules)
+    root_id = runtime.load(terms.Term("A", 0))
+
+    events = runtime.run(max_steps=3)
+
+    assert len(events) == 2
+    assert events[0].before == root_id
+    assert events[0].rule == "expand_leaf"
+    assert events[1].rule == "reduce_f"
+
+    # After reduce, we should get back to the interned root ID (deduplication)
+    assert events[1].after != root_id
+
+
+def test_runtime_skips_unmatched_terms_but_keeps_running():
+    rules = [Rule(name="only_b", pattern=Pattern(sym="B"), action=lambda t, _: t)]
+
+    runtime = Runtime(rules=rules)
+    runtime.load(terms.Term("A", 0))
+    b_id = runtime.store.add_term(terms.Term("B", 0))
+    runtime.scheduler.push(b_id)
+
+    events = runtime.run(max_steps=2)
+
+    assert len(events) == 1
+    assert events[0].before == b_id
+
+
+def test_runtime_load_resets_state():
+    runtime = Runtime(rules=[])
+
+    first_root = runtime.load(terms.Term("X", 0))
+    runtime.run(max_steps=1)
+
+    second_root = runtime.load(terms.Term("Y", 0))
+
+    assert runtime.root_id == second_root
+    assert first_root != second_root
+    assert len(runtime.store.snapshot()) == 1
+    assert runtime.events == []
+
+
+def test_runtime_optionally_walks_children():
+    rules = [
+        Rule(
+            name="expand_shallow_leaves",
+            pattern=Pattern(predicate=lambda t: not t.children and t.scale < 2),
+            action=lambda t, _: terms.expand(t, fanout=2),
+        )
+    ]
+
+    runtime = Runtime(rules=rules, walk_children=True)
+    runtime.load(terms.Term("seed", 0))
+
+    events = runtime.run_until_idle(max_steps=20)
+
+    expanded_symbols = {event.before_term.sym for event in events}
+    assert "seed" in expanded_symbols
+    # With child walking enabled, grandchildren participate in rewriting.
+    assert len(events) >= 1
+
+
+def test_runtime_respects_walk_depth_limit():
+    rules = [
+        Rule(
+            name="mark_targets",
+            pattern=Pattern(sym="target"),
+            action=lambda t, _: terms.Term(f"marked-{t.sym}", t.scale, t.children),
+        )
+    ]
+
+    root = terms.Term("root", 0, children=[terms.Term("mid", 0, children=[terms.Term("target", 0)])])
+
+    shallow = Runtime(rules=rules, walk_children=True, walk_depth=1)
+    shallow.load(root)
+    shallow_events = shallow.run_until_idle(max_steps=5)
+    assert len(shallow_events) == 0
+
+    deep = Runtime(rules=rules, walk_children=True, walk_depth=2)
+    deep.load(root)
+    deep_events = deep.run_until_idle(max_steps=5)
+    assert len(deep_events) == 1
+    assert deep_events[0].after_term.sym.startswith("marked-target")
+
+
+def test_runtime_can_fail_on_ambiguous_matches():
+    rules = [
+        Rule(name="first", pattern=Pattern(sym="seed"), action=lambda t, _: terms.expand(t, fanout=1)),
+        Rule(name="second", pattern=Pattern(sym="seed"), action=lambda t, _: t),
+    ]
+
+    runtime = Runtime(rules=rules, strict_matching=True)
+    runtime.load(terms.Term("seed", 0))
+
+    with pytest.raises(AmbiguousRuleError) as excinfo:
+        runtime.step()
+
+    message = str(excinfo.value)
+    assert "ambiguous match" in message
+    assert "first" in message and "second" in message
+
+
+def test_runtime_tracks_rule_and_scale_counts():
+    rules = [
+        Rule(name="expand", pattern=Pattern(predicate=lambda t: not t.children), action=expand_leaf),
+        Rule(name="reduce", pattern=Pattern(predicate=lambda t: t.sym.startswith("F(")), action=reduce_f_term),
+    ]
+
+    runtime = Runtime(rules=rules, walk_children=False)
+    runtime.load(terms.Term("seed", 1))
+
+    runtime.run_until_idle(max_steps=4)
+
+    stats = runtime.stats()
+
+    assert stats["events"] == 2
+    assert stats["rule_counts"] == {"expand": 1, "reduce": 1}
+    # Expand bumps scale to 2 for the intermediate node; reduce acts at that scale.
+    assert stats["scale_counts"] == {1: 1, 2: 1}
+    assert stats["store_size"] >= 2
+    assert stats["idle"] is True
+    assert stats["budget_exhausted"] is False
+
+
+def test_runtime_honors_custom_scheduler_ordering():
+    rules = [Rule(name="mark", pattern=Pattern(), action=lambda t, _: t)]
+
+    runtime = Runtime(rules=rules, scheduler=LIFOScheduler(), walk_children=True)
+    runtime.load(terms.Term("root", 0, children=[terms.Term("left"), terms.Term("right")]))
+
+    events = runtime.run_until_idle(max_steps=5)
+    seen = [event.before_term.sym for event in events]
+
+    # LIFO scheduling reverses the initial push order: right, left, root.
+    assert seen[:3] == ["right", "left", "root"]
+
+
+def test_runtime_reports_budget_exhaustion_when_work_remains():
+    rules = [
+        Rule(name="expand", pattern=Pattern(predicate=lambda t: not t.children), action=expand_leaf),
+        Rule(name="reduce", pattern=Pattern(predicate=lambda t: t.sym.startswith("F(")), action=reduce_f_term),
+    ]
+
+    runtime = Runtime(rules=rules, walk_children=False)
+    runtime.load(terms.Term("seed", 0))
+
+    runtime.run(max_steps=1)
+
+    stats = runtime.stats()
+    assert stats["budget_exhausted"] is True
+    assert stats["idle"] is False
+    assert stats["frontier"]
+
+    runtime.run_until_idle(max_steps=10)
+
+    completed = runtime.stats()
+    assert completed["budget_exhausted"] is False
+    assert completed["idle"] is True
+
+
+def test_runtime_respects_rule_budgets():
+    rules = [
+        Rule(name="grow", pattern=Pattern(predicate=lambda t: not t.children), action=expand_leaf),
+    ]
+
+    runtime = Runtime(rules=rules, walk_children=True, rule_budgets={"grow": 1})
+    runtime.load(terms.Term("root", 0, children=[terms.Term("left"), terms.Term("right")]))
+
+    runtime.run_until_idle(max_steps=10)
+
+    stats = runtime.stats()
+    assert stats["rule_counts"] == {"grow": 1}
+    assert stats["rule_budget_exhausted"] == ["grow"]
+    # Only the first leaf is rewritten due to the budget cap, leaving work idle but processed.
+    assert stats["events"] >= 1
+
+
+def test_runtime_enforces_structural_constraints_during_execution():
+    rules = [
+        Rule(name="expand", pattern=Pattern(predicate=lambda t: not t.children), action=expand_leaf),
+    ]
+
+    runtime = Runtime(rules=rules, walk_children=True, constraints=StructuralConstraints(max_depth=1))
+    runtime.load(terms.Term("root", 0))
+
+    with pytest.raises(ValueError) as excinfo:
+        runtime.run(max_steps=2)
+
+    assert "Structural constraints" in str(excinfo.value)
+    stats = runtime.stats()
+    assert stats["constraint_exhausted"] is True
+    assert stats["constraint_violations"]
+
+
+def test_runtime_halts_when_term_limit_exhausted():
+    rules = [
+        Rule(name="grow", pattern=Pattern(predicate=lambda t: not t.children), action=expand_leaf),
+    ]
+
+    runtime = Runtime(rules=rules, walk_children=False, max_terms=1)
+    runtime.load(terms.Term("root", 0))
+
+    runtime.run_until_idle(max_steps=10)
+
+    stats = runtime.stats()
+    assert stats["events"] == 1
+    assert stats["term_limit_exhausted"] is True
+    # The store should have more terms than allowed, reflecting the point of exhaustion.
+    assert stats["store_size"] > runtime.max_terms
+
+
+def test_runtime_filters_rules_by_include_and_exclude():
+    rules = [
+        Rule(name="expand", pattern=Pattern(predicate=lambda t: not t.children), action=expand_leaf),
+        Rule(name="reduce", pattern=Pattern(predicate=lambda t: t.sym.startswith("F(")), action=reduce_f_term),
+    ]
+
+    runtime = Runtime(rules=rules, include_rules=["expand"], exclude_rules=["reduce"])
+    runtime.load(terms.Term("seed", 0))
+
+    runtime.run_until_idle(max_steps=5)
+
+    stats = runtime.stats()
+    assert stats["rule_counts"] == {"expand": 1}
+    # Reduce is excluded even though it would match the expanded term.
+    assert stats["events"] == 1
+    assert "reduce" not in stats["rule_counts"]
+
+
+def test_runtime_filters_terms_by_scale():
+    rules = [
+        Rule(name="scale0", pattern=Pattern(scale=0), action=lambda t, _: terms.expand(t, fanout=1)),
+        Rule(name="scale1", pattern=Pattern(scale=1), action=lambda t, _: terms.expand(t, fanout=1)),
+    ]
+
+    root = terms.Term("root", 0, children=[terms.Term("leaf", 1)])
+
+    runtime = Runtime(rules=rules, include_scales=[1], walk_children=True)
+    runtime.load(root)
+
+    runtime.run_until_idle(max_steps=5)
+
+    stats = runtime.stats()
+    assert stats["events"] == 1
+    assert stats["rule_counts"] == {"scale1": 1}
+    assert stats["scale_counts"] == {1: 1}
+
+
+def test_runtime_can_restore_serialized_state():
+    rules = [
+        Rule(name="grow", pattern=Pattern(sym="seed"), action=lambda t, _: terms.expand(t, fanout=1)),
+        Rule(name="normalize", pattern=Pattern(sym="F(seed)"), action=lambda t, _: terms.reduce(t)),
+    ]
+
+    runtime = Runtime(rules=rules)
+    runtime.load(terms.Term("seed", 0))
+    runtime.run(max_steps=1)
+
+    state = runtime.state()
+    restored = Runtime(rules=rules)
+    store = TermStore.from_json(state)
+    restored.load_state(
+        store=store,
+        root_id=state["root"],
+        frontier=state.get("frontier"),
+        processed=state.get("processed"),
+    )
+
+    restored.run_until_idle(max_steps=5)
+
+    stats = restored.stats()
+    assert stats["events"] >= 1
+    assert "normalize" in stats["rule_counts"]
+    assert isinstance(stats["frontier"], list)
+
+
+def test_runtime_can_detect_conflicting_rules():
+    rules = [
+        Rule(name="a", pattern=Pattern(sym="X", scale=0), action=lambda t, s: t),
+        Rule(name="b", pattern=Pattern(sym="X", scale=0), action=lambda t, s: t),
+    ]
+
+    with pytest.raises(ValueError):
+        Runtime(rules=rules, detect_conflicts=True)
+
+    runtime = Runtime(rules=rules, detect_conflicts=False)
+    runtime.load(terms.Term("X", 0))
diff --git a/nanocode-core/tests/test_scheduler.py b/nanocode-core/tests/test_scheduler.py
new file mode 100644
index 0000000000000000000000000000000000000000..5b22030304d23d12a4825c15fdfaaf490cfe565c
--- /dev/null
+++ b/nanocode-core/tests/test_scheduler.py
@@ -0,0 +1,56 @@
+from src.scheduler import FIFOScheduler, LIFOScheduler, RandomScheduler
+
+
+def test_fifo_scheduler_preserves_insertion_order():
+    scheduler = FIFOScheduler()
+
+    for term_id in ("T1", "T2", "T3"):
+        scheduler.push(term_id)
+
+    assert scheduler.pop() == "T1"
+    assert scheduler.pop() == "T2"
+    assert scheduler.pop() == "T3"
+    assert scheduler.pop() is None
+
+
+def test_lifo_scheduler_reverses_insertion_order():
+    scheduler = LIFOScheduler()
+
+    for term_id in ("T1", "T2", "T3"):
+        scheduler.push(term_id)
+
+    assert scheduler.pop() == "T3"
+    assert scheduler.pop() == "T2"
+    assert scheduler.pop() == "T1"
+    assert scheduler.pop() is None
+
+
+def test_random_scheduler_is_seeded_and_reproducible():
+    first = RandomScheduler(seed=7)
+    second = RandomScheduler(seed=7)
+
+    for term_id in ("T1", "T2", "T3", "T4"):
+        first.push(term_id)
+        second.push(term_id)
+
+    first_order = [first.pop() for _ in range(4)]
+    second_order = [second.pop() for _ in range(4)]
+
+    assert first_order == second_order
+    assert sorted(first_order) == ["T1", "T2", "T3", "T4"]
+
+
+def test_random_scheduler_state_round_trip():
+    scheduler = RandomScheduler(seed=2)
+    for term_id in ("T1", "T2", "T3"):
+        scheduler.push(term_id)
+
+    first_pop = scheduler.pop()
+    state = scheduler.state()
+    remaining = scheduler.pending()
+
+    rehydrated = RandomScheduler(seed=scheduler.seed, state=state)
+    for term_id in remaining:
+        rehydrated.push(term_id)
+
+    assert rehydrated.pop() == scheduler.pop()
diff --git a/nanocode-core/tests/test_signature.py b/nanocode-core/tests/test_signature.py
new file mode 100644
index 0000000000000000000000000000000000000000..39f697b3d08f6911e5ff6d4b6ef21368d2de2b5a
--- /dev/null
+++ b/nanocode-core/tests/test_signature.py
@@ -0,0 +1,44 @@
+import pytest
+
+from src.rewrite import Pattern, Rule
+from src.runtime import Runtime
+from src.signature import Signature, SignatureError
+from src.terms import Term
+
+
+def test_signature_validates_tree():
+    signature = Signature.from_dict(
+        {
+            "symbols": {
+                "root": {"min_children": 1, "max_children": 2, "scales": [0]},
+                "leaf": {"min_children": 0, "max_children": 0, "scales": [1]},
+            }
+        }
+    )
+
+    signature.validate_tree(Term(sym="root", scale=0, children=[Term(sym="leaf", scale=1)]))
+
+
+def test_signature_enforced_in_runtime():
+    signature = Signature.from_dict(
+        {
+            "symbols": {
+                "root": {"min_children": 1, "max_children": 1, "scales": [0]},
+                "leaf": {"min_children": 0, "max_children": 0, "scales": [1]},
+            }
+        }
+    )
+
+    def explode(term, store):
+        return Term(sym="leaf", scale=1, children=[Term(sym="leaf", scale=1)])
+
+    runtime = Runtime(
+        [Rule(name="expand", pattern=Pattern(sym="leaf", scale=1), action=explode)],
+        signature=signature,
+        walk_children=True,
+    )
+
+    runtime.load(Term(sym="root", scale=0, children=[Term(sym="leaf", scale=1)]))
+
+    with pytest.raises(SignatureError):
+        runtime.run_until_idle()
diff --git a/nanocode-core/tests/test_term_store.py b/nanocode-core/tests/test_term_store.py
new file mode 100644
index 0000000000000000000000000000000000000000..8e8d12f1c37747f959a7ff046696926cdf99e4f8
--- /dev/null
+++ b/nanocode-core/tests/test_term_store.py
@@ -0,0 +1,35 @@
+from src.term_store import TermStore
+from src.terms import Term
+
+
+def test_term_store_deduplicates_structurally_equal_terms():
+    store = TermStore()
+    t1 = Term("A", 0, [Term("B", 1), Term("C", 1)])
+    t2 = Term("A", 0, [Term("B", 1), Term("C", 1)])
+
+    id1 = store.add_term(t1)
+    id2 = store.add_term(t2)
+
+    assert id1 == id2
+    assert len(store.snapshot()) == 3  # A, B, C
+
+
+def test_term_materialization_round_trip():
+    store = TermStore()
+    root = Term("root", 0, [Term("leaf", 1)])
+    root_id = store.add_term(root)
+
+    rebuilt = store.materialize(root_id)
+    assert rebuilt == root
+
+
+def test_term_store_serialization_round_trip():
+    store = TermStore()
+    root = Term("root", 0, [Term("left", 1), Term("right", 1, [Term("leaf", 2)])])
+    root_id = store.add_term(root)
+
+    payload = store.to_json()
+    rebuilt = TermStore.from_json(payload)
+
+    assert root_id in rebuilt
+    assert rebuilt.materialize(root_id) == root
diff --git a/nanocode-core/tests/test_terms.py b/nanocode-core/tests/test_terms.py
index 6fc163a3b47bf2a51c5c41a832e55169bb11a80f..6c98900c5327847dc647eb2b8b0b6b2f0dd3c024 100644
--- a/nanocode-core/tests/test_terms.py
+++ b/nanocode-core/tests/test_terms.py
@@ -1,6 +1,40 @@
-from src.terms import Term, expand, reduce
-
-def test_expand_reduce_identity():
-    t = Term("A", 0)
-    assert reduce(expand(t)).sym == "A"
-    assert reduce(expand(t)).scale == 0
+import random
+
+from src.terms import Term, expand, reduce
+
+
+def test_expand_reduce_identity():
+    t = Term("A", 0)
+    round_trip = reduce(expand(t))
+    assert round_trip.sym == "A"
+    assert round_trip.scale == 0
+    assert round_trip.children[0].sym.startswith("summary:")
+
+
+def _random_term(depth: int, scale: int = 0) -> Term:
+    sym = random.choice(["a", "b", "c", "x", "y"])
+    if depth <= 1:
+        return Term(sym=sym, scale=scale)
+    fanout = random.randint(1, 3)
+    children = [_random_term(depth - 1, scale=scale) for _ in range(fanout)]
+    return Term(sym=sym, scale=scale, children=children)
+
+
+def test_expand_reduce_preserves_base_symbol_and_summarizes_children():
+    random.seed(42)
+    for _ in range(10):
+        base = _random_term(depth=3)
+        expanded = expand(base, fanout=3)
+        reduced = reduce(expanded)
+        assert reduced.sym == base.sym
+        assert reduced.scale == base.scale
+        summary = reduced.children[0].sym
+        def _flatten(term):
+            if not term.children:
+                yield term.sym
+            for child in term.children:
+                yield from _flatten(child)
+
+        for leaf_sym in _flatten(base):
+            assert leaf_sym in summary
+
diff --git a/nanocode-core/tests/test_trace.py b/nanocode-core/tests/test_trace.py
new file mode 100644
index 0000000000000000000000000000000000000000..bb6b028f33b16540c2821d9ad3f03c6906dcc20b
--- /dev/null
+++ b/nanocode-core/tests/test_trace.py
@@ -0,0 +1,48 @@
+import io
+import json
+
+from src import terms
+from src.rewrite import Pattern, Rule
+from src.runtime import Runtime
+from src.trace import JSONLTracer, dump_events
+
+
+def expand_leaf(term: terms.Term, _store) -> terms.Term:
+    return terms.expand(term, fanout=2)
+
+
+def reduce_f_term(term: terms.Term, _store) -> terms.Term:
+    return terms.reduce(term)
+
+
+def test_jsonl_tracer_captures_runtime_events():
+    rules = [
+        Rule(name="expand_leaf", pattern=Pattern(predicate=lambda t: not t.children), action=expand_leaf),
+        Rule(name="reduce_f", pattern=Pattern(predicate=lambda t: t.sym.startswith("F(")), action=reduce_f_term),
+    ]
+
+    sink = io.StringIO()
+    tracer = JSONLTracer(sink)
+
+    runtime = Runtime(rules=rules, event_hooks=[tracer])
+    root_id = runtime.load(terms.Term("A", 0))
+    runtime.run(max_steps=3)
+
+    lines = [l for l in sink.getvalue().splitlines() if l]
+    assert len(lines) == 2
+
+    first = json.loads(lines[0])
+    assert first["before"] == root_id
+    assert first["rule"] == "expand_leaf"
+    assert first["after_term"]["sym"].startswith("F(")
+
+
+def test_dump_events_serializes_event_stream():
+    rule = Rule(name="echo", pattern=Pattern(predicate=lambda _t: True), action=lambda t, _s: t)
+    runtime = Runtime([rule])
+    runtime.load(terms.Term("Z", 0))
+    events = runtime.run(max_steps=1)
+
+    records = dump_events(events)
+    assert records[0]["before_term"]["sym"] == "Z"
+    assert records[0]["after_term"]["sym"] == "Z"
